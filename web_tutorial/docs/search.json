[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate4Economist: an R package to comabine weather and spatial analysis with surveys",
    "section": "",
    "text": "1 Tutorial Content\n  2 What do I need before starting?\n  \n  2.1 You need R and Rstudio\n  2.2 You need the following packages\n  2.3 You need to install climate4economist\nThis website is meant to provide a series of tutorials to extract spatial data based on survey locations, compute weather extreme indicators, and merge them with survey data based on location and date of interview. The tutorials rely on climate4economist package, which contains functions that wrap up as many steps as possible to ensure it is easier for the user to follow.\nIt is not meant to be a tutorial on spatial analysis, neither a tutorial on weather data and indicators, and neither a tutorial on R. Despite, some of these aspects are describe the main purpose is to provide a guide that can be replicate with other surveys.\nThe target audience includes economists who may have experience with statistical software (e.g. STATA) but are less familiar with spatial data processing in R."
  },
  {
    "objectID": "index.html#tutorial-content",
    "href": "index.html#tutorial-content",
    "title": "Climate4Economist: an R package to comabine weather and spatial analysis with surveys",
    "section": "1 Tutorial Content",
    "text": "1 Tutorial Content\n\n0: Reminder of some R concepts. If you are new to R, read this first. It will give a small description of the pipe command (|&gt;), the package namespace (package::function()), the assign operator (&lt;-), and about how to create and use functions in R.\n1: Extract spatial data based on spatial point. The first tutorial shows how to extract spatial variables based on the survey coordinates. The spatial variables covers some common variables used as control in regression analysis, like elevation, agroecological zones, nighttime light, Urban-Rural Catchment Areas, precipitation, and temperature. The tutorial shows also how to compute some terrain indicators and climatic parameters.\n2: Extract spatial data based on spatial polygons. The second tutorial is very similar to the first but shows how to extract the spatial variable based on areas associated to the survey, like the administrative divisions. The spatial variables are the same of the first tutorial: elevation, agroecological zones, nighttime light, Urban-Rural Catchment Areas, precipitation, temperature, and in addition population. The tutorial shows also how to compute some terrain indicators and climatic parameters.\n3: Compute the Standardize Precipitation Index based on spatial point. The third tutorial shows how to compute the Standardize Precipitation Index (SPI) based on based on the survey coordinates.\n4: Compute the Standardize Precipitation Index based on spatial polygons. The third tutorial shows how to compute the Standardize Precipitation Index (SPI) based on based on on areas associated to the survey, like the administrative divisions.\n5: Compute the dry spell indicator based on spatial point.\n6: Compute the dry spell indicator based on spatial polygons.\n7: Compute the flood indicator based on spatial point.\n8: Compute the flood indicator based on spatial polygons.\n9: Compute the cold indicator based on spatial point.\n10: Compute the cold indicator based on spatial polygons.\n11: Compute the cold indicator in multi country.\n12: Distance analysis."
  },
  {
    "objectID": "index.html#what-do-i-need-before-starting",
    "href": "index.html#what-do-i-need-before-starting",
    "title": "Climate4Economist: an R package to comabine weather and spatial analysis with surveys",
    "section": "2 What do I need before starting?",
    "text": "2 What do I need before starting?\n\n2.1 You need R and Rstudio\nFrom this web page you can download both R and Rstudio.\n\n\n\n\n\n\nNote\n\n\n\nNote that R is the actual computational software and Rstudio is the integrated development environment (IDE), a set of tools built to help you be more productive with R.\n\n\nAnother common alternative IDE to Rstudio is Visual Studio Code.\n\n\n2.2 You need the following packages\n\n1install.packages(\"tidyverse\")\ninstall.packages(\"data.table\")\n2install.packages(\"purrr\")\n3install.packages(\"terra\")\ninstall.packages(\"sf\")\n4install.packages(\"exactextractr\")\n5install.packages(\"tidyterra\")\n6install.packages(\"haven\")\n7install.packages(\"furrr\")\n8install.packages(\"SPEI\")\n9install.packages(\"clock\")\n\n\n1\n\ndata manipulation\n\n2\n\nlist manipulation\n\n3\n\nspatial data operations\n\n4\n\nfast extraction for polygons\n\n5\n\nmanipulation the attribute data of spatial vector\n\n6\n\nhandling of STATA dta\n\n7\n\nparallel computation\n\n8\n\ncalculation of the Standardized Precipitation-Evapotranspiration Index\n\n9\n\ndate and time manipulation\n\n\n\n\n\n\n2.3 You need to install climate4economist\n\ndevtools::install_local(file.path(\"path_to_the_zip_file\", \"climate4economist.zip\"))"
  },
  {
    "objectID": "spi_by_poly.html",
    "href": "spi_by_poly.html",
    "title": "4: Compute the Standardize Precipitation Index based on spatial polygons",
    "section": "",
    "text": "1 Introduction\n  2 Code\n  \n  2.1 Set Up\n  2.2 Read the data\n  2.3 Georeference the survey\n  2.4 PLot\n  2.5 Merge administrative division and survey\n  2.6 Extraction\n  2.7 Compute the SPI\n  2.8 Agregate at the adiministrative divisions\n  2.9 Merge with survey\n  2.10 Select based on the interview\n  2.11 Save"
  },
  {
    "objectID": "spi_by_poly.html#introduction",
    "href": "spi_by_poly.html#introduction",
    "title": "4: Compute the Standardize Precipitation Index based on spatial polygons",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we extract the CHIRPS precipitation observations in Suriname, based on the administrative divisions of the Suriname Survey of Living Conditions for the years 2016/17 and 2022. Based on the extracted precipitation, the tutorial shows how to compute the Standardized Precipitation Index (SPI) and merge it with the survey based on the date of interviews."
  },
  {
    "objectID": "spi_by_poly.html#code",
    "href": "spi_by_poly.html#code",
    "title": "4: Compute the Standardize Precipitation Index based on spatial polygons",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Set Up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only climatic4economist package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\nlibrary(climatic4economist)\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note .. means one step back to the folder directory, i.e. one folder back.\nNote that how to set up the paths depends on your folder organization but there are overall two approaches:\n\nyou can use the R project, by opening the project directly you don’t need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder.\nyou can manually set the working folder with the function setwd().\n\n\n# path to data folder\n1path_to_data &lt;- file.path(\"..\",\n2                          \"..\", \"data\")\n\n# survey \npath_to_wave_1 &lt;- file.path(path_to_data, \"survey\", \"surname\", \"wave 1\",\n                            \"RT001_Public.dta\")\n\npath_to_wave_2 &lt;- file.path(path_to_data, \"survey\", \"surname\", \"wave 2\", \n                            \"2022 RT001_Housing_plus.dta\")\n\n# administrative division\npath_to_adm_div &lt;- file.path(path_to_data, \"adm_div\", \"GAUL\")\n\n# weather variables\npath_to_pre_monthly &lt;- file.path(path_to_data, \"weather\", \"CHIRPS\", \"monthly\",\n                                 \"chirps-v2.0.monthly.nc\")\n# to result folder\npath_to_result &lt;- file.path(path_to_data, \"result\")\n\n\n1\n\nconcatenate the string to make a path\n\n2\n\n.. means one folder back\n\n\n\n\n\n\n\n2.2 Read the data\n\n2.2.1 Survey\nWe begin by reading the surveys, which in this case consist of two waves with potentially different locations. As a result, we need to load both waves. The waves are stored as dta files, so we use the haven::read_dta() function to read them.\nWe only need the hhid, the survey coordinates, and the interview dates. We use dplyr::select() to choose these variables. This passage is optional and we bring with us all the variables, but we won’t use them. Note that the first wave does not include the interview date.\nWe combine the two waves using dplyr::bind_rows().\nWe can use the head() function to preview the data and see how it looks.\n\nwave_1 &lt;- haven::read_dta(path_to_wave_1) |&gt;\n    dplyr::select(hhid, lat_cen, long_cen) |&gt; \n    dplyr::mutate(wave = 1)\n\nwave_2 &lt;- haven::read_dta(path_to_wave_2) |&gt;\n    dplyr::select(hhid, end_date_n, lat_cen, long_cen) |&gt;\n    dplyr::mutate(wave = 2)\n\nsurvey &lt;- dplyr::bind_rows(wave_1, wave_2)\n\nhead(survey)\n\n# A tibble: 6 × 5\n     hhid lat_cen long_cen  wave end_date_n\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    \n1 1010031    5.82    -55.2     1 NA        \n2 1010041    5.82    -55.2     1 NA        \n3 1010051    5.82    -55.2     1 NA        \n4 1010061    5.82    -55.2     1 NA        \n5 1010121    5.82    -55.2     1 NA        \n6 1010131    5.82    -55.2     1 NA        \n\n\n\n\n2.2.2 Aministrative Divisions\nWe read the spatial file containing the national borders of Suriname, we use read_GAUL() to load it. By printing the spatial data, we can obtain key information, such as the dimensions (number of rows and variables), the geometry (which indicates the type of spatial object), and the coordinate reference system (CRS), which links the coordinates to precise locations on the Earth’s surface. The CRS is particularly important when working with different spatial datasets, as mismatched CRSs can prevent the datasets from aligning correctly.\n\nadm_div &lt;- read_GAUL(path_to_adm_div, iso = \"SUR\", lvl = 2)\nadm_div\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 62, 4  (geometries, attributes)\n extent      : -58.04987, -53.97982, 1.83114, 6.004546  (xmin, xmax, ymin, ymax)\n source      : GAUL-SUR-ADM2.geojson\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_adm_div   iso  adm_div_1  adm_div_2\n type        :      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;\n values      :          1   SUR Brokopondo  Brownsweg\n                        2   SUR Brokopondo    Centrum\n                        3   SUR Brokopondo Klaaskreek\n\n\n\n\n2.2.3 Weather\nFinally, we load the precipitation data. Climatic data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or “cell” in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region. In this case, the values are monthly precipitation that fell in that region. We use the function terra::rast() to load the raster data.\nThis particular raster has global coverage, so we crop it to focus on the country area to reduce its size. Although this step is not strictly necessary, it helps decrease the memory load and makes visualizations more manageable. We use the function crop_with_buffer() for this purpose.\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS). Given the importance of the CRS, we extract it using terra::crs() and save it for later use.\nWe also rename the raster layers to reflect the corresponding dates for each layer, as this is useful if we want to track the dates. We use terra::time() to extract the dates.\n\n\n\n\n\n\nNote\n\n\n\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n\n\n\nweather_monthly &lt;- terra::rast(path_to_pre_monthly) |&gt;\n    crop_with_buffer(adm_div)\nweather_monthly\n\nclass       : SpatRaster \ndimensions  : 83, 81, 527  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -58.05, -54, 1.85, 6  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \nsource(s)   : memory\nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       :  precip_1,  precip_2,  precip_3, precip_4, precip_5,   precip_6, ... \nmin values  :  36.21083,  49.09035,  34.04647, 153.5361, 103.0583,   90.83206, ... \nmax values  : 403.70178, 653.45990, 345.78024, 622.1295, 795.1603, 1034.43372, ... \nunit        :  mm/month,  mm/month,  mm/month, mm/month, mm/month,   mm/month, ... \ntime (days) : 1981-01-01 to 2024-11-01 \n\nnames(weather_monthly) &lt;- terra::time(weather_monthly)\nweather_monthly\n\nclass       : SpatRaster \ndimensions  : 83, 81, 527  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -58.05, -54, 1.85, 6  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \nsource(s)   : memory\nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       : 1981-01-01, 1981-02-01, 1981-03-01, 1981-04-01, 1981-05-01, 1981-06-01, ... \nmin values  :   36.21083,   49.09035,   34.04647,   153.5361,   103.0583,   90.83206, ... \nmax values  :  403.70178,  653.45990,  345.78024,   622.1295,   795.1603, 1034.43372, ... \nunit        :   mm/month,   mm/month,   mm/month,   mm/month,   mm/month,   mm/month, ... \ntime (days) : 1981-01-01 to 2024-11-01 \n\n\n\n\n\n\n2.3 Georeference the survey\nAs we’ve mentioned, the weather data is georeferenced, so we need to ensure the same for the survey data. Since many households share the same coordinates, they are linked to the same weather events. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called ID.\nThis is handled by the prepare_coord() function, which requires the coordinates’ variable names as input.\nWe can print the result to check the transformation. The new column, ID, is created by prepare_coord() and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\nsrvy_coord &lt;- prepare_coord(survey, lat_var = lat_cen, lon_var = long_cen)\nsrvy_coord\n\n# A tibble: 4,535 × 6\n   ID           hhid lat_cen long_cen  wave end_date_n\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    \n 1 1     23903250101    3.85    -54.2     2 2022-12-06\n 2 1     23903250201    3.85    -54.2     2 2022-12-06\n 3 1     23903250301    3.85    -54.2     2 2022-12-06\n 4 1     23903250401    3.85    -54.2     2 2022-12-07\n 5 1     23903252201    3.85    -54.2     2 2022-12-07\n 6 1     23903252301    3.85    -54.2     2 2022-12-06\n 7 1     23903252501    3.85    -54.2     2 2022-12-06\n 8 1     23903252601    3.85    -54.2     2 2022-12-06\n 9 1     23903253301    3.85    -54.2     2 2022-12-07\n10 1     23903253501    3.85    -54.2     2 2022-12-06\n# ℹ 4,525 more rows\n\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the georef_coord() function requires the coordinates’ variable names as input.\n\nsrvy_geo &lt;- georef_coord(srvy_coord,\n                          geom = c(\"long_cen\", \"lat_cen\"),\n                          crs = \"EPSG:4326\")\nsrvy_geo\n\n class       : SpatVector \n geometry    : points \n dimensions  : 688, 1  (geometries, attributes)\n extent      : -57.05207, -54.05524, 3.849064, 5.943465  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    ID\n type        : &lt;chr&gt;\n values      :     1\n                   2\n                   3\n\n\n\n\n\n2.4 PLot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\nterra::plot(adm_div, col = \"grey\", main = \"Suriname and PSU centroids\")\nterra::points(srvy_geo, col = \"gold\", alpha = 0.5, cex = 0.5)\nsrvy_geo |&gt; \n    tidyterra::filter(ID %in% c(\"21\", \"22\", \"652\")) |&gt;\n    terra::text(\"ID\", halo = TRUE, hc = \"blue\", col = \"white\", hw = 0.2)\n\n\n\n\n\n\n\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the capital and along the coast.\nHowever, some coordinates falls outside the administrative division, we highlights them with the blue halo. It may difficult to see them without the zoom , but it is worth notice that as it has consequences for which administrative division to associate them.\n\nIn many microeconomics applications some coordinates fall outside the national order of the country, this might happen for various reasons. Sometimes, the coordinates are modified to guarantee anonimacy of the household and the modification bring the coordinates outside the borders, other time it is just an error of the GPS, or the borders maps are not enought precise.\n\nNext, we plot a layer of the precipitation data to see how it overlaps with the spatial coordinates.\n\nterra::plot(weather_monthly, \"2024-10-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-10 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the high spatial resolution of the CHIRPS dataset. However, despite this high resolution, some survey coordinates still fall within the same cell.\n\n\n\n2.5 Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\nsrvy_adm_div &lt;- get_poly_attr_for_point(point = srvy_geo, poly = adm_div)\n\nSome spatial points are outside the polygon. They will be assigned to the closest polygon.\nThese are the points: 21 22 652 688\n\nsrvy_adm_div\n\n# A tibble: 688 × 7\n   ID      lon   lat ID_adm_div iso   adm_div_1  adm_div_2     \n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         \n 1 1     -54.2  3.85 55         SUR   Sipaliwini Tapanahony    \n 2 2     -55.5  3.93 52         SUR   Sipaliwini Boven Suriname\n 3 3     -55.5  4.00 52         SUR   Sipaliwini Boven Suriname\n 4 4     -55.5  4.01 52         SUR   Sipaliwini Boven Suriname\n 5 5     -55.5  4.01 52         SUR   Sipaliwini Boven Suriname\n 6 6     -55.4  4.05 52         SUR   Sipaliwini Boven Suriname\n 7 7     -54.8  4.06 55         SUR   Sipaliwini Tapanahony    \n 8 8     -55.4  4.14 52         SUR   Sipaliwini Boven Suriname\n 9 9     -55.4  4.16 52         SUR   Sipaliwini Boven Suriname\n10 10    -55.2  4.18 6          SUR   Brokopondo Sarakreek     \n# ℹ 678 more rows\n\n\n\n\n\n2.6 Extraction\nFor weather data, we use a different function for extracting the data, namely extract_cell_by_poly(). This function doesn’t aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the SPI for each cell and only later aggregate them.\n\n\n\n\n\n\nNote\n\n\n\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n\n\nLooking at the result, we see first the ID_adm_div column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\npre_cell &lt;- extract_cell_by_poly(weather_monthly, adm_div)\n\n\npre_cell\n\n# A tibble: 5,886 × 531\n   ID_adm_div x_cell y_cell coverage_fraction X1981_01_01 X1981_02_01\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1           -55.3   5.12             0.332        171.        267.\n 2 1           -55.2   5.12             0.633        182.        252.\n 3 1           -55.2   5.12             0.699        207.        253.\n 4 1           -55.1   5.12             0.765        212.        268.\n 5 1           -55.1   5.12             0.491        193.        265.\n 6 1           -55.3   5.07             0.553        177.        272.\n 7 1           -55.2   5.07             1            194.        268.\n 8 1           -55.2   5.07             1            217.        270.\n 9 1           -55.1   5.07             1            218.        274.\n10 1           -55.1   5.07             0.996        195.        266.\n# ℹ 5,876 more rows\n# ℹ 525 more variables: X1981_03_01 &lt;dbl&gt;, X1981_04_01 &lt;dbl&gt;,\n#   X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;, X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;,\n#   X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;, X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;,\n#   X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;, X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;,\n#   X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;, X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;,\n#   X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;, X1982_11_01 &lt;dbl&gt;, …\n\n\n\n\n2.7 Compute the SPI\nWe now compute the SPI with the function compute_spi(). This function requires the precipitation time series for each location and the time scale at which the SPI is computed.\nTo compute the SPI, it is recommended to use at least 30 years of observation to ensure a good estimation of the parameters. More years can strength the estimation but the results can be affected by climate change: if there have been a change in the climate parameters, old observations might be not indicative of the current situation affecting the estimation. There are no clear rule on this, so we leave add the possibility to select the time range of observation with the function select_by_dates(). The function requires both or just one between the starting date, from, and the end date to. If both are provide the the function select between the two dates, if only from is provided the function selects all date after, and if only to is provided the function selects all date before.\nLooking at the result, we see first is the ID_adm_div column, that we will use to merge back with the survey. Then we have the coordinates of each cell and the coverage fraction of the cell within the administrate border. The other columns contain the SPI observations over time, specific to each coordinate.\n\n# coord  &lt;- select_by_dates(coord, from = \"1991-01-01\", to = \"2023-01-01\") \n\nspi3 &lt;- compute_spi(pre_cell, time_scale = 3)\nspi3\n\n# A tibble: 5,886 × 531\n   ID_adm_div x_cell y_cell coverage_fraction X1981_01_01 X1981_02_01\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1           -55.3   4.82          0.000447          NA          NA\n 2 1           -55.3   4.77          0.617             NA          NA\n 3 1           -55.3   4.82          0.915             NA          NA\n 4 1           -55.3   4.87          0.762             NA          NA\n 5 1           -55.3   4.92          0.575             NA          NA\n 6 1           -55.3   4.97          0.715             NA          NA\n 7 1           -55.3   5.02          0.521             NA          NA\n 8 1           -55.3   5.07          0.553             NA          NA\n 9 1           -55.3   5.12          0.332             NA          NA\n10 1           -55.2   4.77          0.369             NA          NA\n# ℹ 5,876 more rows\n# ℹ 525 more variables: X1981_03_01 &lt;dbl&gt;, X1981_04_01 &lt;dbl&gt;,\n#   X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;, X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;,\n#   X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;, X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;,\n#   X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;, X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;,\n#   X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;, X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;,\n#   X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;, X1982_11_01 &lt;dbl&gt;, …\n\n\nIf we want to calculate the SPI with time scale equal to one, we just need to change the time_scale argument.\n\n\n\n2.8 Agregate at the adiministrative divisions\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function agg_to_adm_div() can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the coverage_fraction variable.\nThe argument match_col identifies which columns are aggregated. In this case we want to select all dates observation, so we define the pattern to look for as any number [0-9] repeated four time {4}.\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, ID_adm_div, and a single value of the climatic parameters for each locations.\n\nspi3_adm &lt;- agg_to_adm_div(spi3, match_col = \"[0-9]{4}\")\n\nspi3_adm\n\n# A tibble: 62 × 528\n   ID_adm_div X1981_01_01 X1981_02_01 X1981_03_01 X1981_04_01 X1981_05_01\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1                  NaN         NaN     0.539         0.839       0.783\n 2 10                 NaN         NaN     0.484         0.936       0.378\n 3 11                 NaN         NaN     0.322         1.22        0.171\n 4 12                 NaN         NaN     0.462         1.19        0.510\n 5 13                 NaN         NaN    -0.00298       1.06        0.683\n 6 14                 NaN         NaN     0.0467        1.06        0.644\n 7 15                 NaN         NaN     0.0284        1.05        0.445\n 8 16                 NaN         NaN     0.187         0.800       0.464\n 9 17                 NaN         NaN     0.190         0.912       0.454\n10 18                 NaN         NaN     0.406         1.06        0.416\n# ℹ 52 more rows\n# ℹ 522 more variables: X1981_06_01 &lt;dbl&gt;, X1981_07_01 &lt;dbl&gt;,\n#   X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;, X1981_11_01 &lt;dbl&gt;,\n#   X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;, X1982_03_01 &lt;dbl&gt;,\n#   X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;, X1982_07_01 &lt;dbl&gt;,\n#   X1982_08_01 &lt;dbl&gt;, X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;, X1982_11_01 &lt;dbl&gt;,\n#   X1982_12_01 &lt;dbl&gt;, X1983_01_01 &lt;dbl&gt;, X1983_02_01 &lt;dbl&gt;, …\n\n\n\n\n\n2.9 Merge with survey\nNow, we combine the extracted weather data with the survey data.\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as srvy_adm_div. We first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command |&gt; assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with y = _, where y is the name of the argument and _ refer to the previous merge.\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n1srvy_spi3_adm &lt;- dplyr::inner_join(srvy_adm_div, spi3_adm, by = \"ID_adm_div\") |&gt;\n2    merge_by_common(srvy_coord, y = _)\nsrvy_spi3_adm\n\n\n1\n\nmerge adm info with spatial var\n\n2\n\n_ refers to the output of the previous merge\n\n\n\n\n# A tibble: 4,535 × 539\n   ID        hhid lat_cen long_cen  wave end_date_n   lon   lat ID_adm_div iso  \n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 2 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 3 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 4 1      2.39e10    3.85    -54.2     2 2022-12-07 -54.2  3.85 55         SUR  \n 5 1      2.39e10    3.85    -54.2     2 2022-12-07 -54.2  3.85 55         SUR  \n 6 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 7 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 8 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 9 1      2.39e10    3.85    -54.2     2 2022-12-07 -54.2  3.85 55         SUR  \n10 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n# ℹ 4,525 more rows\n# ℹ 529 more variables: adm_div_1 &lt;chr&gt;, adm_div_2 &lt;chr&gt;, X1981_01_01 &lt;dbl&gt;,\n#   X1981_02_01 &lt;dbl&gt;, X1981_03_01 &lt;dbl&gt;, X1981_04_01 &lt;dbl&gt;, X1981_05_01 &lt;dbl&gt;,\n#   X1981_06_01 &lt;dbl&gt;, X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;,\n#   X1981_10_01 &lt;dbl&gt;, X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;,\n#   X1982_02_01 &lt;dbl&gt;, X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;,\n#   X1982_06_01 &lt;dbl&gt;, X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;, …\n\n\nWe are back at 4535, which matches the original survey.\n\n\n\n2.10 Select based on the interview\nNow that we have merged the SPI values with the survey, we can select just the relevant observations.\nIf we want to select just a subsets of observations we can use the select_by_dates() function. If we want to select based on the date of interview of the survey, we can use select_by_interview(). This last function requires the variable that contains the dates of interview and the interval to select based on the dates. The interval must be express in number of months or in number years. The wide argument specifies how the output should be reported, in wide with each time observation as separate columns, or long, with all observation in one column.\n\nNote that current version of the select_by_interview() functions drops the observations with missing date of interview.\n\nWhat is relevant depends on the particular application, but we can agree that we don’t want to assign weather observations that happened after the interviews, as these cannot influence the answers.\nIn this tutorial we select the 12 months before the interviews using the function select_by_interview(). The argument interview select the variable containing the date of interviews, and the argument interval defines how back in time the function needs to select the observations.\nIf there are missing observations for the date of interviews, the function warns us that these observations are dropped.\n\nspi3_hh &lt;- select_by_dates(srvy_spi3_adm, \"2020-01-01\", \"2023-01-01\") |&gt;\n    select_by_interview(interview = end_date_n,\n                        interval = \"1 year\",\n                        wide = FALSE)\n\nMissing interview are dropped!\n\nspi3_hh\n\n# A tibble: 30,066 × 14\n   ID        hhid lat_cen long_cen  wave end_date_n   lon   lat ID_adm_div iso  \n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 2 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 3 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 4 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 5 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 6 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 7 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 8 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n 9 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n10 1      2.39e10    3.85    -54.2     2 2022-12-06 -54.2  3.85 55         SUR  \n# ℹ 30,056 more rows\n# ℹ 4 more variables: adm_div_1 &lt;chr&gt;, adm_div_2 &lt;chr&gt;, date &lt;chr&gt;, value &lt;dbl&gt;\n\n\n\n\n\n2.11 Save\nThe final step of the code is to save the result. In this case, we save it as a dta file using the haven::write_dta() function.\n\nhaven::write_dta(spi3_hh, file.path(path_to_result, \"spi_3_adm.dta\"))"
  },
  {
    "objectID": "spei_by_poly.html",
    "href": "spei_by_poly.html",
    "title": "5: Compute Standardize Evotranspiration Index based on spatial point",
    "section": "",
    "text": "1 Introduction\n  2 Code\n  \n  2.1 Set up\n  2.2 Read the data\n  2.3 Georeference the Surveys\n  2.4 Crop the spatial variables\n  2.5 Weather Variable Transformation\n  2.6 Extract\n  2.7 Compute SPI and SPEI\n  2.8 Merge with Survey\n  2.9 Save"
  },
  {
    "objectID": "spei_by_poly.html#introduction",
    "href": "spei_by_poly.html#introduction",
    "title": "5: Compute Standardize Evotranspiration Index based on spatial point",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we extract the CHIRPS precipitation observations in Suriname, based on the centroids of the Principal Sampling Units (PSU) of the Suriname Survey of Living Conditions for the years 2016/17 and 2022. Based on the extracted precipitation, the tutorial shows how to compute the Standardized Precipitation Index (SPI) and merge it with the survey based on the date of interviews."
  },
  {
    "objectID": "spei_by_poly.html#code",
    "href": "spei_by_poly.html#code",
    "title": "5: Compute Standardize Evotranspiration Index based on spatial point",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Set up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only climatic4economist package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\nlibrary(climatic4economist)\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note .. means one step back to the folder directory, i.e. one folder back.\nNote that how to set up the paths depends on your folder organization but there are overall two approaches:\n\nyou can use the R project, by opening the project directly you don’t need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder.\nyou can manually set the working folder with the function setwd().\n\n\n# path to data folder\n1path_to_data &lt;- file.path(\"..\",\n2                          \"..\", \"data\")\n\n# survey \npath_to_survey &lt;- file.path(path_to_data, \"survey\", \"AFR\")\n\n# administrative division\npath_to_adm_div &lt;- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre_monthly &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"monthly_pre\",\n                                 \"era5_monthly_avg_total_pre_50_25.nc\")\npath_to_evo_monthly &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"monthly_evo\",\n                                 \"era5_monthly_total_evo_50_25.nc\")\n# to result folder\npath_to_result &lt;- file.path(path_to_data, \"result\")\n\n\n1\n\nconcatenate the string to make a path\n\n2\n\n.. means one folder back\n\n\n\n\n\n\n\n2.2 Read the data\n\n2.2.1 Survey\nWe start by reading the surveys data. The surveys must have an id that uniquely identifies the household and the coordinates of their interviews.\nThe next steps are a bit convoluted. Lets split one by one.\n\nlist.files(path_to_survey, full.names = TRUE). The surveys are stored in two files. We use list.files() to list the files.\nlapply(haven::read_dta). We use the lapply() to apply to each of them the function haven::read_dta(). This last function actually read dta files into R. The result are two separate block of data, each for each file. They are two separated item within a list.\nAt this point we can bind the two separate data into a single one with the function dplyr::bind_rows(). We could have done it before but like this we ensured the two data sets have the same columns.\n\n\nsrvy &lt;- list.files(path_to_survey, pattern = \"dta$\", full.names = TRUE) |&gt;\n    lapply(haven::read_dta) |&gt;\n    dplyr::bind_rows() |&gt; \n    dplyr::filter(country != \"Uganda\") |&gt;\n    # dplyr::select(dplyr::any_of(c(\"country\",  \"year\", \"hhid\", \"lat\", \"lon\", \"interview_date\"))) |&gt;\n    dplyr::group_by(country) |&gt;\n    dplyr::mutate(interview_date = clock::date_parse(interview_date, format = \"%d/%m/%Y\"),\n                  survey_year = clock::get_year(median(interview_date, na.rm = TRUE)),\n                  survey = paste0(country, substr(survey_year, 3, 4)),\n                  survey = gsub(\" \", \"\", survey),\n                  .before = hhid)\n\nsurveys &lt;- srvy$survey |&gt; unique() |&gt; sort()\nsurveys\n\n[1] \"BurkinaFaso22\" \"Ethiopia19\"    \"Malawi19\"      \"Mali22\"       \n[5] \"Nigeria18\"     \"Tanzania21\"    \"Togo22\"       \n\n\nWe can keep all the surveys together but the size can be a challenge. Therefore, we split them with the function dplyr::group_split(). Now, each survey is a separate block, but they are all stored in the same list. We also distinguish the surveys that have spatial coordinates from the others.\n\nno_coord_srvy &lt;- c(\"Nigeria18\", \"Tanzania21\")\nsrvy_adm_cntry &lt;- srvy |&gt;\n    dplyr::filter(survey %in% no_coord_srvy) |&gt; \n    dplyr::select(-c(ea_id, TA_code, ea_code, \n                     zone_code, city_code, subcity_code)) |&gt;\n    dplyr::mutate(dplyr::across(.cols = dplyr::where(is.character),\n                                .fns = ~ifelse(.x == \"\", NA_character_, .x)),\n                  dplyr::across(.cols = dplyr::where(labelled::is.labelled),\n                                .fns = labelled::to_character)) |&gt;\n    dplyr::group_by(survey) |&gt;\n    dplyr::group_split() |&gt;\n    purrr::map(janitor::remove_empty,\n               which = \"cols\") |&gt;\n    setNames(no_coord_srvy) |&gt; \n  purrr::map(\\(x)  x |&gt; \n               dplyr::mutate(dplyr::across(.cols = dplyr::where(is.character),\n                                           .fns = ~ ifelse(.x == \"\" | grepl(\"CONFI\", .x),\n                                                           NA_character_, .x)) )) |&gt; \n  purrr::map(janitor::remove_empty, which = \"cols\")\n\nsrvy_adm_cntry\n\n$Nigeria18\n# A tibble: 40,183 × 17\n   survey_year hhid  country admin1     admin2 admin3   lat   lon interview_date\n         &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1        2018 10001 Nigeria South East Abia   Abia      NA    NA 2018-08-20    \n 2        2018 10002 Nigeria South East Abia   Abia      NA    NA 2018-08-21    \n 3        2018 10003 Nigeria South East Abia   Abia      NA    NA 2018-08-20    \n 4        2018 10004 Nigeria South East Abia   Abia      NA    NA 2018-08-21    \n 5        2018 10005 Nigeria South East Abia   Abia      NA    NA 2018-08-21    \n 6        2018 10008 Nigeria South East Abia   Abia      NA    NA 2018-08-21    \n 7        2018 10009 Nigeria South East Abia   Abia      NA    NA 2018-08-22    \n 8        2018 10010 Nigeria South East Abia   Abia      NA    NA 2018-08-20    \n 9        2018 10031 Nigeria South East Abia   Abia      NA    NA 2018-08-07    \n10        2018 10032 Nigeria South East Abia   Abia      NA    NA 2018-08-08    \n# ℹ 40,173 more rows\n# ℹ 8 more variables: survey &lt;chr&gt;, admin1_code &lt;chr&gt;, admin2_code &lt;chr&gt;,\n#   admin3_code &lt;chr&gt;, admin1_desc &lt;chr&gt;, admin2_desc &lt;chr&gt;, admin3_desc &lt;chr&gt;,\n#   year &lt;chr&gt;\n\n$Tanzania21\n# A tibble: 4,709 × 11\n   survey_year hhid        country interview_date survey region_name region_code\n         &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;date&gt;         &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;\n 1        2021 1000-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n 2        2021 1000-001-02 Tanzan… 2021-09-15     Tanza… arusha                2\n 3        2021 1000-001-03 Tanzan… 2021-09-15     Tanza… arusha                2\n 4        2021 1000-001-06 Tanzan… 2021-09-15     Tanza… arusha                2\n 5        2021 1001-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n 6        2021 1002-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n 7        2021 1003-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n 8        2021 1005-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n 9        2021 1006-001-01 Tanzan… 2021-09-15     Tanza… arusha                2\n10        2021 1006-001-03 Tanzan… 2021-09-15     Tanza… arusha                2\n# ℹ 4,699 more rows\n# ℹ 4 more variables: district_name &lt;chr&gt;, district_code &lt;dbl&gt;,\n#   ward_code &lt;dbl&gt;, village_code &lt;dbl&gt;\n\n\n\nys_coord_srvy &lt;- setdiff(surveys, no_coord_srvy)\n\nsrvy_coord_cntry &lt;- srvy |&gt;\n    dplyr::filter(survey != \"Nigeria18\") |&gt;\n    dplyr::filter(!is.na(lat) & lat &gt; -999999999) |&gt;\n    dplyr::filter(!(lat == 0.00000 & lon == 0.00000)) |&gt;\n    dplyr::select(c(survey, country, hhid, lat, lon, interview_date)) |&gt;\n    dplyr::group_by(survey) |&gt;\n    dplyr::group_split() |&gt;\n    setNames(ys_coord_srvy)\n\nsrvy_coord_cntry\n\n&lt;list_of&lt;\n  tbl_df&lt;\n    survey        : character\n    country       : character\n    hhid          : character\n    lat           : double\n    lon           : double\n    interview_date: date\n  &gt;\n&gt;[5]&gt;\n$BurkinaFaso22\n# A tibble: 3,227 × 6\n   survey        country      hhid     lat     lon interview_date\n   &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;        \n 1 BurkinaFaso22 Burkina Faso 002001  14.4 -0.231  2021-12-21    \n 2 BurkinaFaso22 Burkina Faso 002002  14.4 -0.231  2021-12-21    \n 3 BurkinaFaso22 Burkina Faso 002004  14.4 -0.231  2021-12-21    \n 4 BurkinaFaso22 Burkina Faso 002005  14.4 -0.231  2021-12-21    \n 5 BurkinaFaso22 Burkina Faso 002007  14.4 -0.231  2021-12-21    \n 6 BurkinaFaso22 Burkina Faso 002040  14.5 -0.234  2021-12-22    \n 7 BurkinaFaso22 Burkina Faso 003008  14.0 -0.0260 2022-06-29    \n 8 BurkinaFaso22 Burkina Faso 003019  12.4 -1.54   2022-07-25    \n 9 BurkinaFaso22 Burkina Faso 003021  14.0 -0.0320 2022-06-26    \n10 BurkinaFaso22 Burkina Faso 003027  14.0 -0.0260 2022-06-22    \n# ℹ 3,217 more rows\n\n$Ethiopia19\n# A tibble: 17,830 × 6\n   survey     country  hhid                 lat   lon interview_date\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 Ethiopia19 Ethiopia 010101088800910007  14.3  37.8 2019-06-19    \n 2 Ethiopia19 Ethiopia 010101088800910017  14.3  37.8 2019-06-18    \n 3 Ethiopia19 Ethiopia 010101088800910026  14.3  37.8 2019-06-18    \n 4 Ethiopia19 Ethiopia 010101088800910029  14.3  37.8 2019-06-17    \n 5 Ethiopia19 Ethiopia 010101088800910038  14.3  37.8 2019-06-07    \n 6 Ethiopia19 Ethiopia 010101088800910046  14.3  37.8 2019-06-04    \n 7 Ethiopia19 Ethiopia 010101088800910054  14.3  37.8 2019-06-05    \n 8 Ethiopia19 Ethiopia 010101088800910062  14.3  37.8 2019-06-08    \n 9 Ethiopia19 Ethiopia 010101088800910070  14.3  37.8 2019-06-17    \n10 Ethiopia19 Ethiopia 010101088800910082  14.3  37.8 2019-06-06    \n# ℹ 17,820 more rows\n\n$Malawi19\n# A tibble: 16,955 × 6\n   survey   country hhid           lat   lon interview_date\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 Malawi19 Malawi  101011000014 -9.70  33.2 2019-08-29    \n 2 Malawi19 Malawi  101011000023 -9.70  33.2 2019-08-29    \n 3 Malawi19 Malawi  101011000040 -9.70  33.2 2019-08-28    \n 4 Malawi19 Malawi  101011000071 -9.70  33.2 2019-08-29    \n 5 Malawi19 Malawi  101011000095 -9.70  33.2 2019-08-28    \n 6 Malawi19 Malawi  101011000115 -9.70  33.2 2019-08-29    \n 7 Malawi19 Malawi  101011000126 -9.70  33.2 2019-08-28    \n 8 Malawi19 Malawi  101011000135 -9.70  33.2 2019-08-29    \n 9 Malawi19 Malawi  101011000183 -9.70  33.2 2019-08-28    \n10 Malawi19 Malawi  101011000190 -9.70  33.2 2019-08-28    \n# ℹ 16,945 more rows\n\n$Mali22\n# A tibble: 6,119 × 6\n   survey country hhid    lat   lon interview_date\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 Mali22 Mali    00101  14.7 -12.2 2021-11-10    \n 2 Mali22 Mali    00102  14.7 -12.2 2021-11-10    \n 3 Mali22 Mali    00103  14.7 -12.2 2021-11-11    \n 4 Mali22 Mali    00104  14.7 -12.2 2021-11-11    \n 5 Mali22 Mali    00105  14.7 -12.2 2021-11-10    \n 6 Mali22 Mali    00106  14.7 -12.2 2021-11-10    \n 7 Mali22 Mali    00107  14.7 -12.2 2021-11-10    \n 8 Mali22 Mali    00108  14.7 -12.2 2021-11-10    \n 9 Mali22 Mali    00109  14.7 -12.2 2021-11-10    \n10 Mali22 Mali    00110  14.7 -12.2 2021-11-12    \n# ℹ 6,109 more rows\n\n$Togo22\n# A tibble: 6,462 × 6\n   survey country hhid     lat   lon interview_date\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 Togo22 Togo    001002  6.16  1.23 2021-11-13    \n 2 Togo22 Togo    001004  6.16  1.23 2021-11-14    \n 3 Togo22 Togo    001005  6.16  1.23 2021-11-15    \n 4 Togo22 Togo    001006  6.16  1.23 2021-11-15    \n 5 Togo22 Togo    001007  6.16  1.23 2021-11-14    \n 6 Togo22 Togo    001008  6.16  1.23 2021-11-13    \n 7 Togo22 Togo    001009  6.16  1.23 2021-11-14    \n 8 Togo22 Togo    001010  6.16  1.23 2021-11-15    \n 9 Togo22 Togo    001011  6.16  1.23 2021-11-15    \n10 Togo22 Togo    001013  6.16  1.23 2021-11-15    \n# ℹ 6,452 more rows\n\n\n\n\n2.2.2 Weather data\nTo weather data is stored as nc files. We can read them with the function terra::rast().\nNote how all the data sets have the same coordinate reference system (CRS), i.e. EPSG:4326. This is important because in this way all the data can “spatially” talk to each other.\n\ntpre &lt;- terra::rast(path_to_pre_monthly)\nnames(tpre) &lt;- terra::names(tpre) |&gt; second_to_date()\n\ntevo &lt;- terra::rast(path_to_evo_monthly)\nnames(tevo) &lt;- terra::names(tevo) |&gt; second_to_date()\n\n\n\n2.2.3 Administrative Boundaries\nWe now move to read the administrative divisions. We use the function read_adm_div() to do so. This function looks for spatial polygons for the iso and lvl provided provided.\nEven if we have the coordinates from the survey, we will extract some spatial variables at the administrative division.\n\nadm_div &lt;- read_geoBoundaries(path_to_adm_div,\n                              iso = c(\"BFA\", \"ETH\", \"MLI\", \"MWI\", \"TZA\", \"TGO\"),\n                              lvl = 2) |&gt;\n  setNames(c(\"BurkinaFaso22\", \"Ethiopia19\", \"Mali22\", \"Malawi19\",\n             \"Togo22\", \"Tanzania21\"))\n\nNigeria &lt;- read_geoBoundaries(path_to_adm_div,\n                              iso = c(\"NGA\"),\n                              lvl = 1)\nadm_div$Nigeria18 &lt;- Nigeria\n\nadm_div &lt;- adm_div[sort(names(adm_div))]\n\nadm_div$BurkinaFaso22\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 45, 4  (geometries, attributes)\n extent      : -5.51892, 2.4054, 9.40111, 15.08259  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_adm_div   iso         adm_div_1 adm_div_2\n type        :      &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;\n values      :          1   BFA Boucle du Mouhoun      Bale\n                        2   BFA Boucle du Mouhoun   Mouhoun\n                        3   BFA Boucle du Mouhoun    Nayala\n\n\n\n\n\n2.3 Georeference the Surveys\n\nsrvy_coord_geo &lt;- srvy_coord_cntry |&gt; \n  list(purrr::keep_at(adm_div, ys_coord_srvy), names(srvy_coord_cntry)) |&gt;\n  purrr::pmap(get_poly_attr_for_point)\n\n\nsrvy_adm_geo &lt;- srvy_adm_cntry\nsrvy_adm_geo$Nigeria18 &lt;- srvy_adm_cntry$Nigeria18 |&gt; \n  dplyr::mutate(adm_div_1 = stringr::str_to_title(admin1),\n                adm_div_1 = dplyr::case_when(\n                  adm_div_1 == \"Fct\" ~ \"Abuja Federal Capital Territory\",\n                  .default = adm_div_1))\n\nsrvy_adm_geo$Tanzania21 &lt;- srvy_adm_cntry$Tanzania21 |&gt; \n  dplyr::mutate(adm_div_1 = stringr::str_to_title(region_name),\n                adm_div_2 = stringr::str_to_title(district_name),\n                \n                adm_div_1 = dplyr::case_when(\n                  adm_div_1 == \"Dar Es Salaam\" ~ \"Dar es Salaam\",\n                  adm_div_1 == \"Kaskazini Pemba\" ~ \"North Pemba\",\n                  adm_div_1 == \"Kaskazini Unguja\" ~ \"Zanzibar North\",\n                  adm_div_1 == \"Kusini Pemba\" ~ \"South Pemba\",\n                  adm_div_1 == \"Kusini Unguja\" ~ \"Zanzibar South & Central\",\n                  adm_div_1 == \"Mjini Magharibi Unguja\" ~ \"Zanzibar Urban/West\",\n                  # Songwe was created the 2016 from the western half of Mbeya Region\n                  adm_div_1 == \"Songwe\" ~ \"Mbeya\", \n                  \n                  # wrong adm_div based on district\n                  adm_div_2 == \"Masasi Rural\" ~ \"Mtwara\", \n                  adm_div_2 == \"Kigamboni\" ~ \"Dar es Salaam\", \n                  grepl(\"Kahama\", adm_div_2) & adm_div_1 == \"Katavi\" ~ \"Shinyanga\",  # !!!\n                  adm_div_2 == \"Chakechake\" ~ \"South Pemba\", \n                  adm_div_2 == \"Kinondoni\" ~ \"Dar es Salaam\", \n                  adm_div_2 == \"Songea\" ~ \"Ruvuma\",\n                  grepl(\"Songea\", adm_div_2) ~ \"Ruvuma\",\n              \n                  adm_div_2 == \"Ngorongoro\" ~ \"Arusha\", \n                  adm_div_2 == \"Wete\" ~ \"North Pemba\", \n                  adm_div_2 == \"Tandahimba\" ~ \"Mtwara\", \n                  adm_div_2 == \"Babati\" ~ \"Manyara\",\n                  grepl(\"Baba\", adm_div_2) ~ \"Manyara\", # !!!!\n                  grepl(\"Nzega\", adm_div_2) ~ \"Tabora\",\n                  adm_div_2 == \"Makete\" ~ \"Njombe\",\n                  adm_div_2 == \"Mbeya Urban\" ~ \"Mbeya\",\n                  .default = adm_div_1),\n                \n                adm_div_2 = dplyr::case_when(\n                  # adm_div_1  Arusha\n                  adm_div_2 == \"Arusha Rural\" ~ \"Arusha\",\n                  # adm_div_1  Shinyanga\n                  adm_div_2 == \"Kahama Rural\" ~ \"Kahama\",\n                  adm_div_2 == \"Kahama Town\" ~ \"Kahama Township Authority\",\n                  adm_div_2 == \"Shinyanga Rural\" ~ \"Shinyanga\",\n                  adm_div_2 == \"Nzega Town\" ~ \"Nzega\",\n                  # in 2012 by splitting the Kahama District into Msalala and Ushetu\n                  adm_div_2 == \"Msalala\" ~ \"Kahama\",\n                  adm_div_2 == \"Ushetu\" ~ \"Kahama\",\n                  # adm_div_1  Katavi\n                  adm_div_2 == \"Mpanda Rural\" ~ \"Mpanda\",\n                  # established in 2012 from Mlele\n                  adm_div_2 == \"Mpimbwe\" ~ \"Mlele\",\n                  adm_div_2 == \"Nsimbo\" ~ \"Mlele\",\n                  # adm_div_1  Singida\n                  adm_div_2 == \"Singida Rural\" ~ \"Singida\",\n                  # established in 2015 from Manyoni\n                  adm_div_2 == \"Itigi\" ~ \"Manyoni\",\n                  # adm_div_1 Dar es Salaam\n                  # In 2015 Temeke was divided into Temeke and Kigamboni\n                  adm_div_2 == \"Kigamboni\" ~ \"Temeke\",\n                  # Kinondoni should be onlyu the eastern part\n                  adm_div_2 == \"Ubungo\" ~ \"Kinondoni\",\n                  # adm_div_1 Kagera\n                  adm_div_2 == \"Bukoba Rural\" ~ \"Bukoba\",\n                  # adm_div_1 Dodoma\n                  adm_div_2 == \"Kondoa Urban\" ~ \"Kondoa\",\n                  # adm_div_1 Mbeya\n                  adm_div_2 == \"Mbeya Rural\" ~ \"Mbeya\",\n                  adm_div_2 == \"Mbalali\" ~ \"Mbarali\",\n                  # created in 2013, not perfect geo match\n                  adm_div_2 == \"Busokelo\" ~ \"Makete\",\n                  # adm_div_1 Morogoro\n                  adm_div_2 == \"Ifakara Urban\" ~ \"Kilombero\",\n                  adm_div_2 == \"Morogoro Rural\" ~ \"Morogoro\",\n                  # it should be the western part of Ulanga\n                  adm_div_2 == \"Malinyi\" ~ \"Ulanga\",\n                  # adm_div_1 Kigoma\n                  adm_div_2 == \"Kasulu Rural\" ~ \"Kasulu\",\n                  adm_div_2 == \"Kasulu Town\" ~ \"Kasulu Township Authority\",\n                  adm_div_2 == \"Kigoma Rural\" ~ \"Kigoma\",\n                  adm_div_2 == \"Kigoma Ujiji Urban\" ~ \"Kigoma  Urban\",\n                  # adm_div_1 Mtwara\n                  adm_div_2 == \"Masasi Rural\" ~ \"Masasi\",\n                  adm_div_2 == \"Masasi Urban\" ~ \"Masasi  Township Authority\",\n                  adm_div_2 == \"Mtwara Mikindani\" ~ \"Mtwara Urban\",\n                  adm_div_2 == \"Mtwara Rural\" ~ \"Mtwara\",\n                  # adm_div_1 Geita\n                  adm_div_2 == \"Geita Town\" ~ \"Geita\",\n                  # adm_div_1 Mwanza\n                  adm_div_2 == \"Mwanza Urban\" ~ \"Nyamagana\",\n                  # created in 2015, from the eastern part of Sengerema\n                  adm_div_2 == \"Buchosa\" ~ \"Sengerema\",\n                  # adm_div_1 Iringa\n                  adm_div_2 == \"Iringa Rural\" ~ \"Iringa\",\n                  adm_div_2 == \"Mafinga Town\" ~ \"Mafinga Township Authority\",\n                  # adm_div_1 Njombe\n                  adm_div_2 == \"Makambako Town\" ~ \"Makambako Township Authority\",\n                  adm_div_2 == \"Mbeya Urban\" ~ \"Mbeya\",\n                  adm_div_2 == \"Njombe Rural\" ~ \"Njombe\",\n                  adm_div_2 == \"Njombe Town\" ~ \"Njombe Urban\",\n                  # adm_div_1 South Pemba\n                  adm_div_2 == \"Chakechake\" ~ \"Chake Chake\",\n                  # adm_div_1 Lindi\n                  adm_div_2 == \"Lindi Rural\" ~ \"Lindi\",\n                  # adm_div_1 Manyara\n                  adm_div_2 == \"Babati Rural\" ~ \"Babati\",\n                  adm_div_2 == \"Babati Town\" ~ \"Babati UrbanBabati Urban\",\n                  adm_div_2 == \"Mbulu Town\" ~ \"Mbulu\",\n                  # adm_div_1 Mara\n                  adm_div_2 == \"Butiama\" ~ \"Babati\",\n                  adm_div_2 == \"Musoma Rural\" ~ \"Musoma\",\n                  # adm_div_1 Ruvuma\n                  adm_div_2 == \"Songea Rural\" ~ \"Songea\",\n                  # not clear\n                  adm_div_2 == \"Madaba\" ~ \"Songea\",\n                  # adm_div_1 Simiyu\n                  adm_div_2 == \"Bariadi Town\" ~ \"Bariadi\",\n                  # adm_div_1 Rukwa\n                  adm_div_2 == \"Sumbawanga Rural\" ~ \"Sumbawanga\",\n                  # adm_div_1 Pwani\n                  adm_div_2 == \"Kibaha Rural\" ~ \"Kibaha\",\n                  # Should be the eastern part of Rufiji\n                  adm_div_2 == \"Kibiti\" ~ \"Rufiji\",\n                  # created in 2016 from northen part of Bagamoyo \n                  adm_div_2 == \"Chalinze\" ~ \"Bagamoyo\",\n                  # adm_div_1 Tanga\n                  adm_div_2 == \"Korogwe Rural\" ~ \"Korogwe\",\n                  # Created in 2013 from Lushoto\n                  adm_div_2 == \"Bumbuli\" ~ \"Lushoto\",\n                  # adm_div_1 Kilimanjaro\n                  adm_div_2 == \"Moshi Rural\" ~ \"Moshi\",\n                  .default = adm_div_2))\n\n\n\n2.4 Crop the spatial variables\nThe spatial variables variables we have just load have a global coverage. It might be convenient to reduce the coverage to just the countries we are interested in. We can do this by using the terra::crop() function and the administrative divisions.\n\ntpre_cntry &lt;- purrr::map(adm_div, \n                         crop_with_buffer,\n                         raster = tpre,\n                         buffer = 1)\n\ntevo_cntry &lt;- purrr::map(adm_div, \n                         crop_with_buffer,\n                         raster = tevo,\n                         buffer = 1)\n\n\n\n2.5 Weather Variable Transformation\n\n# From meter to millimeters\npre_cntry_mm &lt;- purrr::map(tpre_cntry, ~ .x*1000)\n\ntevo_cntry_mm &lt;- purrr::map(tevo_cntry, ~ .x*1000)\n\nwb_cntry_mm &lt;- purrr::map2(pre_cntry_mm, tevo_cntry_mm, \\(x, y) x + y)\n\n\n\n2.6 Extract\nFor the extraction of the weather variables, we use the function extract_cell_by_poly(). This function doesn’t aggregate the values within the polygons but extract all the cell values within the polygon separately. This is useful for us as we want to compute the SPI and SPEI for each cell and only later aggregate at the polygon level.\n\ntpre_adm &lt;- purrr::map2(pre_cntry_mm,\n                        adm_div,\n                        extract_cell_by_poly)\n\nwb_adm &lt;- purrr::map2(wb_cntry_mm,\n                      adm_div,\n                      extract_cell_by_poly)\n\n\n\n2.7 Compute SPI and SPEI\nWe now compute the SPI and the SPEI with the function compute_spei() and compute_spi(). These functions requires the water balance and the precipitation time series for each location and the time scale at which the indices are computed.\nTo compute the SPEI, it is recommended to use at least 30 years of observation to ensure a good estimation of the parameters. More years can strength the estimation but the results can be affected by climate change: if there have been a change in the climate parameters, old observations might be not indicative of the current situation affecting the estimation. There are no clear rule on this, so we leave add the possibility to select the time range of observation with the function select_by_dates(). The function requires both or just one between the starting date, from, and the end date to. If both are provide the the function select between the two dates, if only from is provided the function selects all date after, and if only to is provided the function selects all date before.\nLooking at the result, we see first is the ID column, that we will use to merge back with the survey. The other columns contain the SPEI observations over time specific to each coordinate.\n\nspei6 &lt;- purrr::map2(wb_adm,\n                     names(wb_adm),\n                     compute_spei,\n                     time_scale = 6) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPEI: BurkinaFaso22 \nComputing SPEI: Ethiopia19 \nComputing SPEI: Malawi19 \nComputing SPEI: Mali22 \nComputing SPEI: Nigeria18 \nComputing SPEI: Tanzania21 \nComputing SPEI: Togo22 \n\nspei3 &lt;- purrr::map2(wb_adm,\n                     names(wb_adm),\n                     compute_spei,\n                     time_scale = 3) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPEI: BurkinaFaso22 \nComputing SPEI: Ethiopia19 \nComputing SPEI: Malawi19 \nComputing SPEI: Mali22 \nComputing SPEI: Nigeria18 \nComputing SPEI: Tanzania21 \nComputing SPEI: Togo22 \n\nspei1 &lt;- purrr::map2(wb_adm,\n                     names(wb_adm),\n                     compute_spei,\n                     time_scale = 1) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPEI: BurkinaFaso22 \nComputing SPEI: Ethiopia19 \nComputing SPEI: Malawi19 \nComputing SPEI: Mali22 \nComputing SPEI: Nigeria18 \nComputing SPEI: Tanzania21 \nComputing SPEI: Togo22 \n\n\n\nspi6 &lt;- purrr::map2(tpre_adm,\n                    names(tpre_adm),\n                    compute_spi,\n                    time_scale = 6) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPI: BurkinaFaso22 \nComputing SPI: Ethiopia19 \nComputing SPI: Malawi19 \nComputing SPI: Mali22 \nComputing SPI: Nigeria18 \nComputing SPI: Tanzania21 \nComputing SPI: Togo22 \n\nspi3 &lt;- purrr::map2(tpre_adm,\n                    names(tpre_adm),\n                    compute_spi,\n                    time_scale = 3) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPI: BurkinaFaso22 \nComputing SPI: Ethiopia19 \nComputing SPI: Malawi19 \nComputing SPI: Mali22 \nComputing SPI: Nigeria18 \nComputing SPI: Tanzania21 \nComputing SPI: Togo22 \n\nspi1 &lt;- purrr::map2(tpre_adm,\n                    names(tpre_adm),\n                    compute_spi,\n                    time_scale = 1) |&gt;\n  purrr::map(agg_to_adm_div,\n             match_col = \"^X[0-9]\")\n\nComputing SPI: BurkinaFaso22 \nComputing SPI: Ethiopia19 \nComputing SPI: Malawi19 \nComputing SPI: Mali22 \nComputing SPI: Nigeria18 \nComputing SPI: Tanzania21 \nComputing SPI: Togo22 \n\n\nWe have computed the standardized indicators for each cells but we need additional information to assign each cells to the administrative divisions. We take advantage of the column ID_adm_div to merge the standardized indices with the administrative divisions.\n\nspei1_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spei1, merge_by_common)\n\nspei3_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spei3, merge_by_common)\n\nspei6_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spei6, merge_by_common)\n\n\nspi1_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spi1, merge_by_common)\n\nspi3_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spi3, merge_by_common)\n\nspi6_adm &lt;- purrr::map(adm_div, terra::values) |&gt;\n  purrr::map2(spi6, merge_by_common)\n\n\n\n2.8 Merge with Survey\n\nsrvy_all_geo &lt;- c(srvy_adm_geo, srvy_coord_geo)\nsrvy_all_geo &lt;- srvy_all_geo[sort(names(srvy_all_geo))]\n\nspi1_hh &lt;- purrr::map2(srvy_all_geo,\n                       spi1_adm,\n                       merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\nspi3_hh &lt;- purrr::map2(srvy_all_geo,\n                       spi3_adm,\n                       merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\nspi6_hh &lt;- purrr::map2(srvy_all_geo,\n                       spi6_adm,\n                       merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\nspei1_hh &lt;- purrr::map2(srvy_all_geo,\n                        spei1_adm,\n                        merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\nspei3_hh &lt;- purrr::map2(srvy_all_geo,\n                        spei3_adm,\n                        merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\nspei6_hh &lt;- purrr::map2(srvy_all_geo,\n                        spei6_adm,\n                        merge_by_common) |&gt;\n    purrr::map(select_by_interview,\n               interview = interview_date,\n               interval = \"2 year\",\n               wide = FALSE)\n\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\nMissing interview are dropped!\n\n\n\n\n2.9 Save\nThe final step of the code is to save the result. In this case, we save it as a dta file using the haven::write_dta() function.\n\nspei6_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spei6.dta\"))\nspei3_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spei3.dta\"))\nspei1_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spei1.dta\"))\n\n\nspi6_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spi6.dta\"))\nspi3_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spi3.dta\"))\nspi1_hh |&gt;\n    dplyr::bind_rows() |&gt;\n    haven::write_dta(file.path(path_to_result, \"spi1.dta\"))"
  },
  {
    "objectID": "extract_by_poly.html",
    "href": "extract_by_poly.html",
    "title": "2: Extract spatial data based on spatial polygons",
    "section": "",
    "text": "1 Introduction\n  2 Code\n  \n  2.1 Set Up\n  2.2 Read the Data\n  2.3 Georeference the Surveys\n  2.4 Merge administrative division and survey\n  2.5 Crop the spatial variables\n  2.6 Plot\n  2.7 Modify the Spatial Variables\n  2.8 Extraction\n  2.9 Cmpute Long Run Climatic Parameter\n  2.10 Merge with Survey\n  2.11 Write\n  \n  3 Take home messages\n  4 Appendix\n  \n  4.1 Want to know about the data?"
  },
  {
    "objectID": "extract_by_poly.html#introduction",
    "href": "extract_by_poly.html#introduction",
    "title": "2: Extract spatial data based on spatial polygons",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis tutorial show how too extract spatial control variables based on surveys administrative divisions. The survey refers to Ethiopia 2019 and comes from the The World Bank Living Standards Measurement Study (LSMS). The spatial variables are nighttime light, agroecological zones, Urban-Rural Catchment Area, elevation, and climatic parameters. More information about these datasets are in the Appendix."
  },
  {
    "objectID": "extract_by_poly.html#code",
    "href": "extract_by_poly.html#code",
    "title": "2: Extract spatial data based on spatial polygons",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Set Up\nWe start by setting up the stage for our analysis. First, we load the necessary packages. We load only climatic4economist package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\nlibrary(climatic4economist)\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note .. means one step back to the folder directory, i.e. one folder back.\nNote that how to set up the paths depends on your folder organization but there are overall two approaches:\n\nyou can use the R project, by opening the project directly you don’t need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder.\nyou can manually set the working folder with the function setwd().\n\n\n# path to data folder\n1path_to_data &lt;- file.path(\"..\",\n2                          \"..\", \"data\")\n\n# survey and administrative division\npath_to_survey  &lt;- file.path(path_to_data, \"survey\", \"LSMS\", \"LSMS_ETH19.dta\")\npath_to_adm_div &lt;- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tpr.nc\")\npath_to_tmp &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tmp.nc\")\n\n# control variables\npath_to_elevation  &lt;- file.path(path_to_data, \"spatial\", \"elevation\", \"GloFAS\",\n                               \"elevation_glofas_v4_0.nc\")\npath_to_urca       &lt;- file.path(path_to_data, \"spatial\", \"URCA\", \n                               \"URCA.tif\")\npath_to_pop        &lt;- file.path(path_to_data, \"spatial\", \"population\", \"WorldPop\",\n                               \"uncontraint_1km_global\", \"ppp_2019_1km_Aggregated.tif\")\npath_to_nightlight &lt;- file.path(path_to_data, \"spatial\", \"nighttime_light\",\n                                \"VIIRS\", \"VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif\")\npath_to_aez        &lt;- file.path(path_to_data, \"spatial\", \"AgroEcological\", \"AEZ\", \n                                \"GAEZv5\",  \"GAEZ-V5.AEZ33-10km.tif\")\n\n# to result folder\npath_to_result &lt;- file.path(path_to_data, \"result\")\n\n\n1\n\nconcatenate the string to make a path\n\n2\n\n.. means one folder back\n\n\n\n\n\n\n\n2.2 Read the Data\n\n2.2.1 Survey Data\nWe start by reading the surveys data. The survey is stored as dta file, so we use the haven::read_dta() function to read it.\nWe only need the hhid, the survey coordinates, and the interview dates. We use dplyr::select() to choose these variables. This passage is optional and we bring with us all the variables, but we won’t use them.\nThen we create/modify some variables with the function dplyr::mutate(). We transform the the variable interview_date from string into data, and we get the year of the median value of the date of interviews. This passage is important as it allows us to define the most appropriate year to select for the spatial variables.\n\n1srvy &lt;- haven::read_dta(path_to_survey) |&gt;\n2    dplyr::select(survey_year, hhid, country, lat, lon, interview_date) |&gt;\n    dplyr::mutate(\n3        interview_date = clock::date_parse(interview_date,\n4                                           format = \"%Y-%m-%d\"),\n5        survey_year    = clock::get_year(median(interview_date)),\n        .before = hhid)\n\n\n1\n\nread dta type data\n\n2\n\nselect relevant variables\n\n3\n\ntransform string into date type\n\n4\n\nspecify format type\n\n5\n\nfind the median year of the interviews\n\n\n\n\n\n\n2.2.2 Spatial Data\nFinally, we load the spatial data. This data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or “cell” in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region.\nTo spatial data is usually stored as tif file or nc. We can read both of them them with the function terra::rast().\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS), i.e. EPSG:4326.\n\n\n\n\n\n\nImportant\n\n\n\nWhen working with multiple spatial data, you must ensure that they have the same coordinate reference system (CRS). This is important because in this way all the data can “spatially” talk to each other.\n\n\n\n1pop &lt;- terra::rast(path_to_pop) |&gt;\n2  setNames(\"pop\")\npop\n\nnightlight &lt;- terra::rast(path_to_nightlight) |&gt;\n  setNames(\"nightlight\")\nnightlight\n\nelevation &lt;- terra::rast(path_to_elevation)\nelevation\n\nurca &lt;- terra::rast(path_to_urca)\nurca\n\naez &lt;- terra::rast(path_to_aez) |&gt;\n    setNames(\"aez\") \naez\n\n\n1\n\nread raster type data\n\n2\n\nchange the name of the layer\n\n\n\n\nclass       : SpatRaster \ndimensions  : 18720, 43200, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : -180.0012, 179.9987, -72.00042, 83.99958  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : ppp_2019_1km_Aggregated.tif \nname        : pop \nclass       : SpatRaster \ndimensions  : 33601, 86401, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : -180.0021, 180.0021, -65.00208, 75.00208  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif \nname        : nightlight \nclass       : SpatRaster \ndimensions  : 3000, 7200, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : elevation_glofas_v4_0.nc \nvarname     : elevation (Height above sea level) \nname        : elevation \nunit        :         m \nclass       : SpatRaster \ndimensions  : 17235, 43200, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : -180, 180, -60, 83.625  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : URCA.tif \nname        : URCA \nclass       : SpatRaster \ndimensions  : 2160, 4320, 1  (nrow, ncol, nlyr)\nresolution  : 0.08333333, 0.08333333  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : GAEZ-V5.AEZ33-10km.tif \nname        : aez \n\n\nNow we also read the weather observation. The same consideration about the coordinate reference system (CRS) is still valid. When we work with raster that have also observations over time, it is important to check how and where the time and date information is stored. Sometimes it is stored in the metadata and you can access it using terra::time(), other time it is already saved as the name of the layer and you can access it using names(). Sometimes, like in this case the date information is stored in the names but the format is based on second passed from 1970-01-01 00:00. To transform this observation into readable date we can use the function second_to_date().\n\n\n\n\n\n\nWarning\n\n\n\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n\n\n\npre &lt;- terra::rast(path_to_pre)\npre\n1names(pre) &lt;- terra::names(pre) |&gt; second_to_date()\npre\n\ntmp &lt;- terra::rast(path_to_tmp)\nnames(tmp) &lt;- terra::names(tmp) |&gt; second_to_date()\n\n\n1\n\ntransform the layers name with second into dates\n\n\n\n\nclass       : SpatRaster \ndimensions  : 741, 811, 904  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : -26.05, 55.05, -36.05, 38.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : afr_month_50_25_tpr.nc:tp \nvarname     : tp (Total precipitation) \nnames       : tp_va~52000, tp_va~73600, tp_va~54400, tp_va~76000, tp_va~84000, tp_va~05600, ... \nunit        :           m,           m,           m,           m,           m,           m, ... \nclass       : SpatRaster \ndimensions  : 741, 811, 904  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : -26.05, 55.05, -36.05, 38.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : afr_month_50_25_tpr.nc:tp \nvarname     : tp (Total precipitation) \nnames       : 1950-01-01, 1950-02-01, 1950-03-01, 1950-04-01, 1950-05-01, 1950-06-01, ... \nunit        :          m,          m,          m,          m,          m,          m, ... \n\n\n\n\n2.2.3 Administrative Boundaries\nWe now move to read the administrative divisions. We use the function read_geoBoundaries() to do so. This function looks for spatial polygons for the iso and lvl provided provided.\nAs we have the coordinates, we don’t actually need the administrative divisions for the extraction. However, we will use it to reduce the coverage of the spatial variables and to make some plots.\nThe same consideration about the coordinate reference system (CRS) is still valid.\n\nadm_div &lt;- read_geoBoundaries(path_to_adm_div, iso = \"ETH\", lvl = 2)\nadm_div\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 74, 4  (geometries, attributes)\n extent      : 33.00224, 47.95925, 3.400365, 14.84602  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_adm_div   iso adm_div_1 adm_div_2\n type        :      &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n values      :          1   ETH    Somali     Afder\n                        2   ETH   Gambela    Agnuak\n                        3   ETH     SNNPR     Alaba\n\n\n\n\n\n\n2.3 Georeference the Surveys\nAs we’ve mentioned, the spatial data is georeferenced, so we need to ensure the same for the survey data. We use the spatial coordinates to assign the administrative division to each household. We must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called ID.\nThis is handled by the prepare_coord() function, which requires the coordinates’ variable names as input.\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the georef_coord() function requires the coordinates’ variable names as input. Usually, the WGS 84 CRS is the default coordinate references system for coordinates. In this case it matches the weather coordinate references system.\nWe can print the result to check the transformation. The new column, ID, is created by prepare_coord() and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\nsrvy_coord &lt;- prepare_coord(srvy,\n                            lon_var = lon,\n                            lat_var = lat)\nsrvy_coord\n\n# A tibble: 6,505 × 7\n   ID    survey_year hhid               country    lat   lon interview_date\n   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 1            2019 051103088801903002 Ethiopia  3.61  39.0 2019-08-28    \n 2 1            2019 051103088801903012 Ethiopia  3.61  39.0 2019-08-28    \n 3 1            2019 051103088801903022 Ethiopia  3.61  39.0 2019-08-28    \n 4 1            2019 051103088801903032 Ethiopia  3.61  39.0 2019-08-29    \n 5 1            2019 051103088801903042 Ethiopia  3.61  39.0 2019-08-29    \n 6 1            2019 051103088801903052 Ethiopia  3.61  39.0 2019-08-28    \n 7 1            2019 051103088801903062 Ethiopia  3.61  39.0 2019-08-28    \n 8 1            2019 051103088801903072 Ethiopia  3.61  39.0 2019-08-28    \n 9 1            2019 051103088801903082 Ethiopia  3.61  39.0 2019-08-28    \n10 1            2019 051103088801903092 Ethiopia  3.61  39.0 2019-08-29    \n# ℹ 6,495 more rows\n\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The function also the coordinates’ variable names as input.\n\nsrvy_geo &lt;- georef_coord(srvy_coord,\n                         geom = c(\"lon\", \"lat\"),\n                         crs = \"EPSG:4326\")\nsrvy_geo\n\n class       : SpatVector \n geometry    : points \n dimensions  : 516, 1  (geometries, attributes)\n extent      : 33.43483, 47.30784, 3.609384, 14.47715  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    ID\n type        : &lt;chr&gt;\n values      :     1\n                   2\n                   3\n\n\n\n\n\n2.4 Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\nsrvy_adm_div &lt;- get_poly_attr_for_point(srvy_geo, adm_div)\nsrvy_adm_div\n\n# A tibble: 516 × 7\n   ID      lon   lat ID_adm_div iso   adm_div_1 adm_div_2\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 1      39.0  3.61 10         ETH   Oromia    Borena   \n 2 2      41.8  4.01 37         ETH   Somali    Liben    \n 3 3      41.9  4.44 1          ETH   Somali    Afder    \n 4 4      41.5  4.73 37         ETH   Somali    Liben    \n 5 5      36.0  4.74 56         ETH   SNNPR     South Omo\n 6 6      38.1  4.85 10         ETH   Oromia    Borena   \n 7 7      37.4  4.96 10         ETH   Oromia    Borena   \n 8 8      40.7  5.11 37         ETH   Somali    Liben    \n 9 9      41.9  5.15 1          ETH   Somali    Afder    \n10 10     44.6  5.24 1          ETH   Somali    Afder    \n# ℹ 506 more rows\n\n\n\n\n\n2.5 Crop the spatial variables\nThe spatial variables variables we have just load have a global coverage. It might be convenient to reduce the coverage to just the countries we are interested in. We can do this by using the crop_with_buffer() function and the administrative divisions. As the name suggest, this function allows to specify a buffer around the vector data to increase the spatial extent and crop a larger portion. This is useful as some survey coordinates are at the edge of the administrative borders or, in some rare cases, just outside the borders as consequence of the coordinates modification fro location anonymization. Further, to compute some spatial indicators in one cell we need the surrounding cell values and if we crop exactly at the borders those cell values at the edge won’t have the he surrounding cells.\nThe buffer argument of the function specifies the increase around the spatial extent. By default, it is in the same unit of measure of the data.\nThis is not a compulsory step but it reduce the memory burden and allows for more meaningful plotting.\n\npop_cntry &lt;- crop_with_buffer(pop, adm_div, buffer = 1)\n\nnghtlght_cntry &lt;- crop_with_buffer(nightlight, adm_div, buffer = 1)\n\nelevatn_cntry &lt;- crop_with_buffer(elevation, adm_div, buffer = 1)\n\nurca_cntry &lt;- crop_with_buffer(urca, adm_div, buffer = 1)\n\naez_cntry &lt;- crop_with_buffer(aez, adm_div, buffer = 1)\n\npre_cntry &lt;- crop_with_buffer(pre, adm_div, buffer = 1)\n\ntmp_cntry &lt;- crop_with_buffer(tmp, adm_div, buffer = 1)\n\n\n\n\n2.6 Plot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n1terra::plot(adm_div, col = \"grey\", main = \"District of Ethiopia and Survey Coordinates\")\n2terra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n1\n\nplot raster\n\n2\n\nadd survey locations\n\n\n\n\n\n\n\n\n\n\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the major cities and in the North.\nNext, we plot the spatial variables to see how it overlaps with the spatial coordinates.\n\n1terra::plot(elevatn_cntry, main = \"Elevation\")\n2terra::lines(adm_div, col = \"white\", lwd = 1)\n3terra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n1\n\nplot raster\n\n2\n\nadd administrative borders\n\n3\n\nadd survey locations\n\n\n\n\n\n\n\n\n\n\nterra::plot(log(1+pop_cntry), main = \"Log Population\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\nterra::plot(urca_cntry, main = \"URCA\")\nterra::lines(adm_div, col = \"black\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\n\n\n\n\n\n\nterra::plot(log(1+nghtlght_cntry), main = \"Log Nighttime Light\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\n\n\n\n\n\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"ryb\"),\n            main = \"Monthly temperature at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"black\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the different spatial resolution, with precipitation having a lower one. The consequence is that some survey coordinates still fall within the same cell.\n\n\n\n2.7 Modify the Spatial Variables\n\n2.7.1 Compute Terrain Indicators\nNow we compute some terrain indicators based on elevation. The terrain indicators are:\n\nTRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and its 8 surrounding cells.\nSlope is the average difference between the value of a cell and its 8 surrounding cells.\nRoughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells.\n\n\nterrain_cntry &lt;-  terra::terrain(elevatn_cntry,\n1                                 v = c(\"slope\", \"TRI\", \"roughness\"),\n2                                 neighbors = 8,\n                                 unit = \"degrees\")\n\nnghtlght_cntry$ln_nightlight &lt;- log(1 + nghtlght_cntry)\n\n\n1\n\nthe terrain indicators\n\n2\n\nhow many neighboring cells, 8 (queen case) or 4 (rook case)\n\n\n\n\n\n\n2.7.2 Weather Variable Transformation\nThe original unit of measure of the weather data is in meter for precipitation and Kelvin for temperature. These unit of measure are not very intuitive, therefore we change them into millimeter and Celsius respectively.\n\n# From meter to millimeters\npre_cntry_mm &lt;- pre_cntry*1000\n\n# From Kelvin to Celsius\ntmp_cntry_c &lt;- tmp_cntry - 273.15\n\n\n\n\n2.7.3 Compute the Surface Area of the Administrative Divisons\nWe now compute the surface are of each administrative division. We will use it for computing the population density. We use the function dplyr::mutate() to add the variable area_km and the function terra::expanse() to compute the surface area.\n\nadm_div_area &lt;- adm_div |&gt;\n    dplyr::mutate(area_km = terra::expanse(adm_div, unit = \"km\"))\n\n\n\n\n\n2.8 Extraction\n\n2.8.1 Spatial Variables\nWe extract the spatial data based on the administrative division using the extract_by_poly() function. This function requires the raster with the spatial data, the administrative division, and the aggregation function as input. The aggregation function, fn_agg, defines how the cell values that fall within an administrative division are combined into a single value. Note that by default all the cell values are weighted by the coverage area of the cell that fall within the division.\nContrary to the other spatial variable, for population we use adm_div_area to extract the values as we need the surface area to calculate the population density.\nLooking at the result, we see first the ID_adm_div column, that identifies the unique administrative divisions. The second to fourth column are the additional information coming from the administrative division data. The last column contain the spatial observations aggregated at the administrative division.\n\npop_adm &lt;- extract_by_poly(pop_cntry, adm_div_area, fn_agg = \"sum\")\npop_adm\n\n# A tibble: 74 × 6\n   ID_adm_div iso   adm_div_1        adm_div_2  area_km      pop\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 1          ETH   Somali           Afder       62044.  791202.\n 2 2          ETH   Gambela          Agnuak      23507.  250289.\n 3 3          ETH   SNNPR            Alaba         863.  326562.\n 4 4          ETH   Oromia           Arsi        20974. 3761471.\n 5 5          ETH   Beneshangul Gumu Asosa       14397.  513373.\n 6 6          ETH   Amhara           Awi/Agew     8950. 1249906.\n 7 7          ETH   Oromia           Bale        54644. 2011606.\n 8 8          ETH   SNNPR            Basketo       419.   79602.\n 9 9          ETH   SNNPR            Bench Maji  19085.  921810.\n10 10         ETH   Oromia           Borena      52437. 1406311.\n# ℹ 64 more rows\n\nnghtlght_adm &lt;- extract_by_poly(nghtlght_cntry, adm_div, fn_agg = \"mean\")\n\nelevation_adm &lt;- extract_by_poly(elevatn_cntry, adm_div, fn_agg = \"mean\")\n\nterrain_adm &lt;- extract_by_poly(terrain_cntry, adm_div, fn_agg = \"mean\")\n\nurca_adm &lt;- extract_by_poly(urca_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm &lt;- extract_by_poly(aez_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm\n\n# A tibble: 74 × 5\n   ID_adm_div iso   adm_div_1        adm_div_2    aez\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;dbl&gt;\n 1 1          ETH   Somali           Afder         26\n 2 2          ETH   Gambela          Agnuak         2\n 3 3          ETH   SNNPR            Alaba          5\n 4 4          ETH   Oromia           Arsi           6\n 5 5          ETH   Beneshangul Gumu Asosa          2\n 6 6          ETH   Amhara           Awi/Agew       5\n 7 7          ETH   Oromia           Bale           1\n 8 8          ETH   SNNPR            Basketo       26\n 9 9          ETH   SNNPR            Bench Maji    26\n10 10         ETH   Oromia           Borena         1\n# ℹ 64 more rows\n\n\n\n\n2.8.2 Weather Variables\nFro weather data, we use a different function for extracting the data, namely extract_cell_by_poly(). Contrary to the function extract_by_poly(), this doesn’t aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the long run climatic parameter for cell and only later aggregate them.\n\n\n\n\n\n\nNote\n\n\n\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n\n\nLooking at the result, we see first the ID_adm_div column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\npre_cell &lt;- extract_cell_by_poly(pre_cntry_mm, adm_div)\n\ntmp_cell &lt;- extract_cell_by_poly(tmp_cntry_c, adm_div)\n\n\ntmp_cell\n\n# A tibble: 11,845 × 908\n   ID_adm_div x_cell y_cell coverage_fraction X1950_01_01 X1950_02_01\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1            41      6.7            0.0309        23.0        24.3\n 2 1            41.1    6.7            0.398         23.8        25.1\n 3 1            41.2    6.7            0.0640        23.9        25.2\n 4 1            40.9    6.6            0.0118        22.0        23.3\n 5 1            41      6.6            0.673         22.9        24.2\n 6 1            41.1    6.6            1             23.8        25.1\n 7 1            41.2    6.6            0.887         24.2        25.5\n 8 1            41.3    6.6            0.430         24.6        25.8\n 9 1            41.4    6.6            0.0915        24.6        25.8\n10 1            41.8    6.6            0.103         26.9        28.0\n# ℹ 11,835 more rows\n# ℹ 902 more variables: X1950_03_01 &lt;dbl&gt;, X1950_04_01 &lt;dbl&gt;,\n#   X1950_05_01 &lt;dbl&gt;, X1950_06_01 &lt;dbl&gt;, X1950_07_01 &lt;dbl&gt;, X1950_08_01 &lt;dbl&gt;,\n#   X1950_09_01 &lt;dbl&gt;, X1950_10_01 &lt;dbl&gt;, X1950_11_01 &lt;dbl&gt;, X1950_12_01 &lt;dbl&gt;,\n#   X1951_01_01 &lt;dbl&gt;, X1951_02_01 &lt;dbl&gt;, X1951_03_01 &lt;dbl&gt;, X1951_04_01 &lt;dbl&gt;,\n#   X1951_05_01 &lt;dbl&gt;, X1951_06_01 &lt;dbl&gt;, X1951_07_01 &lt;dbl&gt;, X1951_08_01 &lt;dbl&gt;,\n#   X1951_09_01 &lt;dbl&gt;, X1951_10_01 &lt;dbl&gt;, X1951_11_01 &lt;dbl&gt;, …\n\n\n\n\n\n\n2.9 Cmpute Long Run Climatic Parameter\nWe want to describe the long run climatic condition in each locations. Rule of thumb is to use 30 years of weather observations to capture climatic features. Therefore, we select the 30 years before each survey.\nCheck the names with the date of observations and how it has changed since before.\n\npre_cell_30yrs &lt;- select_by_dates(pre_cell, from = \"1989\", to = \"2019\" )\ntmp_cell_30yrs &lt;- select_by_dates(tmp_cell, from = \"1989\", to = \"2019\")\ntmp_cell_30yrs\n\n# A tibble: 11,845 × 365\n   ID_adm_div x_cell y_cell coverage_fraction X1989_01_01 X1989_02_01\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1            41      6.7            0.0309        24.3        24.2\n 2 1            41.1    6.7            0.398         25.0        24.9\n 3 1            41.2    6.7            0.0640        25.0        25.0\n 4 1            40.9    6.6            0.0118        23.5        23.3\n 5 1            41      6.6            0.673         24.1        24.1\n 6 1            41.1    6.6            1             24.9        24.9\n 7 1            41.2    6.6            0.887         25.3        25.3\n 8 1            41.3    6.6            0.430         25.5        25.4\n 9 1            41.4    6.6            0.0915        25.4        25.3\n10 1            41.8    6.6            0.103         27.4        27.8\n# ℹ 11,835 more rows\n# ℹ 359 more variables: X1989_03_01 &lt;dbl&gt;, X1989_04_01 &lt;dbl&gt;,\n#   X1989_05_01 &lt;dbl&gt;, X1989_06_01 &lt;dbl&gt;, X1989_07_01 &lt;dbl&gt;, X1989_08_01 &lt;dbl&gt;,\n#   X1989_09_01 &lt;dbl&gt;, X1989_10_01 &lt;dbl&gt;, X1989_11_01 &lt;dbl&gt;, X1989_12_01 &lt;dbl&gt;,\n#   X1990_01_01 &lt;dbl&gt;, X1990_02_01 &lt;dbl&gt;, X1990_03_01 &lt;dbl&gt;, X1990_04_01 &lt;dbl&gt;,\n#   X1990_05_01 &lt;dbl&gt;, X1990_06_01 &lt;dbl&gt;, X1990_07_01 &lt;dbl&gt;, X1990_08_01 &lt;dbl&gt;,\n#   X1990_09_01 &lt;dbl&gt;, X1990_10_01 &lt;dbl&gt;, X1990_11_01 &lt;dbl&gt;, …\n\n\nNow we can compute the long run climatic parameter. We calculate the mean, the standard deviation, and the coefficient of variation. We collect all the parameter in a separate object parameter. This object is a names list of functions and we construct it with this structure name = function, then the list() function puts them together. This passage is not compulsory but allows to perform the computation of multiple parameters in a tidy and efficient way. Otherwise we could have directly add them inside the calc_par().\nThe function calc_par() calculates the required parameters.\nThe results have a similar structure, with the first columns that identify the specific locations and the other the computed parameters. Note how we are still carrying on the coverage_fraction variable as we will need it for aggregating the climatic parameter at the administrative division.\n\nparameter &lt;- list(std = sd, avg = mean, coef_var = cv)\nparameter\n\n$std\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n&lt;bytecode: 0x17e832da0&gt;\n&lt;environment: namespace:stats&gt;\n\n$avg\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x12cadd2f8&gt;\n&lt;environment: namespace:base&gt;\n\n$coef_var\nfunction(x, na_rm = TRUE) {\n    avg &lt;- mean(x, na.rm = na_rm)\n    if (is.nan(avg)) return(NA_real_)\n    if (avg == 0) return(NA_real_)  # Avoid division by zero\n    sd(x, na.rm = na_rm) / mean(x, na.rm = na_rm)\n}\n&lt;bytecode: 0x17e834978&gt;\n&lt;environment: namespace:climatic4economist&gt;\n\npre_par_cell &lt;- calc_par(pre_cell_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_cell &lt;- calc_par(tmp_cell_30yrs, pars = parameter, prefix = \"tmp\")\n\ntmp_par_cell\n\n# A tibble: 11,845 × 7\n   ID_adm_div x_cell y_cell coverage_fraction tmp_std tmp_avg tmp_coef_var\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 1            40.9   5.7             0.249     1.33    26.8       0.0497\n 2 1            40.9   5.8             0.673     1.37    27.1       0.0505\n 3 1            40.9   5.9             0.725     1.37    26.6       0.0516\n 4 1            40.9   6               0.728     1.32    25.5       0.0518\n 5 1            40.9   6.1             0.811     1.28    24.8       0.0517\n 6 1            40.9   6.2             0.719     1.29    24.5       0.0525\n 7 1            40.9   6.3             0.780     1.25    24.1       0.0520\n 8 1            40.9   6.4             0.892     1.24    23.8       0.0521\n 9 1            40.9   6.5             0.509     1.22    23.4       0.0520\n10 1            40.9   6.6             0.0118    1.18    22.9       0.0515\n# ℹ 11,835 more rows\n\n\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function agg_to_adm_div() can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the coverage_fraction variable.\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, ID_adm_div, and a single value of the climatic parameters for each locations.\n\npre_par_adm &lt;- agg_to_adm_div(pre_par_cell , match_col = \"pre\")\ntmp_par_adm &lt;- agg_to_adm_div(tmp_par_cell, match_col = \"tmp\")\n\ntmp_par_adm\n\n# A tibble: 74 × 4\n   ID_adm_div tmp_std tmp_avg tmp_coef_var\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 1             1.27    27.4       0.0464\n 2 10            1.59    22.5       0.0715\n 3 11            1.84    20.0       0.0920\n 4 12            1.65    20.9       0.0789\n 5 13            1.76    21.6       0.0808\n 6 14            1.17    26.5       0.0441\n 7 15            1.68    18.3       0.0919\n 8 16            1.18    21.3       0.0558\n 9 17            1.40    20.2       0.0697\n10 18            2.11    20.4       0.103 \n# ℹ 64 more rows\n\n\n\n\n\n2.10 Merge with Survey\nNow that we have everything, we can combine all the extracted data and then merge them with the survey. We start by combining the data into a unique data set. To do so we start by create a list with the function list(), each element of the list is a different spatial variable and then we combine the elements of the list with the function purrr::reduce(). This last function require another function as input to drive the combination and we choose to use merge_by_common(), which merges two data by their common variable names.\nWhy not using directly merge_by_common()? Because the function works with just two datasets and we have eight different spatial datasets. We can cumulatively merge the datasets one by one or we can use the purrr::reduce().\nThen we compute also the population density and the logarithmic transformation of the nighttime light. We use the dplyr::mutate() function to add these two new variables. We use the argument .after to specify where the position of the variable among the columns.\n\n1sptl_adm &lt;- list(pop_adm,\n                 nghtlght_adm,\n                 terrain_adm, \n                 elevation_adm, \n                 urca_adm, \n                 aez_adm, \n                 pre_par_adm, \n                 tmp_par_adm) |&gt;\n2    purrr::reduce(merge_by_common) |&gt;\n3    dplyr::mutate(pop_density = pop/area_km, .after = pop)\n\nsptl_adm\n\n\n1\n\ncombine the data into a list\n\n2\n\nmerge all the elements of the list\n\n3\n\ncreate new variables\n\n\n\n\n# A tibble: 74 × 21\n   ID_adm_div iso   adm_div_1    adm_div_2 area_km    pop pop_density nightlight\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 1          ETH   Somali       Afder      62044. 7.91e5        12.8   0.000139\n 2 2          ETH   Gambela      Agnuak     23507. 2.50e5        10.6   0.00118 \n 3 3          ETH   SNNPR        Alaba        863. 3.27e5       378.    0.0387  \n 4 4          ETH   Oromia       Arsi       20974. 3.76e6       179.    0.0108  \n 5 5          ETH   Beneshangul… Asosa      14397. 5.13e5        35.7   0.00389 \n 6 6          ETH   Amhara       Awi/Agew    8950. 1.25e6       140.    0.00539 \n 7 7          ETH   Oromia       Bale       54644. 2.01e6        36.8   0.00182 \n 8 8          ETH   SNNPR        Basketo      419. 7.96e4       190.    0       \n 9 9          ETH   SNNPR        Bench Ma…  19085. 9.22e5        48.3   0.00289 \n10 10         ETH   Oromia       Borena     52437. 1.41e6        26.8   0.00143 \n# ℹ 64 more rows\n# ℹ 13 more variables: ln_nightlight &lt;dbl&gt;, slope &lt;dbl&gt;, TRI &lt;dbl&gt;,\n#   roughness &lt;dbl&gt;, elevation &lt;dbl&gt;, URCA &lt;dbl&gt;, aez &lt;dbl&gt;, pre_std &lt;dbl&gt;,\n#   pre_avg &lt;dbl&gt;, pre_coef_var &lt;dbl&gt;, tmp_std &lt;dbl&gt;, tmp_avg &lt;dbl&gt;,\n#   tmp_coef_var &lt;dbl&gt;\n\n\nNow that we have all the control variables together, we can merge them with the surveys information. The function merge_by_common() will do it for us.\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as srvy_adm_div.\nWe first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command |&gt; assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with y = _, where y is the name of the argument and _ refer to the previous merge.\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n1srvy_sptl_adm &lt;- merge_by_common(srvy_adm_div, sptl_adm) |&gt;\n2    merge_by_common(srvy_coord, y = _)\nsrvy_sptl_adm\n\n\n1\n\nmerge adm info with spatial var\n\n2\n\n_ refers to the output of the previous merge\n\n\n\n\n# A tibble: 6,506 × 28\n   ID    survey_year hhid    country   lat   lon interview_date ID_adm_div iso  \n   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;         &lt;chr&gt;      &lt;chr&gt;\n 1 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 2 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 3 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 4 1            2019 051103… Ethiop…  3.61  39.0 2019-08-29     10         ETH  \n 5 1            2019 051103… Ethiop…  3.61  39.0 2019-08-29     10         ETH  \n 6 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 7 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 8 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n 9 1            2019 051103… Ethiop…  3.61  39.0 2019-08-28     10         ETH  \n10 1            2019 051103… Ethiop…  3.61  39.0 2019-08-29     10         ETH  \n# ℹ 6,496 more rows\n# ℹ 19 more variables: adm_div_1 &lt;chr&gt;, adm_div_2 &lt;chr&gt;, area_km &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, pop_density &lt;dbl&gt;, nightlight &lt;dbl&gt;, ln_nightlight &lt;dbl&gt;,\n#   slope &lt;dbl&gt;, TRI &lt;dbl&gt;, roughness &lt;dbl&gt;, elevation &lt;dbl&gt;, URCA &lt;dbl&gt;,\n#   aez &lt;dbl&gt;, pre_std &lt;dbl&gt;, pre_avg &lt;dbl&gt;, pre_coef_var &lt;dbl&gt;, tmp_std &lt;dbl&gt;,\n#   tmp_avg &lt;dbl&gt;, tmp_coef_var &lt;dbl&gt;\n\n\n\n\n\n2.11 Write\nHere we are at the end, let’s save the results. We want to save the result as dta so we will use the haven::write_dta() function.\n\nhaven::write_dta(srvy_sptl_adm,\n                 file.path(path_to_result, \"ETH_sp_adm.dta\"))"
  },
  {
    "objectID": "extract_by_poly.html#take-home-messages",
    "href": "extract_by_poly.html#take-home-messages",
    "title": "2: Extract spatial data based on spatial polygons",
    "section": "3 Take home messages",
    "text": "3 Take home messages\n\nWhen working with multiple spatial data:\n\nremember to control the Coordinate Reference System of all dataset\nplot the data to check everything is going well\n\nbased on the typology of data we use different function\n\nreading\n\nfor dta use haven::read_dta() or and haven::_write_dta()\nfor spatial vectors use terra::vect() or read_adm_div() for administrative divisions in specific country and level.\nfor spatial raster use terra::rast()\n\nextraction\n\nspatial points, use extract_by_coord()\nspatial polygons, use extract_by_poly()\ncells within polygons, use extract_cell_by_poly()\n\n\nWhen working with raster data\n\ncheck the unit of measure\nif it is a time series check also the date format\n\nWhen working with spatial polygons, like administrative divisions valuate if you want to extract the values already aggregated or each cells separately\n\nfor example the terrain indicators were computed for each cells and then we moved to the extraction\nfor the climatic parameters, we extract each cells separately, we compute the parameters for each cells, and only later we aggregate them.\n\nWhen georeferencing the survey location we take advantage that many interviews share the same locations. Hence, we extract the variables just for these unique locations. However, same of these unique locations may fall within the same value cells, so the actual information might be even lower."
  },
  {
    "objectID": "extract_by_poly.html#appendix",
    "href": "extract_by_poly.html#appendix",
    "title": "2: Extract spatial data based on spatial polygons",
    "section": "4 Appendix",
    "text": "4 Appendix\n\n4.1 Want to know about the data?\n\n4.1.1 Weather\nWeather observation are obtained from ERA5-Land reanalysis dataset. H-TESSEL is the land surface model that is the basis of ERA5-Land. The data is a post-processed monthly-mean average of the original ERA5-Land dataset.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.1° x 0.1° lon lat\n\n\ntemporal resolution\nmonth\n\n\ntime frame\nJan. 1950 - Dec. 2022\n\n\nunit of measure\nmeter or Kelvin\n\n\n\nSuggested citation:\n\nMuñoz Sabater, J. (2019): ERA5-Land monthly averaged data from 1950 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). DOI: 10.24381/cds.68d2bb30\n\nIt is possible to find additional information:\n\nhere\nand the related manual here.\n\nThe data can be freely download from\n\nhere.\n\n\n4.1.1.0.1 Total precipitation\nAccumulated liquid and frozen water, including rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Precipitation variables do not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth.\n\n\n4.1.1.0.2 2 metre above ground temperature\nTemperature of air at 2m above the surface of land, sea or in-land waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth’s surface, taking account of the atmospheric conditions.\n\n\n\n\n4.1.2 Spatial variables\n\n4.1.2.1 Agro Ecological Zones\nThe Agro-ecological Zones classification (33 classes) provides a characterization of bio-physical resources relevant to agricultural production systems. AEZ definitions and map classes follow a rigorous methodology and an explicit set of principles. The inventory combines spatial layers of thermal and moisture regimes with broad categories of soil/terrain qualities. It also indicates locations of areas with irrigated soils and shows land with severely limiting bio-physical constraints including very cold and very dry (desert) areas as well as areas with very steep terrain or very poor soil/terrain conditions. The AEZ classification dataset is part of the GAEZ v5 Land and Water Resources theme and Agro-ecological Zones sub-theme. All results are derived from the Agro-ecological Zones (AEZ) modeling framework, developed collaboratively by the Food and Agriculture Organization (FAO) and the International Institute for Applied Systems Analysis (IIASA).\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n10 km.\n\n\ntemporal resolution\n20 years\n\n\ntime frame\n2001–2020\n\n\nunit of measure\nclassification by climate/soil/terrain/LC (33 classes)\n\n\n\nSuggested citation:\n\nFAO & IIASA. 2025. Global Agro-ecological Zoning version 5 (GAEZ v5) Model Documentation. https://github.com/un-fao/gaezv5/wiki\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.2 Urban-Rural Catchment Area (URCA)\nUrban–rural catchment areas showing the catchment areas around cities and towns of different sizes (the no data value is 128). Each rural pixel is assigned to one defined travel time category to one of seven urban agglomeration sizes.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.03° x 0.03° lon lat\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2015\n\n\nunit of measure\ntravel time category to different urban hierarchy\n\n\n\nSuggested citation:\n\nCattaneo, Andrea; Nelson, Andy; McMenomy, Theresa (2020). Urban-rural continuum. figshare. Dataset. https://doi.org/10.6084/m9.figshare.12579572.v4\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.3 Population\nThe units are number of people per pixel. The mapping approach is Random Forest-based dasymetric redistribution.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n30 arc second (~1km)\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2010 - 2020\n\n\nunit of measure\nestimated count of people per grid-cell\n\n\n\nSuggested citation:\n\nWorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00647\n\nIt is possible to find additional information from:\n\nhere\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.4 Nighttime light\nVIIRS nighttime lights (VNL) version V2.1: annual values obtained by from the monthly averages with filtering to remove extraneous features such as biomass burning, aurora, and background.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n15 arc second\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2012 - 2021\n\n\nunit of measure\nnW/cm2/sr, average-masked\n\n\n\nSuggested citation:\n\nElvidge, C.D, Zhizhin, M., Ghosh T., Hsu FC, Taneja J. Annual time series of global VIIRS nighttime lights derived from monthly averages:2012 to 2019. Remote Sensing 2021, 13(5), p.922, doi:10.3390/rs13050922\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.5 Elevation\nThe Global Flood Awareness System (GloFAS) is one component of the Copernicus Emergency Management Service (CEMS). It is designed to support preparatory measures for flood events worldwide, particularly in large transnational river basins.\nElevation is obtained from the auxiliary variables of GloFAS. Each pixel is the mean height elevation above sea level.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.03° x 0.03° lon lat\n\n\ntemporal resolution\n30 years\n\n\ntime frame\n1981 - 2010\n\n\nunit of measure\nMeter (m)\n\n\n\nWeb resources:\n\nhere\n\nData access:\n\nhere\n\n\n\n\n\n4.1.3 Survey\nThe Living Standards Measurement Study - Integrated Surveys on Agriculture (LSMS-ISA) is a unique system of longitudinal surveys designed to improve the understanding of household and individual welfare, livelihoods and smallholder agriculture in Africa. The LSMS team works with national statistics offices to design and implement household surveys with a strong focus on agriculture.\nSuggested citation:\n\nCentral Statistics Agency of Ethiopia. (2020). Socioeconomic Survey 2018-2019 [Data set]. World Bank, Development Data Group. https://doi.org/10.48529/K739-C548\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.4 Administrative boundaries\nThe administrative divisions are obtained from GeoBoundaries[^2]. GeoBoundaries Built by the community and William & Mary geoLab, the geoBoundaries Global Database of Political Administrative Boundaries Database is an online, open license (CC BY 4.0) resource of information on administrative boundaries (i.e., state, county) for every country in the world. Since 2016, we have tracked approximately 1 million boundaries within over 200 entities, including all UN member states.\nSuggested citation:\n\nRunfola D, Anderson A, Baier H, Crittenden M, Dowker E, Fuhrig S, et al. (2020) geoBoundaries: A global database of political administrative boundaries. PLoS ONE 15(4): e0231866. https://doi.org/10.1371/journal.pone.0231866.\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere."
  },
  {
    "objectID": "distance_density_updated.html",
    "href": "distance_density_updated.html",
    "title": "Tutorial Functions average distance and average density",
    "section": "",
    "text": "1 Tutorial to calculate average distance and point density statistics\n  2 Overview of Steps\n  3 Loading packages\n  4 Setting up working directory and path to wrapper functions\n  5 Calculating average distance to rivers for each polygon ID\n  6 Loading inputs\n  7 Calculating average distance to rivers for each ID polygon in target\n  8 Calculating average density to point-buildings for each ID polygon in target\n  9 Using Buildings dataset as shape_input\n  10 Using points rather than non-intersecting buffers"
  },
  {
    "objectID": "distance_density_updated.html#tutorial-to-calculate-average-distance-and-point-density-statistics",
    "href": "distance_density_updated.html#tutorial-to-calculate-average-distance-and-point-density-statistics",
    "title": "Tutorial Functions average distance and average density",
    "section": "1 Tutorial to calculate average distance and point density statistics",
    "text": "1 Tutorial to calculate average distance and point density statistics\nThis guide provides a step-by-step approach to compute the average distance and point density statistics for a target set of polygons/points coordinates in R. It uses own functions from climatic4economist package: ave_dist_2shp() and ave_point_density_2shp(). The inputs are shapefile files loaded as sf_dataframe R objects and the returning outputs are dataframe R objects."
  },
  {
    "objectID": "distance_density_updated.html#overview-of-steps",
    "href": "distance_density_updated.html#overview-of-steps",
    "title": "Tutorial Functions average distance and average density",
    "section": "2 Overview of Steps",
    "text": "2 Overview of Steps\nWe will go through the following steps:\n\nLoad the data\nPrepare the data\nCompute the average distance to rivers for each ID (village, adm-div, etc.)\nCompute the average point density of buildings for each ID (village, adm-div, etc.)"
  },
  {
    "objectID": "distance_density_updated.html#loading-packages",
    "href": "distance_density_updated.html#loading-packages",
    "title": "Tutorial Functions average distance and average density",
    "section": "3 Loading packages",
    "text": "3 Loading packages\n\nlibrary(\"dplyr\")\nlibrary(\"raster\")\nlibrary(\"sf\")\nlibrary(\"data.table\")\nlibrary(\"datawizard\")\nlibrary(\"zoo\")\nlibrary(\"doParallel\")\nlibrary(\"foreach\")\nlibrary(\"parallel\")\nlibrary(\"ncdf4\")\nlibrary(\"SPEI\")\nlibrary(\"ggplot2\")\nlibrary(\"foreign\")\nlibrary(\"tidyr\")\nlibrary(\"terra\")\nlibrary(\"stars\")\nlibrary(\"yarrr\")\nlibrary(\"wesanderson\")\n#library(\"ClimateOperators\")\nlibrary(\"rlist\")\nlibrary(\"writexl\")\nlibrary(\"readxl\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"miceadds\")\nlibrary(\"stargazer\") #Display table on latex format\nlibrary(\"foreign\")\nlibrary(\"dplyr\") #data management\nlibrary(\"tidyverse\") #data management\nlibrary(\"tibble\") # tibbles\nlibrary(\"caret\")\nlibrary(\"naniar\")\nlibrary(\"mapproj\")\nlibrary(\"ggmap\")\nlibrary(\"RgoogleMaps\")\nlibrary(\"mapview\")\nlibrary(\"leaflet\")\nlibrary(\"haven\")\nlibrary(\"rnaturalearth\")\nlibrary(\"reshape2\")\nlibrary(\"sjlabelled\")\nlibrary(\"RColorBrewer\")\nlibrary(\"climatic4economist\")\ndevtools::document() \n\n\n\nAll packages loaded"
  },
  {
    "objectID": "distance_density_updated.html#setting-up-working-directory-and-path-to-wrapper-functions",
    "href": "distance_density_updated.html#setting-up-working-directory-and-path-to-wrapper-functions",
    "title": "Tutorial Functions average distance and average density",
    "section": "4 Setting up working directory and path to wrapper functions",
    "text": "4 Setting up working directory and path to wrapper functions\nWorking directory set up as default with folder’s location.\n\n# Otherwise use\n#setwd(\"C:/Users/X/OneDrive - Food and Agriculture Organization/PROJECT11_GEOINDICATORS_REPOSITORY\")\n\nrm(list = ls()) # remove existing objects\n\n\n# load wrapper functions\n# source(file.path(#\"web_tutorial\",\n#                  \"functions.R\"))\n\npath_to_project &lt;- file.path(\"..\", \"..\",\n                        \"data\", \"data_project\", \"benin\",\n                        \"PROJECT11_GEOINDICATORS_REPOSITORY\")"
  },
  {
    "objectID": "distance_density_updated.html#calculating-average-distance-to-rivers-for-each-polygon-id",
    "href": "distance_density_updated.html#calculating-average-distance-to-rivers-for-each-polygon-id",
    "title": "Tutorial Functions average distance and average density",
    "section": "5 Calculating average distance to rivers for each polygon ID",
    "text": "5 Calculating average distance to rivers for each polygon ID"
  },
  {
    "objectID": "distance_density_updated.html#loading-inputs",
    "href": "distance_density_updated.html#loading-inputs",
    "title": "Tutorial Functions average distance and average density",
    "section": "6 Loading inputs",
    "text": "6 Loading inputs\nAll input have to be set up as EPSG4326 or WGS84 as coordinate system\n\n6.0.1 Shapefile extension for the target country (Benin)\nThis uses GADM database (see GADM)\n\npath_gadm3 &lt;- file.path(path_to_project,\n                        \"INPUT\",\n                        \"Benin_shp\",\"Benin-ADMlevels\",\"Shapefile\",\n                        \"BEN_shp\",\"gadm41_BEN_3.shp\")\n\ngadm3 &lt;- st_as_sf(st_read(path_gadm3)) |&gt;\n  dplyr::select(-NL_NAME_1, -NL_NAME_2, -NL_NAME_3, \n                -VARNAME_3, -CC_3, -HASC_3,-TYPE_3,\n                -ENGTYPE_3,-COUNTRY)\n\n\ngadm3 &lt;- st_transform(gadm3, 4326) #defining as EPSG4326 or WGS84\ngadm3\n\nSimple feature collection with 546 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 0.774345 ymin: 6.23491 xmax: 3.851701 ymax: 12.41835\nGeodetic CRS:  WGS 84\nFirst 10 features:\n          GID_3 GID_0   GID_1  NAME_1     GID_2    NAME_2     NAME_3\n1   BEN.1.1.1_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara  Banikoara\n2   BEN.1.1.2_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara   Founougo\n3   BEN.1.1.3_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara   Gomparou\n4   BEN.1.1.4_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara    Goumori\n5   BEN.1.1.5_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara      Kokey\n6   BEN.1.1.6_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara Kokiborou*\n7   BEN.1.1.7_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara      Ounet\n8   BEN.1.1.8_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara Somp?r?kou\n9   BEN.1.1.9_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara     Soroko\n10 BEN.1.1.10_1   BEN BEN.1_1 Alibori BEN.1.1_1 Banikoara      Toura\n                         geometry\n1  MULTIPOLYGON (((2.415323 11...\n2  MULTIPOLYGON (((2.346528 11...\n3  MULTIPOLYGON (((2.464541 11...\n4  MULTIPOLYGON (((2.070032 11...\n5  MULTIPOLYGON (((2.802918 11...\n6  MULTIPOLYGON (((2.26218 11....\n7  MULTIPOLYGON (((2.405739 11...\n8  MULTIPOLYGON (((2.463629 11...\n9  MULTIPOLYGON (((2.338276 11...\n10 MULTIPOLYGON (((2.317923 11...\n\n\n\n\n6.0.2 Shapefile target country (Benin)\nThis file is the shapefile with the polygon’s structure. Each ID is enumerated in column id_gns. The original file with X, Y villages’ coordinates can be found here Villages Benin. The file was modified and non-intersecting buffers (using Voronoi’s structure) of 2km were constructed using the villages’ coordinates.\n\npath_polygons_gns &lt;- file.path(path_to_project,\n                        \"INPUT\",\"QGIS_map_voronoi\",\n                        \"villages_voronoibuffer_adm4_gns.shp\")\n\npolygons_gns &lt;- st_as_sf(st_read(path_polygons_gns))\n\n\npolygons_gns &lt;- st_transform(polygons_gns, 4326) #defining as EPSG4326 or WGS84\npolygons_gns\n\nSimple feature collection with 3599 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 0.7645527 ymin: 6.222888 xmax: 3.839671 ymax: 12.42317\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ID gid_0   gid_1     gid_2       gid_3  name_1    name_2    name_3\n1   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n2   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n3   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n4   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n5   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n6   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n7   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n8   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n9   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n10  1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n           type_3 engtype_3 id_gns  full_name full_nm_nd sort_name desig_cd\n1  Arrondissement   Borough   2058       Gano       Gano      GANO      PPL\n2  Arrondissement   Borough   1426     Kokabo     Kokabo    KOKABO      PPL\n3  Arrondissement   Borough    973      Wagui      Wagui     WAGUI      PPL\n4  Arrondissement   Borough   3392   Atabénou   Atabenou  ATABENOU      PPL\n5  Arrondissement   Borough   2726      Komon      Komon     KOMON      PPL\n6  Arrondissement   Borough   2263      Dérou      Derou     DEROU      PPL\n7  Arrondissement   Borough   3386  Banikoara  Banikoara BANIKOARA      PPL\n8  Arrondissement   Borough    453 Toké-Banta Toke-Banta TOKEBANTA      PPL\n9  Arrondissement   Borough   1873   Gomparou   Gomparou  GOMPAROU      PPL\n10 Arrondissement   Borough    581    Sonworé    Sonwore   SONWORE      PPL\n     adm1    lat_y    lon_x                       geometry\n1  BJ-000 11.24685 2.414291 MULTIPOLYGON (((2.423529 11...\n2  BJ-000 11.28007 2.427724 MULTIPOLYGON (((2.434365 11...\n3  BJ-000 11.28146 2.400951 MULTIPOLYGON (((2.392831 11...\n4  BJ-000 11.28761 2.381083 MULTIPOLYGON (((2.392831 11...\n5  BJ-000 11.28951 2.423543 MULTIPOLYGON (((2.426221 11...\n6  BJ-000 11.29832 2.401679 MULTIPOLYGON (((2.413442 11...\n7   BJ-AL 11.29845 2.438561 MULTIPOLYGON (((2.447332 11...\n8  BJ-000 11.30770 2.414829 MULTIPOLYGON (((2.430931 11...\n9   BJ-AL 11.33134 2.441141 MULTIPOLYGON (((2.437974 11...\n10 BJ-000 11.35827 2.422149 MULTIPOLYGON (((2.440276 11...\n\n\n\n\n6.0.3 Shapefile shape_input\nWe will use the shapefile of African Rivers from FAO as our shape_input (see FAO rivers1 and here FAO rivers2)\n\npath_rivers_fao &lt;- file.path(path_to_project, \"INPUT\",\"FAO_GIS\",\"rivers_africa\",\"rivers_africa_37333.shp\")\n\nrivers_fao &lt;- st_as_sf(st_read(path_rivers_fao))\n\n\nrivers_fao &lt;- st_transform(rivers_fao, 4326) #defining as EPSG4326 or WGS84\nrivers_fao\n\nSimple feature collection with 185730 features and 18 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -17.23958 ymin: -34.76458 xmax: 54.35 ymax: 37.21875\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   FID_af_str ARCID FROM_NODE TO_NODE FID_sub_ba SUB_BAS MAJ_BAS\n1           0     1         2       1      32890  201591    7020\n2           1     2         5       4      32890  201591    7020\n3           2     3         6       7      32890  201591    7020\n4           3     4         7       3      32890  201591    7020\n5           4     5         8       6      32890  201591    7020\n6           5     6        10       8      32890  201591    7020\n7           6     7        11       9      32890  201591    7020\n8           7     8        13       8      32890  201591    7020\n9           8     9        14       6      32890  201591    7020\n10          9    10        15       7      32890  201591    7020\n                    MAJ_NAME            SUB_NAME MAJ_AREA LEGEND SUBBAS_ID\n1  Mediterranean South Coast Algerian east coast   558292     20   7201591\n2  Mediterranean South Coast Algerian east coast   558292     20   7201591\n3  Mediterranean South Coast Algerian east coast   558292     20   7201591\n4  Mediterranean South Coast Algerian east coast   558292     20   7201591\n5  Mediterranean South Coast Algerian east coast   558292     20   7201591\n6  Mediterranean South Coast Algerian east coast   558292     20   7201591\n7  Mediterranean South Coast Algerian east coast   558292     20   7201591\n8  Mediterranean South Coast Algerian east coast   558292     20   7201591\n9  Mediterranean South Coast Algerian east coast   558292     20   7201591\n10 Mediterranean South Coast Algerian east coast   558292     20   7201591\n   TOBAS_ID Strahler A_Strahler RASTERVA_2 RASTERVA_1 Regime\n1      -999        1          8        609       5202      P\n2      -999        1          8        460       2537      P\n3      -999        2          7        518      97325      P\n4      -999        3          6        478     218144      P\n5      -999        2          7        518      41565      P\n6      -999        1          8        518       8568      P\n7      -999        1          8        413       9945      P\n8      -999        1          8        638       5032      P\n9      -999        1          8        648       6162      P\n10     -999        2          7        478     106369      P\n                         geometry\n1  MULTILINESTRING ((9.220833 ...\n2  MULTILINESTRING ((9.945833 ...\n3  MULTILINESTRING ((9.65625 3...\n4  MULTILINESTRING ((9.735417 ...\n5  MULTILINESTRING ((9.647917 ...\n6  MULTILINESTRING ((9.614583 ...\n7  MULTILINESTRING ((10.075 37...\n8  MULTILINESTRING ((9.479167 ...\n9  MULTILINESTRING ((9.2375 37...\n10 MULTILINESTRING ((9.68125 3...\n\n\nAs the shapefile of rivers corresponds to all Africa, we will crop it to country’s area using the extension from the object gamd, which has the extension of Benin. It will be enlarged a bit in case to guarantee it does contain all country’s borders. Notice also that for using the function raster::crop, we need to define the object as an Spatial one, do the cropping and define it again as sf_dataframe, for which, we need to define it again with the same coordinate system WGS84 (otherwise our function ave_point_density_2shp will trew an error).\n\next_benin&lt;-raster::extent(gadm3)\next_benin@xmin&lt;-ext_benin@xmin-0.5\next_benin@xmax&lt;-ext_benin@xmax+0.5\next_benin@ymin&lt;-ext_benin@ymin-0.5\next_benin@ymax&lt;-ext_benin@ymax+0.5\n\n#cropping\nrivers_fao &lt;- st_as_sf(raster::crop(as(rivers_fao, \"Spatial\"),ext_benin))\n\nrivers_fao &lt;- st_transform(rivers_fao, 4326) #defining as EPSG4326 or WGS84 again as I only defined as sf dataframe before\nrivers_fao\n\nSimple feature collection with 1796 features and 18 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 0.274345 ymin: 5.78125 xmax: 4.351701 ymax: 12.91835\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   FID_af_str ARCID FROM_NODE TO_NODE FID_sub_ba SUB_BAS MAJ_BAS MAJ_NAME\n1       85827 85828     89460   89298      32706   20872    7002    Niger\n2       85828 85829     89461   89407      32705   20871    7002    Niger\n3       85829 85830     89348   89462      32705   20871    7002    Niger\n4       85871 85872     89334   89502      32706   20872    7002    Niger\n5       85872 85873     89502   89460      32706   20872    7002    Niger\n6       85918 85919     89264   89462      32705   20871    7002    Niger\n7       85930 85931     89569   89381      32706   20872    7002    Niger\n8       85931 85932     89070   89571      32692   20843    7002    Niger\n9       85979 85980     88722   89617      32701   20854    7002    Niger\n10      85980 85981     89553   89617      32701   20854    7002    Niger\n        SUB_NAME MAJ_AREA LEGEND SUBBAS_ID TOBAS_ID Strahler A_Strahler\n1           Faga  2136941      2   7020872  7020871        4          5\n2        Niger 9  2136941      2   7020871  7020859        1          8\n3        Niger 9  2136941      2   7020871  7020859        6          3\n4           Faga  2136941      2   7020872  7020871        1          8\n5           Faga  2136941      2   7020872  7020871        4          5\n6        Niger 9  2136941      2   7020871  7020859        4          5\n7           Faga  2136941      2   7020872  7020871        1          8\n8       Sokoto 1  2136941      2   7020843  7020841        1          8\n9  Dallol Maouri  2136941      2   7020854  7020853        2          7\n10 Dallol Maouri  2136941      2   7020854  7020853        1          8\n   RASTERVA_2 RASTERVA_1 Regime                       geometry\n1         182     907940      P MULTILINESTRING ((0.5229167...\n2         421      10060      I MULTILINESTRING ((2.102083 ...\n3          55   66217700      P MULTILINESTRING ((2.376455 ...\n4         667       5300      I MULTILINESTRING ((0.4721039...\n5         674     894820      P MULTILINESTRING ((0.4979167...\n6         158    1462580      P MULTILINESTRING ((2.256649 ...\n7         262       5870      I MULTILINESTRING ((0.76875 1...\n8         280       6090      I MULTILINESTRING ((4.34375 1...\n9         705     178340      I MULTILINESTRING ((3.49375 1...\n10        586       5100      I MULTILINESTRING ((3.514583 ..."
  },
  {
    "objectID": "distance_density_updated.html#calculating-average-distance-to-rivers-for-each-id-polygon-in-target",
    "href": "distance_density_updated.html#calculating-average-distance-to-rivers-for-each-id-polygon-in-target",
    "title": "Tutorial Functions average distance and average density",
    "section": "7 Calculating average distance to rivers for each ID polygon in target",
    "text": "7 Calculating average distance to rivers for each ID polygon in target\nThis wrapper function calculates the average distance (in m) of each ID element in target to the object shape_input. It uses distanceFromPoints which works better, faster than st_distance function. For the moment, the function works better for UTM coordinate system.\ncountry_ext defines the whole extension of the country, including country borders. Here, we use gadm3 is our country’s extension for Benin.\ntarget is the shapefile (adm-div such as communes, villages or HH coordinates) for which the average distance will be calculated. We use polygons_gns as our target polygon for which we will calculate the distance.\nshape_input is the shapefile for which a raster of distances will be calculated (eg. roads, rivers, etc.). In our case, we use rivers_fao as the object from which the distances will be calculated.\nUTM corresponds to the country’s location in the UTM coordinate system. As for calculating distances it is more convenient to use meters instead of degrees, and the coordinate system needs to be transformed into the UTM of the country, which is more precise around the equator. We define UTM=31, which corresponds to Benin. Otherwise, it uses default at the global level (3857) (under development).\nres_raster is the resolution in meters for the raster of distances that will be created. A higher resolution increase the time of processing. We define a resolution of 10000 which corresponds to 10 km. Internally, it will create a raster of distances of 10 km, and each pixel size of 10kmX10km.\nname_dist name of the average distance variable constructed. It will be created as ave_dist_NAME with name_dist=“NAME” giving the name of the distance variable that will be created. In our case, ave_dist_rivers.\n\nave_dist_rivers &lt;- ave_dist_2shp(country_ext=gadm3,target=polygons_gns,shape_input=rivers_fao,UTM=31,res_raster=10000,name_dist=\"rivers\")\n\n[1] \"UTM is: 31\"\n\nave_dist_rivers[1:10, ] #print first 10 elements\n\n   ID gid_0   gid_1     gid_2       gid_3  name_1    name_2    name_3\n1   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n2   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n3   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n4   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n5   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n6   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n7   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n8   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n9   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n10  1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n           type_3 engtype_3 id_gns  full_name full_nm_nd sort_name desig_cd\n1  Arrondissement   Borough   2058       Gano       Gano      GANO      PPL\n2  Arrondissement   Borough   1426     Kokabo     Kokabo    KOKABO      PPL\n3  Arrondissement   Borough    973      Wagui      Wagui     WAGUI      PPL\n4  Arrondissement   Borough   3392   Atabénou   Atabenou  ATABENOU      PPL\n5  Arrondissement   Borough   2726      Komon      Komon     KOMON      PPL\n6  Arrondissement   Borough   2263      Dérou      Derou     DEROU      PPL\n7  Arrondissement   Borough   3386  Banikoara  Banikoara BANIKOARA      PPL\n8  Arrondissement   Borough    453 Toké-Banta Toke-Banta TOKEBANTA      PPL\n9  Arrondissement   Borough   1873   Gomparou   Gomparou  GOMPAROU      PPL\n10 Arrondissement   Borough    581    Sonworé    Sonwore   SONWORE      PPL\n     adm1    lat_y    lon_x ave_dist_rivers\n1  BJ-000 11.24685 2.414291        4885.229\n2  BJ-000 11.28007 2.427724        4885.229\n3  BJ-000 11.28146 2.400951        4885.229\n4  BJ-000 11.28761 2.381083        4885.229\n5  BJ-000 11.28951 2.423543        4885.229\n6  BJ-000 11.29832 2.401679        4885.229\n7   BJ-AL 11.29845 2.438561        5402.293\n8  BJ-000 11.30770 2.414829        4885.229\n9   BJ-AL 11.33134 2.441141        6628.781\n10 BJ-000 11.35827 2.422149        6761.864"
  },
  {
    "objectID": "distance_density_updated.html#calculating-average-density-to-point-buildings-for-each-id-polygon-in-target",
    "href": "distance_density_updated.html#calculating-average-density-to-point-buildings-for-each-id-polygon-in-target",
    "title": "Tutorial Functions average distance and average density",
    "section": "8 Calculating average density to point-buildings for each ID polygon in target",
    "text": "8 Calculating average density to point-buildings for each ID polygon in target\nThis wrapper function calculates the average density of points for each ID element of targetto the object shape_input ."
  },
  {
    "objectID": "distance_density_updated.html#using-buildings-dataset-as-shape_input",
    "href": "distance_density_updated.html#using-buildings-dataset-as-shape_input",
    "title": "Tutorial Functions average distance and average density",
    "section": "9 Using Buildings dataset as shape_input",
    "text": "9 Using Buildings dataset as shape_input\nWhile some of the objects are already defined as in the previous function (gadm3 and polygons_gns), we will use as shape_input a shapefile that contains the points with buildings located in the country (see OCHA Datahum). The file comes in zip format as it is opened, and transformed as an sf_dataframe\n\npath2buildings &lt;- file.path(path_to_project,\n                        \"INPUT\", \"Benin_OCRA\",\"Buildings\",\"hotosm_ben_buildings_polygons_shp.zip\") #using zip\n\n# Extract the ZIP file and Find the file among the extracted files. There is only one\noutput_directory_buildings &lt;- tempdir()  # Using a temporary directory\nunzip(path2buildings, exdir = output_directory_buildings)\nextracted_files &lt;- list.files(output_directory_buildings, full.names = TRUE)\nbuildings_file_path &lt;- extracted_files[grep(\"\\\\.shp$\", extracted_files)]\n\n#reading file\nbuildings_ocra &lt;- st_as_sf(st_read(buildings_file_path))\n\n\nbuildings_ocra &lt;- st_transform(buildings_ocra, 4326) #defining as EPSG4326 or WGS84\nbuildings_ocra\n\nSimple feature collection with 813093 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 0.7848944 ymin: 6.236586 xmax: 3.836711 ymax: 12.40539\nGeodetic CRS:  WGS 84\nFirst 10 features:\n      osm_id addrcity office                      name buildingle addrhousen\n1   81452793     &lt;NA&gt;   &lt;NA&gt;                Tri Postal       &lt;NA&gt;       &lt;NA&gt;\n2   81766299     &lt;NA&gt;   &lt;NA&gt;                      &lt;NA&gt;          8       &lt;NA&gt;\n3  116969223     &lt;NA&gt;   &lt;NA&gt;                Site Pompe       &lt;NA&gt;       &lt;NA&gt;\n4  116969224     &lt;NA&gt;   &lt;NA&gt;             Station SONAB       &lt;NA&gt;       &lt;NA&gt;\n5  116969225     &lt;NA&gt;   &lt;NA&gt;                Site Pompe       &lt;NA&gt;       &lt;NA&gt;\n6  116985473     &lt;NA&gt;   &lt;NA&gt;                      SBEE       &lt;NA&gt;       &lt;NA&gt;\n7  116985474     &lt;NA&gt;   &lt;NA&gt; Hôtel de Ville de Bassila       &lt;NA&gt;       &lt;NA&gt;\n8  116985476     &lt;NA&gt;   &lt;NA&gt; Hôtel de Ville de Bassila       &lt;NA&gt;       &lt;NA&gt;\n9  167008127     &lt;NA&gt;   &lt;NA&gt;                      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n10 167008128     &lt;NA&gt;   &lt;NA&gt;                      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n   addrfull building addrstreet source buildingma\n1      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n2      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n3      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n4      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n5      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n6      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n7      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n8      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n9      &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n10     &lt;NA&gt;      yes       &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;\n                         geometry\n1  MULTIPOLYGON (((2.386848 6....\n2  MULTIPOLYGON (((2.386307 6....\n3  MULTIPOLYGON (((1.666625 9....\n4  MULTIPOLYGON (((1.667526 9....\n5  MULTIPOLYGON (((1.6682 9.00...\n6  MULTIPOLYGON (((1.667723 8....\n7  MULTIPOLYGON (((1.667386 8....\n8  MULTIPOLYGON (((1.667395 8....\n9  MULTIPOLYGON (((2.340467 6....\n10 MULTIPOLYGON (((2.340359 6....\n\n\nBelow we describe the inputs for the function:\ncountry_ext defines the whole extension of the country, including country borders. Here, we use gadm3 is our country’s extension for Benin.\ntarget is the shapefile (adm-div such as communes, villages or HH coordinates) for which the average point density will be calculated. We use polygons_gns as our target polygon for which we will calculate the point density.\nshape_input is the shapefile from which to calculate points density (eg. buildings, roads, rivers, etc.) In our case, it will be our building’s object buildings_ocra.\nres_raster is the resolution in degrees for the raster of distances that will be created and applied to `shape_input’ (0.1 could be equivalent to 10km or 0.1*100000=10000m around the Equator). A higher resolution increases the time of processing.\nname_dist name of the average density variable constructed. It will be created as ave_den_NAME with name_density=“NAME In our case, ave_den_allbuildings.\n\nave_den_allbuildings &lt;-ave_point_density_2shp(country_ext=gadm3,\n                                              target=polygons_gns,\n                                              shape_input=buildings_ocra,\n                                              res_raster=0.1,\n                                              name_density=\"allbuildings\")\nave_den_allbuildings[1:10, ] #print first 10 elements\n\n   ID gid_0   gid_1     gid_2       gid_3  name_1    name_2    name_3\n1   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n2   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n3   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n4   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n5   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n6   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n7   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n8   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n9   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n10  1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n           type_3 engtype_3 id_gns  full_name full_nm_nd sort_name desig_cd\n1  Arrondissement   Borough   2058       Gano       Gano      GANO      PPL\n2  Arrondissement   Borough   1426     Kokabo     Kokabo    KOKABO      PPL\n3  Arrondissement   Borough    973      Wagui      Wagui     WAGUI      PPL\n4  Arrondissement   Borough   3392   Atabénou   Atabenou  ATABENOU      PPL\n5  Arrondissement   Borough   2726      Komon      Komon     KOMON      PPL\n6  Arrondissement   Borough   2263      Dérou      Derou     DEROU      PPL\n7  Arrondissement   Borough   3386  Banikoara  Banikoara BANIKOARA      PPL\n8  Arrondissement   Borough    453 Toké-Banta Toke-Banta TOKEBANTA      PPL\n9  Arrondissement   Borough   1873   Gomparou   Gomparou  GOMPAROU      PPL\n10 Arrondissement   Borough    581    Sonworé    Sonwore   SONWORE      PPL\n     adm1    lat_y    lon_x ave_den_allbuildings\n1  BJ-000 11.24685 2.414291             804.0000\n2  BJ-000 11.28007 2.427724             804.0000\n3  BJ-000 11.28146 2.400951             804.0000\n4  BJ-000 11.28761 2.381083             654.0000\n5  BJ-000 11.28951 2.423543             804.0000\n6  BJ-000 11.29832 2.401679             804.0000\n7   BJ-AL 11.29845 2.438561             804.0000\n8  BJ-000 11.30770 2.414829             595.8421\n9   BJ-AL 11.33134 2.441141              92.1000\n10 BJ-000 11.35827 2.422149              13.0000"
  },
  {
    "objectID": "distance_density_updated.html#using-points-rather-than-non-intersecting-buffers",
    "href": "distance_density_updated.html#using-points-rather-than-non-intersecting-buffers",
    "title": "Tutorial Functions average distance and average density",
    "section": "10 Using points rather than non-intersecting buffers",
    "text": "10 Using points rather than non-intersecting buffers\nTesting function for points:\n\npath_points_gns &lt;- file.path(path_to_project,\n                        \"INPUT\",\"QGIS_map_voronoi\",\"villages_points_adm4_gns.shp\")\n\npoints_gns &lt;- st_as_sf(st_read(path_points_gns))\n\n\npoints_gns &lt;- st_transform(points_gns, 4326) #defining as EPSG4326 or WGS84\npoints_gns\n\nSimple feature collection with 3599 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.782811 ymin: 6.240975 xmax: 3.821388 ymax: 12.40508\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ID gid_0   gid_1     gid_2       gid_3  name_1    name_2    name_3\n1   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n2   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n3   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n4   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n5   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n6   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n7   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n8   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n9   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n10  1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n           type_3 engtype_3 id_gns  full_name full_nm_nd sort_name desig_cd\n1  Arrondissement   Borough   2058       Gano       Gano      GANO      PPL\n2  Arrondissement   Borough   1426     Kokabo     Kokabo    KOKABO      PPL\n3  Arrondissement   Borough    973      Wagui      Wagui     WAGUI      PPL\n4  Arrondissement   Borough   3392   Atabénou   Atabenou  ATABENOU      PPL\n5  Arrondissement   Borough   2726      Komon      Komon     KOMON      PPL\n6  Arrondissement   Borough   2263      Dérou      Derou     DEROU      PPL\n7  Arrondissement   Borough   3386  Banikoara  Banikoara BANIKOARA      PPL\n8  Arrondissement   Borough    453 Toké-Banta Toke-Banta TOKEBANTA      PPL\n9  Arrondissement   Borough   1873   Gomparou   Gomparou  GOMPAROU      PPL\n10 Arrondissement   Borough    581    Sonworé    Sonwore   SONWORE      PPL\n     adm1    lat_y    lon_x                  geometry\n1  BJ-000 11.24685 2.414291 POINT (2.414291 11.24685)\n2  BJ-000 11.28007 2.427724 POINT (2.427724 11.28007)\n3  BJ-000 11.28146 2.400951 POINT (2.400951 11.28146)\n4  BJ-000 11.28761 2.381083 POINT (2.381083 11.28761)\n5  BJ-000 11.28951 2.423543 POINT (2.423543 11.28951)\n6  BJ-000 11.29832 2.401679 POINT (2.401679 11.29832)\n7   BJ-AL 11.29845 2.438561 POINT (2.438561 11.29845)\n8  BJ-000 11.30770 2.414829  POINT (2.414829 11.3077)\n9   BJ-AL 11.33134 2.441141 POINT (2.441141 11.33134)\n10 BJ-000 11.35827 2.422149 POINT (2.422149 11.35827)\n\n\n\n10.1 Calculating distance\n\nave_dist_rivers_points &lt;- ave_dist_2shp(country_ext=gadm3,target=points_gns,shape_input=rivers_fao,UTM=31,res_raster=10000,name_dist=\"rivers\")\n\n[1] \"UTM is: 31\"\n\nave_dist_rivers_points[1:10, ] #print first 10 elements\n\n   ID gid_0   gid_1     gid_2       gid_3  name_1    name_2    name_3\n1   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n2   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n3   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n4   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n5   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n6   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n7   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n8   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n9   1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n10  1   BEN BEN.1_1 BEN.1.1_1 BEN.1.1.1_1 Alibori Banikoara Banikoara\n           type_3 engtype_3 id_gns  full_name full_nm_nd sort_name desig_cd\n1  Arrondissement   Borough   2058       Gano       Gano      GANO      PPL\n2  Arrondissement   Borough   1426     Kokabo     Kokabo    KOKABO      PPL\n3  Arrondissement   Borough    973      Wagui      Wagui     WAGUI      PPL\n4  Arrondissement   Borough   3392   Atabénou   Atabenou  ATABENOU      PPL\n5  Arrondissement   Borough   2726      Komon      Komon     KOMON      PPL\n6  Arrondissement   Borough   2263      Dérou      Derou     DEROU      PPL\n7  Arrondissement   Borough   3386  Banikoara  Banikoara BANIKOARA      PPL\n8  Arrondissement   Borough    453 Toké-Banta Toke-Banta TOKEBANTA      PPL\n9  Arrondissement   Borough   1873   Gomparou   Gomparou  GOMPAROU      PPL\n10 Arrondissement   Borough    581    Sonworé    Sonwore   SONWORE      PPL\n     adm1    lat_y    lon_x ave_dist_rivers\n1  BJ-000 11.24685 2.414291        5455.917\n2  BJ-000 11.28007 2.427724        5483.610\n3  BJ-000 11.28146 2.400951        4977.966\n4  BJ-000 11.28761 2.381083        5037.752\n5  BJ-000 11.28951 2.423543        5582.816\n6  BJ-000 11.29832 2.401679        5329.104\n7   BJ-AL 11.29845 2.438561        6046.705\n8  BJ-000 11.30770 2.414829        5767.787\n9   BJ-AL 11.33134 2.441141        6715.481\n10 BJ-000 11.35827 2.422149        6887.261\n\n\n\n10.1.1 Comparing results by polygons and by points\n\np1 &lt;- hist(ave_dist_rivers_points$ave_dist_rivers)\n\n\n\n\n\n\n\np2 &lt;- hist(ave_dist_rivers$ave_dist_rivers)\n\n\n\n\n\n\n\n\n\nplot(p1, col=rgb(0,0,1,1/4))  # first histogram\nplot(p2, col=rgb(1,0,0,1/4), add=T)  # add\nlegend(\"topright\", legend=c( \"ave_dist_rivers_points\",\"ave_dist_rivers\"), \n       fill=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)), border=NA)"
  },
  {
    "objectID": "extract_by_point.html",
    "href": "extract_by_point.html",
    "title": "1: Extract spatial data based on spatial point",
    "section": "",
    "text": "1 Introduction\n  2 Code\n  \n  2.1 Set Up\n  2.2 Read the Data\n  2.3 Georeference the Surveys\n  2.4 Crop the spatial variables\n  2.5 Plot\n  2.6 Modify the Spatial Variables\n  2.7 Extraction\n  2.8 Cmpute Long Run Climatic Parameter\n  2.9 Merge with Survey\n  2.10 Write\n  \n  3 Take home messages\n  4 Appendix\n  \n  4.1 Want to know about the data?"
  },
  {
    "objectID": "extract_by_point.html#introduction",
    "href": "extract_by_point.html#introduction",
    "title": "1: Extract spatial data based on spatial point",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis tutorial show how too extract spatial control variables based on surveys coordinate locations. The survey refers to Ethiopia 2019 and comes from the The World Bank Living Standards Measurement Study (LSMS). The spatial variables are nighttime light, agroecological zones, Urban-Rural Catchment Area, elevation, and climatic parameters. More information about these datasets are in the Appendix."
  },
  {
    "objectID": "extract_by_point.html#code",
    "href": "extract_by_point.html#code",
    "title": "1: Extract spatial data based on spatial point",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Set Up\nWe start by setting up the stage for our analysis. First, we load the necessary packages. We load only climatic4economist package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\nlibrary(climatic4economist)\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note .. means one step back to the folder directory, i.e. one folder back.\nNote that how to set up the paths depends on your folder organization but there are overall two approaches:\n\nyou can use the R project, by opening the project directly you don’t need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder.\nyou can manually set the working folder with the function setwd().\n\n\n# path to data folder\n1path_to_data &lt;- file.path(\"..\",\n2                          \"..\", \"data\")\n\n# survey and administrative division\npath_to_survey  &lt;- file.path(path_to_data, \"survey\", \"LSMS\", \"LSMS_ETH19.dta\")\npath_to_adm_div &lt;- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tpr.nc\")\npath_to_tmp &lt;- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tmp.nc\")\n\n# control variables\npath_to_elevation  &lt;- file.path(path_to_data, \"spatial\", \"elevation\", \"GloFAS\",\n                               \"elevation_glofas_v4_0.nc\")\npath_to_urca       &lt;- file.path(path_to_data, \"spatial\", \"URCA\", \n                               \"URCA.tif\")\npath_to_pop        &lt;- file.path(path_to_data, \"spatial\", \"population\", \"WorldPop\",\n                               \"uncontraint_1km_global\", \"ppp_2019_1km_Aggregated.tif\")\npath_to_nightlight &lt;- file.path(path_to_data, \"spatial\", \"nighttime_light\",\n                                \"VIIRS\", \"VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif\")\npath_to_aez        &lt;- file.path(path_to_data, \"spatial\", \"AgroEcological\", \"AEZ\", \n                                \"GAEZv5\",  \"GAEZ-V5.AEZ33-10km.tif\")\n\n# to result folder\npath_to_result &lt;- file.path(path_to_data, \"result\")\n\n\n1\n\nconcatenate the string to make a path\n\n2\n\n.. means one folder back\n\n\n\n\n\n\n\n2.2 Read the Data\n\n2.2.1 Survey Data\nWe start by reading the surveys data. The survey is stored as dta file, so we use the haven::read_dta() function to read it.\nWe only need the hhid, the survey coordinates, and the interview dates. We use dplyr::select() to choose these variables. This passage is optional and we bring with us all the variables, but we won’t use them.\nThen we create/modify some variables with the function dplyr::mutate(). We transform the the variable interview_date from string into data, and we get the year of the median value of the date of interviews. This passage is important as it allows us to define the most appropriate year to select for the spatial variables.\n\n1srvy &lt;- haven::read_dta(path_to_survey) |&gt;\n2    dplyr::select(survey_year, hhid, country, lat, lon, interview_date) |&gt;\n    dplyr::mutate(\n3        interview_date = clock::date_parse(interview_date,\n4                                           format = \"%Y-%m-%d\"),\n5        survey_year    = clock::get_year(median(interview_date)),\n        .before = hhid)\n\n\n1\n\nread dta type data\n\n2\n\nselect relevant variables\n\n3\n\ntransform string into date type\n\n4\n\nspecify format type\n\n5\n\nfind the median year of the interviews\n\n\n\n\n\n\n2.2.2 Spatial Data\nFinally, we load the spatial data. This data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or “cell” in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region.\nTo spatial data is usually stored as tif file or nc. We can read both of them them with the function terra::rast().\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS), i.e. EPSG:4326.\n\n\n\n\n\n\nImportant\n\n\n\nWhen working with multiple spatial data, you must ensure that they have the same coordinate reference system (CRS). This is important because in this way all the data can “spatially” talk to each other.\n\n\n\n1nightlight &lt;- terra::rast(path_to_nightlight) |&gt;\n2  setNames(\"nightlight\")\nnightlight\n\nelevation &lt;- terra::rast(path_to_elevation)\nelevation\n\nurca &lt;- terra::rast(path_to_urca)\nurca\n\naez &lt;- terra::rast(path_to_aez) |&gt;\n    setNames(\"aez\") \naez\n\n\n1\n\nread raster type data\n\n2\n\nchange the name of the layer\n\n\n\n\nclass       : SpatRaster \ndimensions  : 33601, 86401, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : -180.0021, 180.0021, -65.00208, 75.00208  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif \nname        : nightlight \nclass       : SpatRaster \ndimensions  : 3000, 7200, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : elevation_glofas_v4_0.nc \nvarname     : elevation (Height above sea level) \nname        : elevation \nunit        :         m \nclass       : SpatRaster \ndimensions  : 17235, 43200, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : -180, 180, -60, 83.625  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : URCA.tif \nname        : URCA \nclass       : SpatRaster \ndimensions  : 2160, 4320, 1  (nrow, ncol, nlyr)\nresolution  : 0.08333333, 0.08333333  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : GAEZ-V5.AEZ33-10km.tif \nname        : aez \n\n\nNow we also read the weather observation. The same consideration about the coordinate reference system (CRS) is still valid. When we work with raster that have also observations over time, it is important to check how and where the time and date information is stored. Sometimes it is stored in the metadata and you can access it using terra::time(), other time it is already saved as the name of the layer and you can access it using names(). Sometimes, like in this case the date information is stored in the names but the format is based on second passed from 1970-01-01 00:00. To transform this observation into readable date we can use the function second_to_date().\n\n\n\n\n\n\nWarning\n\n\n\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n\n\n\npre &lt;- terra::rast(path_to_pre)\npre\n1names(pre) &lt;- terra::names(pre) |&gt; second_to_date()\npre\n\ntmp &lt;- terra::rast(path_to_tmp)\nnames(tmp) &lt;- terra::names(tmp) |&gt; second_to_date()\n\n\n1\n\ntransform the layers name with second into dates\n\n\n\n\nclass       : SpatRaster \ndimensions  : 741, 811, 904  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : -26.05, 55.05, -36.05, 38.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : afr_month_50_25_tpr.nc:tp \nvarname     : tp (Total precipitation) \nnames       : tp_va~52000, tp_va~73600, tp_va~54400, tp_va~76000, tp_va~84000, tp_va~05600, ... \nunit        :           m,           m,           m,           m,           m,           m, ... \nclass       : SpatRaster \ndimensions  : 741, 811, 904  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : -26.05, 55.05, -36.05, 38.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : afr_month_50_25_tpr.nc:tp \nvarname     : tp (Total precipitation) \nnames       : 1950-01-01, 1950-02-01, 1950-03-01, 1950-04-01, 1950-05-01, 1950-06-01, ... \nunit        :          m,          m,          m,          m,          m,          m, ... \n\n\n\n\n2.2.3 Administrative Boundaries\nWe now move to read the administrative divisions. We use the function read_geoBoundaries() to do so. This function looks for spatial polygons for the iso and lvl provided provided.\nAs we have the coordinates, we don’t actually need the administrative divisions for the extraction. However, we will use it to reduce the coverage of the spatial variables and to make some plots.\nThe same consideration about the coordinate reference system (CRS) is still valid.\n\nadm_div &lt;- read_geoBoundaries(path_to_adm_div, iso = \"ETH\", lvl = 2)\nadm_div\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 74, 4  (geometries, attributes)\n extent      : 33.00224, 47.95925, 3.400365, 14.84602  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_adm_div   iso adm_div_1 adm_div_2\n type        :      &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n values      :          1   ETH    Somali     Afder\n                        2   ETH   Gambela    Agnuak\n                        3   ETH     SNNPR     Alaba\n\n\n\n\n\n\n2.3 Georeference the Surveys\nAs we’ve mentioned, the spatial data is georeferenced, so we need to ensure the same for the survey data. Since many households share the same coordinates, they are linked to the same location. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called ID.\nThis is handled by the prepare_coord() function, which requires the coordinates’ variable names as input.\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the georef_coord() function requires the coordinates’ variable names as input. Usually, the WGS 84 CRS is the default coordinate references system for coordinates. In this case it matches the weather coordinate references system.\nWe can print the result to check the transformation. The new column, ID, is created by prepare_coord() and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\nsrvy_coord &lt;- prepare_coord(srvy,\n                            lon_var = lon,\n                            lat_var = lat)\nsrvy_coord\n\n# A tibble: 6,505 × 7\n   ID    survey_year hhid               country    lat   lon interview_date\n   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        \n 1 1            2019 051103088801903002 Ethiopia  3.61  39.0 2019-08-28    \n 2 1            2019 051103088801903012 Ethiopia  3.61  39.0 2019-08-28    \n 3 1            2019 051103088801903022 Ethiopia  3.61  39.0 2019-08-28    \n 4 1            2019 051103088801903032 Ethiopia  3.61  39.0 2019-08-29    \n 5 1            2019 051103088801903042 Ethiopia  3.61  39.0 2019-08-29    \n 6 1            2019 051103088801903052 Ethiopia  3.61  39.0 2019-08-28    \n 7 1            2019 051103088801903062 Ethiopia  3.61  39.0 2019-08-28    \n 8 1            2019 051103088801903072 Ethiopia  3.61  39.0 2019-08-28    \n 9 1            2019 051103088801903082 Ethiopia  3.61  39.0 2019-08-28    \n10 1            2019 051103088801903092 Ethiopia  3.61  39.0 2019-08-29    \n# ℹ 6,495 more rows\n\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The function also the coordinates’ variable names as input.\n\nsrvy_geo &lt;- georef_coord(srvy_coord,\n                         geom = c(\"lon\", \"lat\"),\n                         crs = \"EPSG:4326\")\nsrvy_geo\n\n class       : SpatVector \n geometry    : points \n dimensions  : 516, 1  (geometries, attributes)\n extent      : 33.43483, 47.30784, 3.609384, 14.47715  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    ID\n type        : &lt;chr&gt;\n values      :     1\n                   2\n                   3\n\n\n\n\n\n\n\n\nNote\n\n\n\nPay attention on the reduced number of observation between srvy_coord and srvy_geo. From 6505 rows to 516, these are the actual unique locations from the survey.\n\n\n\n\n\n2.4 Crop the spatial variables\nThe spatial variables variables we have just load have a global coverage. It might be convenient to reduce the coverage to just the countries we are interested in. We can do this by using the crop_with_buffer() function and the administrative divisions. As the name suggest, this function allows to specify a buffer around the vector data to increase the spatial extent and crop a larger portion. This is useful as some survey coordinates are at the edge of the administrative borders or, in some rare cases, just outside the borders as consequence of the coordinates modification fro location anonymization. Further, to compute some spatial indicators in one cell we need the surrounding cell values and if we crop exactly at the borders those cell values at the edge won’t have the he surrounding cells.\nThe buffer argument of the function specifies the increase around the spatial extent. By default, it is in the same unit of measure of the data.\nThis is not a compulsory step but it reduce the memory burden and allows for more meaningful plotting.\n\nnghtlght_cntry &lt;- crop_with_buffer(nightlight, adm_div, buffer = 1)\n\nelevatn_cntry &lt;- crop_with_buffer(elevation, adm_div, buffer = 1)\n\nurca_cntry &lt;- crop_with_buffer(urca, adm_div, buffer = 1)\n\naez_cntry &lt;- crop_with_buffer(aez, adm_div, buffer = 1)\n\npre_cntry &lt;- crop_with_buffer(pre, adm_div, buffer = 1)\n\ntmp_cntry &lt;- crop_with_buffer(tmp, adm_div, buffer = 1)\n\n\n\n\n2.5 Plot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n1terra::plot(adm_div, col = \"grey\", main = \"District of Ethiopia and Survey Coordinates\")\n2terra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n1\n\nplot raster\n\n2\n\nadd survey locations\n\n\n\n\n\n\n\n\n\n\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the major cities and in the North.\nNext, we plot the spatial variables to see how it overlaps with the spatial coordinates.\n\n1terra::plot(elevatn_cntry, main = \"Elevation\")\n2terra::lines(adm_div, col = \"white\", lwd = 1)\n3terra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n1\n\nplot raster\n\n2\n\nadd administrative borders\n\n3\n\nadd survey locations\n\n\n\n\n\n\n\n\n\n\nterra::plot(urca_cntry, main = \"URCA\")\nterra::lines(adm_div, col = \"black\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\n\n\n\n\n\n\nterra::plot(log(1+nghtlght_cntry), main = \"Log Nighttime Light\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\n\n\n\n\n\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"ryb\"),\n            main = \"Monthly temperature at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"black\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the different spatial resolution, with precipitation having a lower one. The consequence is that some survey coordinates still fall within the same cell.\n\n\n\n2.6 Modify the Spatial Variables\n\n2.6.1 Compute Terrain Indicators\nNow we compute some terrain indicators based on elevation. The terrain indicators are:\n\nTRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and its 8 surrounding cells.\nSlope is the average difference between the value of a cell and its 8 surrounding cells.\nRoughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells.\n\n\nterrain_cntry &lt;-  terra::terrain(elevatn_cntry,\n1                                 v = c(\"slope\", \"TRI\", \"roughness\"),\n2                                 neighbors = 8,\n                                 unit = \"degrees\")\n\n\n1\n\nthe terrain indicators\n\n2\n\nhow many neighboring cells, 8 (queen case) or 4 (rook case)\n\n\n\n\n\n\n2.6.2 Weather Variable Transformation\nThe original unit of measure of the weather data is in meter for precipitation and Kelvin for temperature. These unit of measure are not very intuitive, therefore we change them into millimeter and Celsius respectively.\n\n# From meter to millimeters\npre_cntry_mm &lt;- pre_cntry*1000\n\n# From Kelvin to Celsius\ntmp_cntry_c &lt;- tmp_cntry - 273.15\n\n\n\n\n\n2.7 Extraction\nNext, we extract the spatial data based on the survey coordinates using the extract_by_coord() function. This function requires the raster with the spatial data and the georeferenced coordinates as inputs.\nLooking at the result, we see first the ID column, that identifies the unique survey coordinates. The second and third column are the coordinates of the cells. The other columns contain the spatial observations, specific to each coordinate. For the weather data we have the time series of observations over time, specific to each coordinate.\n\nnghtlght_coord &lt;- extract_by_coord(nghtlght_cntry, srvy_geo)\nnghtlght_coord\n\n# A tibble: 516 × 4\n   ID    x_cell y_cell nightlight\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 1       39.0   3.61      0    \n 2 2       41.8   4.01      0    \n 3 3       41.9   4.44      0    \n 4 4       41.5   4.73      0    \n 5 5       36.0   4.75      0    \n 6 6       38.1   4.85      0    \n 7 7       37.4   4.97      0    \n 8 8       40.7   5.11      0.816\n 9 9       41.9   5.15      0    \n10 10      44.6   5.24      0    \n# ℹ 506 more rows\n\nelevation_coord &lt;- extract_by_coord(elevatn_cntry, srvy_geo)\n\nterrain_coord &lt;- extract_by_coord(terrain_cntry, srvy_geo)\n\nurca_coord &lt;- extract_by_coord(urca_cntry, srvy_geo)\n\naez_coord &lt;- extract_by_coord(aez_cntry, srvy_geo)\n\npre_coord &lt;- extract_by_coord(pre_cntry, srvy_geo)\n\ntmp_coord &lt;- extract_by_coord(tmp_cntry, srvy_geo)\ntmp_coord\n\n# A tibble: 516 × 907\n   ID    x_cell y_cell X1950_01_01 X1950_02_01 X1950_03_01 X1950_04_01\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1       39     3.60        296.        298.        295.        294.\n 2 2       41.8   4.00        303.        304.        300.        299.\n 3 3       41.9   4.40        303.        304.        300.        299.\n 4 4       41.5   4.7         301.        302.        299.        298.\n 5 5       36     4.7         301.        304.        300.        299.\n 6 6       38.1   4.8         293.        295.        292.        292.\n 7 7       37.4   5.00        294.        296.        293.        292.\n 8 8       40.7   5.1         296.        298.        295.        294.\n 9 9       41.9   5.1         303.        303.        300.        298.\n10 10      44.6   5.2         301.        302.        301.        300.\n# ℹ 506 more rows\n# ℹ 900 more variables: X1950_05_01 &lt;dbl&gt;, X1950_06_01 &lt;dbl&gt;,\n#   X1950_07_01 &lt;dbl&gt;, X1950_08_01 &lt;dbl&gt;, X1950_09_01 &lt;dbl&gt;, X1950_10_01 &lt;dbl&gt;,\n#   X1950_11_01 &lt;dbl&gt;, X1950_12_01 &lt;dbl&gt;, X1951_01_01 &lt;dbl&gt;, X1951_02_01 &lt;dbl&gt;,\n#   X1951_03_01 &lt;dbl&gt;, X1951_04_01 &lt;dbl&gt;, X1951_05_01 &lt;dbl&gt;, X1951_06_01 &lt;dbl&gt;,\n#   X1951_07_01 &lt;dbl&gt;, X1951_08_01 &lt;dbl&gt;, X1951_09_01 &lt;dbl&gt;, X1951_10_01 &lt;dbl&gt;,\n#   X1951_11_01 &lt;dbl&gt;, X1951_12_01 &lt;dbl&gt;, X1952_01_01 &lt;dbl&gt;, …\n\n\nAgain we have a row for each unique location from the survey. However, if we want to know how many different cells there are we can look unique cell coordinates.\n\nunique_cell &lt;- tmp_coord |&gt;\n1  dplyr::distinct(x_cell, y_cell)\nnrow(unique_cell)\n\n\n1\n\nidentifies the unique combination of the variables\n\n\n\n\n[1] 365\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that now the number of rows is 365, this is the actual different weather observations that we can merge with the survey. We start with 6505 different household, then we have 516 different survey coordinates, and we end up with 365 different weather observations.\n\n\n\n\n\n2.8 Cmpute Long Run Climatic Parameter\nWe want to describe the long run climatic condition in each locations. Rule of thumb is to use 30 years of weather observations to capture climatic features. Therefore, we select the 30 years before each survey.\nCheck the names with the date of observations and how it has changed since before.\n\npre_coord_30yrs &lt;- select_by_dates(pre_coord, from = \"1989\", to = \"2019\")\ntmp_coord_30yrs &lt;- select_by_dates(tmp_coord, from = \"1989\", to = \"2019\")\n\ntmp_coord_30yrs\n\n# A tibble: 516 × 364\n   ID    x_cell y_cell X1989_01_01 X1989_02_01 X1989_03_01 X1989_04_01\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1       39     3.60        297.        297.        297.        294.\n 2 2       41.8   4.00        303.        304.        303.        300.\n 3 3       41.9   4.40        303.        304.        303.        300.\n 4 4       41.5   4.7         301.        301.        301.        299.\n 5 5       36     4.7         303.        303.        304.        302.\n 6 6       38.1   4.8         294.        295.        294.        292.\n 7 7       37.4   5.00        296.        296.        296.        294.\n 8 8       40.7   5.1         297.        297.        297.        295.\n 9 9       41.9   5.1         303.        303.        303.        300.\n10 10      44.6   5.2         301.        301.        303.        302.\n# ℹ 506 more rows\n# ℹ 357 more variables: X1989_05_01 &lt;dbl&gt;, X1989_06_01 &lt;dbl&gt;,\n#   X1989_07_01 &lt;dbl&gt;, X1989_08_01 &lt;dbl&gt;, X1989_09_01 &lt;dbl&gt;, X1989_10_01 &lt;dbl&gt;,\n#   X1989_11_01 &lt;dbl&gt;, X1989_12_01 &lt;dbl&gt;, X1990_01_01 &lt;dbl&gt;, X1990_02_01 &lt;dbl&gt;,\n#   X1990_03_01 &lt;dbl&gt;, X1990_04_01 &lt;dbl&gt;, X1990_05_01 &lt;dbl&gt;, X1990_06_01 &lt;dbl&gt;,\n#   X1990_07_01 &lt;dbl&gt;, X1990_08_01 &lt;dbl&gt;, X1990_09_01 &lt;dbl&gt;, X1990_10_01 &lt;dbl&gt;,\n#   X1990_11_01 &lt;dbl&gt;, X1990_12_01 &lt;dbl&gt;, X1991_01_01 &lt;dbl&gt;, …\n\n\nNow we can compute the long run climatic parameter. We calculate the mean, the standard deviation, and the coefficient of variation. We collect all the parameter in a separate object parameter. This object is a names list of functions and we construct it with this structure name = function, then the list() function puts them together. This passage is not compulsory but allows to perform the computation of multiple parameters in a tidy and efficient way. Otherwise we could have directly add them inside the calc_par().\nThe function calc_par() calculates the required parameters.\nThe results have a similar structure, with the first columns that identify the specific locations and the other the computed parameters. Note how we are still carrying on the coverage_fraction variable as we will need it for aggregating the climatic parameter at the administrative division.\n\nparameter &lt;- list(std = sd, avg = mean, coef_var = cv)\nparameter\n\n$std\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n&lt;bytecode: 0x11edeff30&gt;\n&lt;environment: namespace:stats&gt;\n\n$avg\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x10c127cf8&gt;\n&lt;environment: namespace:base&gt;\n\n$coef_var\nfunction(x, na_rm = TRUE) {\n    avg &lt;- mean(x, na.rm = na_rm)\n    if (is.nan(avg)) return(NA_real_)\n    if (avg == 0) return(NA_real_)  # Avoid division by zero\n    sd(x, na.rm = na_rm) / mean(x, na.rm = na_rm)\n}\n&lt;bytecode: 0x11ede4320&gt;\n&lt;environment: namespace:climatic4economist&gt;\n\npre_par_coord &lt;- calc_par(pre_coord_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_coord &lt;- calc_par(tmp_coord_30yrs, pars = parameter, prefix = \"tmp\")\ntmp_par_coord\n\n# A tibble: 516 × 6\n   ID    x_cell y_cell tmp_std tmp_avg tmp_coef_var\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 1       39     3.60    1.71    295.      0.00580\n 2 10      44.6   5.2     1.05    302.      0.00347\n 3 100     37.9   7.5     1.35    290.      0.00464\n 4 101     45     7.5     1.17    298.      0.00392\n 5 102     36.8   7.6     1.31    291.      0.00449\n 6 103     34.3   7.7     2.45    302.      0.00811\n 7 104     38.3   7.7     1.41    292.      0.00481\n 8 105     34.1   7.7     2.45    302.      0.00812\n 9 106     37.4   7.7     1.79    293.      0.00613\n10 107     37.5   7.8     1.55    291.      0.00534\n# ℹ 506 more rows\n\n\n\n\n\n2.9 Merge with Survey\nNow that we have everything, we can combine all the extracted data and then merge them with the survey. We start by combining the data into a unique data set. To do so we start by create a list with the function list(), each element of the list is a different spatial variable.\nWe need to drop the cell coordinates. This because, the cell resolution is different among the spatial datasets, despite the same CRS, and thus the coordinates of the cells are slightly different among datasets. This impinges the merge and at this stage of the analysis we don’t need the information they are carrying anymore.\nTo drop them requires a convoluted approach, but it takes advantage that all the datasets are grouped in the same list and the procedure is the same for each dataset. To apply a function to each element of a list we can use the function purrr::map(). As arguments, this function requires the function we want to apply, namely dplyr::select, and additional arguments for the function, -c(x_cell, y_cell) which are the columns we want to to drop. Note the minus symbol as it tells the function we want to drop the columns and not keep them.\nThen we combine the elements of the list with the function purrr::reduce(). This last function require another function as input to drive the combination and we choose to use merge_by_common(), which merges two data by their common variable names.\nWhy not using directly merge_by_common()? Because the function works with just two datasets and we have eight different spatial datasets. We can cumulatively merge the datasets one by one or we can use the purrr::reduce().\nThen we compute the logarithmic transformation of the nighttime light, with the dplyr::mutate() function. We use the argument .after to specify where the position of the variable.\n\n1sptl_coord &lt;- list(nghtlght_coord,\n                   terrain_coord, \n                   elevation_coord, \n                   urca_coord, \n                   aez_coord, \n                   pre_par_coord, \n                   tmp_par_coord) |&gt;\n2    purrr::map(dplyr::select, -c(x_cell, y_cell)) |&gt;\n3    purrr::reduce(merge_by_common) |&gt;\n4    dplyr::mutate(ln_nightlight = log(1+nightlight), .after = nightlight)\nsptl_coord\n\n\n1\n\ncombine the data into a list\n\n2\n\nremove variables from each element of the list\n\n3\n\nmerge all the elements of the list\n\n4\n\ncreate new variable\n\n\n\n\n# A tibble: 516 × 15\n   ID    nightlight ln_nightlight  slope    TRI roughness elevation  URCA   aez\n   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 1          0             0     0.394   37.7     138.       1140.    13     1\n 2 2          0             0     0.274   23.7      67.0       249.    19    26\n 3 3          0             0     0.0540  16.3      38.1       196.    12    26\n 4 4          0             0     0.961   73.5     215.        466.    30    26\n 5 5          0             0     0.0237   1.94      7.63      369.    13    29\n 6 6          0             0     2.10   195.      619.       1838.    12    26\n 7 7          0             0     0.939   81.4     252.       1514.    19     1\n 8 8          0.816         0.597 0.625   42.6     155.       1215.     5     1\n 9 9          0             0     0.264   18.2      60.9       281.    12    26\n10 10         0             0     0.512   38.3     149.        241.    20    29\n# ℹ 506 more rows\n# ℹ 6 more variables: pre_std &lt;dbl&gt;, pre_avg &lt;dbl&gt;, pre_coef_var &lt;dbl&gt;,\n#   tmp_std &lt;dbl&gt;, tmp_avg &lt;dbl&gt;, tmp_coef_var &lt;dbl&gt;\n\n\nNow that we have all the control variables together, we can merge them with the surveys information. The function merge_by_common() will do it for us.\nWe can see that the result has all the information we retained from the surveys and the new extracted spatial variables.\n\nsrvy_sptl_coord &lt;- merge_by_common(srvy_coord, sptl_coord)\nsrvy_sptl_coord\n\n# A tibble: 6,505 × 21\n   ID    survey_year hhid          country   lat   lon interview_date nightlight\n   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;              &lt;dbl&gt;\n 1 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 2 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 3 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 4 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-29              0\n 5 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-29              0\n 6 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 7 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 8 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n 9 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-28              0\n10 1            2019 051103088801… Ethiop…  3.61  39.0 2019-08-29              0\n# ℹ 6,495 more rows\n# ℹ 13 more variables: ln_nightlight &lt;dbl&gt;, slope &lt;dbl&gt;, TRI &lt;dbl&gt;,\n#   roughness &lt;dbl&gt;, elevation &lt;dbl&gt;, URCA &lt;int&gt;, aez &lt;int&gt;, pre_std &lt;dbl&gt;,\n#   pre_avg &lt;dbl&gt;, pre_coef_var &lt;dbl&gt;, tmp_std &lt;dbl&gt;, tmp_avg &lt;dbl&gt;,\n#   tmp_coef_var &lt;dbl&gt;\n\n\n\n\n\n2.10 Write\nHere we are at the end, let’s save the results. We want to save the result as dta so we will use the haven::write_dta() function.\n\nhaven::write_dta(srvy_sptl_coord,\n                 file.path(path_to_result, \"ETH_sp_coord.dta\"))"
  },
  {
    "objectID": "extract_by_point.html#take-home-messages",
    "href": "extract_by_point.html#take-home-messages",
    "title": "1: Extract spatial data based on spatial point",
    "section": "3 Take home messages",
    "text": "3 Take home messages\n\nWhen working with multiple spatial data:\n\nremember to control the Coordinate Reference System of all dataset\nplot the data to check everything is going well\n\nbased on the typology of data we use different function to read them\n\nfor dta use haven::read_dta() or and haven::_write_dta()\nfor spatial vectors read_GAUL() for administrative divisions in specific country and level, otherwise terra::vect()\nfor spatial raster use terra::rast()\n\nExtraction\n\nsince many household share the same locations, we take advantage of this by extracting the weather data only at the unique locations, we achieve this with the function prepare_coord().\nsame of these unique locations may fall within the same value cells, so the actual information might be even lower.\n\nWhen working with raster data\n\ncheck the unit of measure\nif it is a time series check also the date format\n\nWhen working with survey and time series data, remember to select the appropriate observations, for example not those that happened after the interview."
  },
  {
    "objectID": "extract_by_point.html#appendix",
    "href": "extract_by_point.html#appendix",
    "title": "1: Extract spatial data based on spatial point",
    "section": "4 Appendix",
    "text": "4 Appendix\n\n4.1 Want to know about the data?\n\n4.1.1 Weather\nWeather observation are obtained from ERA5-Land reanalysis dataset. H-TESSEL is the land surface model that is the basis of ERA5-Land. The data is a post-processed monthly-mean average of the original ERA5-Land dataset.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.1° x 0.1° lon lat\n\n\ntemporal resolution\nmonth\n\n\ntime frame\nJan. 1950 - Dec. 2022\n\n\nunit of measure\nmeter or Kelvin\n\n\n\nSuggested citation:\n\nMuñoz Sabater, J. (2019): ERA5-Land monthly averaged data from 1950 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). DOI: 10.24381/cds.68d2bb30\n\nIt is possible to find additional information:\n\nhere\nand the related manual here.\n\nThe data can be freely download from\n\nhere.\n\n\n4.1.1.0.1 Total precipitation\nAccumulated liquid and frozen water, including rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Precipitation variables do not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth.\n\n\n4.1.1.0.2 2 metre above ground temperature\nTemperature of air at 2m above the surface of land, sea or in-land waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth’s surface, taking account of the atmospheric conditions.\n\n\n\n\n4.1.2 Spatial variables\n\n4.1.2.1 Agro Ecological Zones\nThe Agro-ecological Zones classification (33 classes) provides a characterization of bio-physical resources relevant to agricultural production systems. AEZ definitions and map classes follow a rigorous methodology and an explicit set of principles. The inventory combines spatial layers of thermal and moisture regimes with broad categories of soil/terrain qualities. It also indicates locations of areas with irrigated soils and shows land with severely limiting bio-physical constraints including very cold and very dry (desert) areas as well as areas with very steep terrain or very poor soil/terrain conditions. The AEZ classification dataset is part of the GAEZ v5 Land and Water Resources theme and Agro-ecological Zones sub-theme. All results are derived from the Agro-ecological Zones (AEZ) modeling framework, developed collaboratively by the Food and Agriculture Organization (FAO) and the International Institute for Applied Systems Analysis (IIASA).\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n10 km.\n\n\ntemporal resolution\n20 years\n\n\ntime frame\n2001–2020\n\n\nunit of measure\nclassification by climate/soil/terrain/LC (33 classes)\n\n\n\nSuggested citation:\n\nFAO & IIASA. 2025. Global Agro-ecological Zoning version 5 (GAEZ v5) Model Documentation. https://github.com/un-fao/gaezv5/wiki\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.2 Urban-Rural Catchment Area (URCA)\nUrban–rural catchment areas showing the catchment areas around cities and towns of different sizes (the no data value is 128). Each rural pixel is assigned to one defined travel time category to one of seven urban agglomeration sizes.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.03° x 0.03° lon lat\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2015\n\n\nunit of measure\ntravel time category to different urban hierarchy\n\n\n\nSuggested citation:\n\nCattaneo, Andrea; Nelson, Andy; McMenomy, Theresa (2020). Urban-rural continuum. figshare. Dataset. https://doi.org/10.6084/m9.figshare.12579572.v4\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.3 Population\nThe units are number of people per pixel. The mapping approach is Random Forest-based dasymetric redistribution.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n30 arc second (~1km)\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2010 - 2020\n\n\nunit of measure\nestimated count of people per grid-cell\n\n\n\nSuggested citation:\n\nWorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00647\n\nIt is possible to find additional information from:\n\nhere\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.4 Nighttime light\nVIIRS nighttime lights (VNL) version V2.1: annual values obtained by from the monthly averages with filtering to remove extraneous features such as biomass burning, aurora, and background.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n15 arc second\n\n\ntemporal resolution\nyear\n\n\ntime frame\n2012 - 2021\n\n\nunit of measure\nnW/cm2/sr, average-masked\n\n\n\nSuggested citation:\n\nElvidge, C.D, Zhizhin, M., Ghosh T., Hsu FC, Taneja J. Annual time series of global VIIRS nighttime lights derived from monthly averages:2012 to 2019. Remote Sensing 2021, 13(5), p.922, doi:10.3390/rs13050922\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.2.5 Elevation\nThe Global Flood Awareness System (GloFAS) is one component of the Copernicus Emergency Management Service (CEMS). It is designed to support preparatory measures for flood events worldwide, particularly in large transnational river basins.\nElevation is obtained from the auxiliary variables of GloFAS. Each pixel is the mean height elevation above sea level.\n\n\n\nParameter\nValue\n\n\n\n\nspatial resolution\n0.03° x 0.03° lon lat\n\n\ntemporal resolution\n30 years\n\n\ntime frame\n1981 - 2010\n\n\nunit of measure\nMeter (m)\n\n\n\nWeb resources:\n\nhere\n\nData access:\n\nhere\n\n\n\n\n\n4.1.3 Survey\nThe Living Standards Measurement Study - Integrated Surveys on Agriculture (LSMS-ISA) is a unique system of longitudinal surveys designed to improve the understanding of household and individual welfare, livelihoods and smallholder agriculture in Africa. The LSMS team works with national statistics offices to design and implement household surveys with a strong focus on agriculture.\nSuggested citation:\n\nCentral Statistics Agency of Ethiopia. (2020). Socioeconomic Survey 2018-2019 [Data set]. World Bank, Development Data Group. https://doi.org/10.48529/K739-C548\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere.\n\n\n\n\n4.1.4 Administrative boundaries\nThe administrative divisions are obtained from GeoBoundaries[^2]. GeoBoundaries Built by the community and William & Mary geoLab, the geoBoundaries Global Database of Political Administrative Boundaries Database is an online, open license (CC BY 4.0) resource of information on administrative boundaries (i.e., state, county) for every country in the world. Since 2016, we have tracked approximately 1 million boundaries within over 200 entities, including all UN member states.\nSuggested citation:\n\nRunfola D, Anderson A, Baier H, Crittenden M, Dowker E, Fuhrig S, et al. (2020) geoBoundaries: A global database of political administrative boundaries. PLoS ONE 15(4): e0231866. https://doi.org/10.1371/journal.pone.0231866.\n\nIt is possible to find additional information:\n\nhere.\n\nThe data can be freely download from:\n\nhere."
  },
  {
    "objectID": "r_reminder.html",
    "href": "r_reminder.html",
    "title": "0: Reminder of some R concepts",
    "section": "",
    "text": "New to R? Read this before the tutorials!\n  \n  How to set up a working directory\n  R Project\n  The pipe command\n  The package namespaces\n  The assign operator\n  Functions"
  },
  {
    "objectID": "r_reminder.html#new-to-r-read-this-before-the-tutorials",
    "href": "r_reminder.html#new-to-r-read-this-before-the-tutorials",
    "title": "0: Reminder of some R concepts",
    "section": "New to R? Read this before the tutorials!",
    "text": "New to R? Read this before the tutorials!\n\nHow to set up a working directory\nA working directory is the folder where R reads and saves files by default.\n\n# Replace the path below with your own folder path\nsetwd(\"path/to/your/folder\")\n# or\nsetwd(file.path(\"path\", \"to\", \"your\", \"folder\"))\n\n# Confirm the working directory is set\ngetwd()\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively you can create an R Project\n\n\n\n\n\nR Project\nAn R Project is a simple way to organize your work in RStudio or R.\nIt automatically sets your working directory to the project folder, making your workflow smoother and reproducible.\nR Project has a particular type of extension .Rproj, and usually has this icon: \n\nStep 1 - creates a .Rproj file in your project folder\n\nIn RStudio, go to File &gt; New Project…\n\nChoose New Directory or Existing Directory (if you already have a folder ready)\n\nGive your project a name and location\n\nClick Create Project\n\nStep 2: Open Your R Project\n\nDouble-click the .Rproj file or open it from RStudio’s Recent Projects\n\nThe working directory is automatically set to the project folder\n\n\n\n\n\n\n\n\nNote\n\n\n\nR Project have their own custom options, you can modify them trhought the global option window\n\n\n\n\n\nThe pipe command\nThe pipe command |&gt;. It lets you pass the result of one expression as the first argument to the next, creating a fluid chain of functions.\nInstead of nesting functions inside each other, you can pipe the output forward, making the code easier to read.\n\n4 |&gt; log() |&gt; exp()\n\nexp(log(4))\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe base R pipe |&gt; was introduced in R 4.1.0.\nIn some tutorials, you might also see %&gt;%, which comes from the magrittr or dplyr packages. Both do a similar thing, but |&gt; is now the official base R version.\n\n\n\n\n\n\nThe package namespaces\nThe package namespaces package_name::function_name(). As the name suggests, namespaces provide “spaces” for “names”. They provide a context for looking up the value of an object associated with a name. When we write terra::vect() we are asking R to look for the function vect() in the terra package.\nIt’s a fairly advanced topic, and by-and-large, not that important! When you first start using namespaces, it’ll seem like a lot of work for little gain. However, having a high quality namespace helps encapsulate your package and makes it self-contained. This ensures that other packages won’t interfere with your code, that your code won’t interfere with other packages, and that your package works regardless of the environment in which it’s run.\nYou can avoid using every time the name space by just loading the necessary packages at the beginning of the code (in the set up section for example). This is the most known and common approach. To do so just add library(name_of_package), for example library(terra). Then we can just call the function without the name space, like this vect().\n\n\n\nThe assign operator\nThe assign operator &lt;-. This is a peculiarity of R and it is used to assign values to variables. However, &lt;- is preferred in R scripts because it makes assignments visually distinct from comparisons (==) and function arguments (=).\nNote that the operators &lt;- and = can be used, almost interchangeably. However, inside function calls, you should use = to name arguments, in some situations it creates unintended results.\n\n# wrong\navg &lt;- mean(x, na.rm &lt;- TRUE)\n\n# correct\navg &lt;- mean(x, na.rm = TRUE)\n\n\n\n\nFunctions\nIn Stata, you’re used to running do-files or programs to automate tasks. In R, functions play a similar role: they help you organize code and reuse it easily.\nA function in R looks like this:\n\nmy_function &lt;- function(input1, input2) {\n  # Do something with the inputs\n  result &lt;- input1 + input2\n  return(result)\n}\n\n\nmy_function is the function’s name.\nfunction(input1, input2) defines what inputs (arguments) it takes.\nInside {}, you write the code that runs when you call the function.\nreturn(result) tells R what the output should be.\n\nYou call the function like this:\n\nmy_function(3, 5)\n# Output: 8\n\nNote that you can change the order of the inputs if you properly label them.\n\nmy_function(input2 = 5, input1 = 3)\n# Output: 8\n\nKey points for Stata users:\n\nFunctions in R must be assigned to a name using &lt;- (the assignment operator).\nYou can think of functions a little like Stata’s program define, but in R, every function can return a value to be used later.\nYou can nest functions inside other functions, making your analysis scripts cleaner and easier to read.\nEvry time you are copying and paste a chuck of code, it is an occasion to write the function."
  },
  {
    "objectID": "spi_by_points.html",
    "href": "spi_by_points.html",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "",
    "text": "1 Introduction\n  2 Code\n  \n  2.1 Set Up\n  2.2 Read the data\n  2.3 Georeference the survey\n  2.4 Plot\n  2.5 Extract\n  2.6 Compute the SPI\n  2.7 Merge with survey\n  2.8 Select based on the interview\n  2.9 Save\n  \n  3 Take home messages\n  4 Appendix\n  \n  4.1 Want to know about the data?"
  },
  {
    "objectID": "spi_by_points.html#introduction",
    "href": "spi_by_points.html#introduction",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we extract the CHIRPS precipitation observations in Suriname, based on the centroids of the Principal Sampling Units (PSU) of the Suriname Survey of Living Conditions for the years 2016/17 and 2022. Based on the extracted precipitation, the tutorial shows how to compute the Standardized Precipitation Index (SPI) and merge it with the survey based on the date of interviews."
  },
  {
    "objectID": "spi_by_points.html#code",
    "href": "spi_by_points.html#code",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Set Up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only climatic4economist package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\nlibrary(climatic4economist)\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note .. means one step back to the folder directory, i.e. one folder back.\nNote that how to set up the paths depends on your folder organization but there are overall two approaches:\n\nyou can use the R project, by opening the project directly you don’t need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder.\nyou can manually set the working folder with the function setwd().\n\n\n# path to data folder\n1path_to_data &lt;- file.path(\"..\",\n2                          \"..\", \"data\")\n\n# survey \npath_to_wave_1 &lt;- file.path(path_to_data, \"survey\", \"suriname\", \"wave 1\",\n                            \"RT001_Public.dta\")\n\npath_to_wave_2 &lt;- file.path(path_to_data, \"survey\", \"suriname\", \"wave 2\", \n                            \"2022 RT001_Housing_plus.dta\")\n\n# administrative division\npath_to_adm_div &lt;- file.path(path_to_data, \"adm_div\", \"GAUL\")\n\n# weather variables\npath_to_pre_monthly &lt;- file.path(path_to_data, \"weather\", \"CHIRPS\", \"monthly\",\n                                 \"chirps-v2.0.monthly.nc\")\n# to result folder\npath_to_result &lt;- file.path(path_to_data, \"result\")\n\n\n1\n\nconcatenate the string to make a path\n\n2\n\n.. means one folder back\n\n\n\n\n\n\n\n2.2 Read the data\n\n2.2.1 Survey\nWe begin by reading the surveys, which in this case consist of two waves with potentially different locations. As a result, we need to load both waves. The waves are stored as dta files, so we use the haven::read_dta() function to read them.\nWe only need the hhid, the survey coordinates, and the interview dates. We use dplyr::select() to choose these variables. This passage is optional and we bring with us all the variables, but we won’t use them. Note that the first wave does not include the interview date.\nWe combine the two waves using dplyr::bind_rows().\nWe can use the head() function to preview the data and see how it looks.\n\nwave_1 &lt;- haven::read_dta(path_to_wave_1) |&gt;\n    dplyr::select(hhid, lat_cen, long_cen) |&gt; \n    dplyr::mutate(wave = 1)\n\nwave_2 &lt;- haven::read_dta(path_to_wave_2) |&gt;\n    dplyr::select(hhid, end_date_n, lat_cen, long_cen) |&gt;\n    dplyr::mutate(wave = 2)\n\nsurvey &lt;- dplyr::bind_rows(wave_1, wave_2)\n\nhead(survey)\n\n# A tibble: 6 × 5\n     hhid lat_cen long_cen  wave end_date_n\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    \n1 1010031    5.82    -55.2     1 NA        \n2 1010041    5.82    -55.2     1 NA        \n3 1010051    5.82    -55.2     1 NA        \n4 1010061    5.82    -55.2     1 NA        \n5 1010121    5.82    -55.2     1 NA        \n6 1010131    5.82    -55.2     1 NA        \n\n\n\n\n2.2.2 Aministrative Divisions\nWe read the spatial file containing the national borders of Suriname, we use read_GAUL() to load it. By printing the spatial data, we can obtain key information, such as the dimensions (number of rows and variables), the geometry (which indicates the type of spatial object), and the coordinate reference system (CRS), which links the coordinates to precise locations on the Earth’s surface. The CRS is particularly important when working with different spatial datasets, as mismatched CRSs can prevent the datasets from aligning correctly.\n\nadm_div &lt;- read_GAUL(path_to_adm_div, iso = \"SUR\", lvl = 2)\nadm_div\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 62, 4  (geometries, attributes)\n extent      : -58.04987, -53.97982, 1.83114, 6.004546  (xmin, xmax, ymin, ymax)\n source      : GAUL-SUR-ADM2.geojson\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_adm_div   iso  adm_div_1  adm_div_2\n type        :      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;\n values      :          1   SUR Brokopondo  Brownsweg\n                        2   SUR Brokopondo    Centrum\n                        3   SUR Brokopondo Klaaskreek\n\n\n\n\n2.2.3 Weather\nFinally, we load the precipitation data. Climatic data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or “cell” in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region. In this case, the values are monthly precipitation that fell in that region. We use the function terra::rast() to load the raster data.\nThis particular raster has global coverage, so we crop it to focus on the country area to reduce its size. Although this step is not strictly necessary, it helps decrease the memory load and makes visualizations more manageable. We use the function crop_with_buffer() for this purpose.\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS). Given the importance of the CRS, we extract it using terra::crs() and save it for later use.\nWe also rename the raster layers to reflect the corresponding dates for each layer, as this is useful if we want to track the dates. We use terra::time() to extract the dates.\n\n\n\n\n\n\nNote\n\n\n\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n\n\n\nweather_monthly &lt;- terra::rast(path_to_pre_monthly) |&gt;\n    crop_with_buffer(adm_div)\nweather_monthly\n\nclass       : SpatRaster \nsize        : 83, 81, 527  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -58.05, -54, 1.85, 6  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       :  precip_1,  precip_2,  precip_3, precip_4, precip_5,   precip_6,      ... \nmin values  :  36.21083,  49.09035,  34.04647, 153.5361, 103.0583,   90.83206,      ... \nmax values  : 403.70178, 653.45990, 345.78024, 622.1295, 795.1603, 1034.43372,      ... \nunit        : mm/month \ntime (days) : 1981-01-01 to 2024-11-01 (527 steps) \n\nnames(weather_monthly) &lt;- terra::time(weather_monthly)\nweather_monthly\n\nclass       : SpatRaster \nsize        : 83, 81, 527  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -58.05, -54, 1.85, 6  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       : 1981-01-01, 1981-02-01, 1981-03-01, 1981-04-01, 1981-05-01, 1981-06-01,      ... \nmin values  :   36.21083,   49.09035,   34.04647,   153.5361,   103.0583,   90.83206,      ... \nmax values  :  403.70178,  653.45990,  345.78024,   622.1295,   795.1603, 1034.43372,      ... \nunit        : mm/month \ntime (days) : 1981-01-01 to 2024-11-01 (527 steps) \n\n\n\n\n\n\n2.3 Georeference the survey\nAs we’ve mentioned, the weather data is georeferenced, so we need to ensure the same for the survey data. Since many households share the same coordinates, they are linked to the same weather events. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called ID.\nThis is handled by the prepare_coord() function, which requires the coordinates’ variable names as input.\nWe can print the result to check the transformation. The new column, ID, is created by prepare_coord() and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\nsrvy_coord &lt;- prepare_coord(survey, lat_var = lat_cen, lon_var = long_cen)\nsrvy_coord\n\n# A tibble: 4,535 × 6\n   ID           hhid lat_cen long_cen  wave end_date_n\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    \n 1 1     23903250101    3.85    -54.2     2 2022-12-06\n 2 1     23903250201    3.85    -54.2     2 2022-12-06\n 3 1     23903250301    3.85    -54.2     2 2022-12-06\n 4 1     23903250401    3.85    -54.2     2 2022-12-07\n 5 1     23903252201    3.85    -54.2     2 2022-12-07\n 6 1     23903252301    3.85    -54.2     2 2022-12-06\n 7 1     23903252501    3.85    -54.2     2 2022-12-06\n 8 1     23903252601    3.85    -54.2     2 2022-12-06\n 9 1     23903253301    3.85    -54.2     2 2022-12-07\n10 1     23903253501    3.85    -54.2     2 2022-12-06\n# ℹ 4,525 more rows\n\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the georef_coord() function. When performing this transformation, it’s crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the georef_coord() function requires the coordinates’ variable names as input.\n\nsrvy_geo &lt;- georef_coord(srvy_coord,\n                          geom = c(\"long_cen\", \"lat_cen\"),\n                          crs = \"EPSG:4326\")\nsrvy_geo\n\n class       : SpatVector \n geometry    : points \n dimensions  : 688, 1  (geometries, attributes)\n extent      : -57.05207, -54.05524, 3.849064, 5.943465  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    ID\n type        : &lt;chr&gt;\n values      :     1\n                   2\n                   3\n\n\nNote how there are 688 rows. These are the unique locations from the survey.\n\n\n\n2.4 Plot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\nterra::plot(adm_div, col = \"grey\", main = \"Suriname and PSU centroids\")\nterra::points(srvy_geo, col = \"gold\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the capital and along the coast.\nNext, we plot a layer of the precipitation data to see how it overlaps with the spatial coordinates.\n\nterra::plot(weather_monthly, \"2024-10-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-10 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\n\n\n\n\n\n\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the high spatial resolution of the CHIRPS dataset. However, despite this high resolution, some survey coordinates still fall within the same cell.\n\n\n\n2.5 Extract\nNext, we extract the weather data based on the survey coordinates using the extract_by_coord() function. This function requires the raster with the weather data and the georeferenced coordinates as inputs.\nLooking at the result, we see first the ID column, that identifies the unique survey coordinates. The second and third column are the coordinates of the cells. The other columns contain the weather observations over time specific to each coordinate.\n\npre_coord &lt;- extract_by_coord(raster = weather_monthly, \n                              coord = srvy_geo)\npre_coord\n\n# A tibble: 688 × 530\n   ID    x_cell y_cell X1981_01_01 X1981_02_01 X1981_03_01 X1981_04_01\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1      -54.2   3.82        255.        260.       133.         359.\n 2 2      -55.5   3.92        225.        249.        87.2        380.\n 3 3      -55.5   3.97        184.        239.        86.9        384.\n 4 4      -55.5   4.02        186.        251.        86.3        368.\n 5 5      -55.5   4.02        186.        251.        86.3        368.\n 6 6      -55.4   4.02        167.        265.        86.6        379.\n 7 7      -54.8   4.07        140.        261.       107.         347.\n 8 8      -55.4   4.12        186.        257.        96.2        347.\n 9 9      -55.4   4.17        210.        228.        87.0        376.\n10 10     -55.2   4.17        213.        260.        90.0        339.\n# ℹ 678 more rows\n# ℹ 523 more variables: X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;,\n#   X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;,\n#   X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;,\n#   X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;,\n#   X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;, X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;,\n#   X1982_11_01 &lt;dbl&gt;, X1982_12_01 &lt;dbl&gt;, X1983_01_01 &lt;dbl&gt;, …\n\n\nAgain we have a row for each unique location from the survey. However, if we want to know how many different cells there are we can look unique cell coordinates.\n\nunique_cell &lt;- pre_coord |&gt;\n  dplyr::distinct(x_cell, y_cell)\nnrow(unique_cell)\n\n[1] 107\n\n\nWe see that now the number of rows is 107, this is the actual different weather observation that we can merge with the survey.\n\n\n\n2.6 Compute the SPI\nWe now compute the SPI with the function compute_spi(). This function requires the precipitation time series for each location and the time scale at which the SPI is computed.\nTo compute the SPI, it is recommended to use at least 30 years of observation to ensure a good estimation of the parameters. More years can strength the estimation but the results can be affected by climate change: if there have been a change in the climate parameters, old observations might be not indicative of the current situation affecting the estimation. There are no clear rule on this, so we leave add the possibility to select the time range of observation with the function select_by_dates(). The function requires both or just one between the starting date, from, and the end date to. If both are provide the the function select between the two dates, if only from is provided the function selects all date after, and if only to is provided the function selects all date before.\nLooking at the result, we see first is the ID column, that we will use to merge back with the survey. The other columns contain the SPI observations over time specific to each coordinate.\n\n# coord  &lt;- select_by_dates(coord, from = \"1991-01-01\", to = \"2023-01-01\") \n\nspi3 &lt;- compute_spi(pre_coord, time_scale = 3)\nspi3\n\n# A tibble: 688 × 530\n   ID    x_cell y_cell X1981_01_01 X1981_02_01 X1981_03_01 X1981_04_01\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1      -54.2   3.82          NA          NA     -0.0896       0.333\n 2 10     -55.2   4.17          NA          NA     -0.173        0.208\n 3 100    -55.2   5.67          NA          NA      0.124        0.624\n 4 101    -55.4   5.72          NA          NA      0.0581       0.918\n 5 102    -55.2   5.72          NA          NA      0.0618       0.843\n 6 103    -55.2   5.72          NA          NA      0.0618       0.843\n 7 104    -55.2   5.72          NA          NA      0.0618       0.843\n 8 105    -55.2   5.72          NA          NA      0.0618       0.843\n 9 106    -55.1   5.72          NA          NA      0.121        0.994\n10 107    -55.1   5.72          NA          NA      0.121        0.994\n# ℹ 678 more rows\n# ℹ 523 more variables: X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;,\n#   X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;,\n#   X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;,\n#   X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;,\n#   X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;, X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;,\n#   X1982_11_01 &lt;dbl&gt;, X1982_12_01 &lt;dbl&gt;, X1983_01_01 &lt;dbl&gt;, …\n\n\nIf we want to calculate the SPI with time scale equal to one, we just need to change the time_scale argument.\n\nspi1 &lt;- compute_spi(pre_coord, time_scale = 1)\nspi1\n\n# A tibble: 688 × 530\n   ID    x_cell y_cell X1981_01_01 X1981_02_01 X1981_03_01 X1981_04_01\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1      -54.2   3.82      0.115        0.543      -0.801       0.913\n 2 10     -55.2   4.17     -0.0471       0.737      -1.06        0.688\n 3 100    -55.2   5.67      0.0636       0.592      -0.170       0.838\n 4 101    -55.4   5.72     -0.722        1.04        0.136       0.755\n 5 102    -55.2   5.72     -0.478        1.06       -0.131       0.755\n 6 103    -55.2   5.72     -0.478        1.06       -0.131       0.755\n 7 104    -55.2   5.72     -0.478        1.06       -0.131       0.755\n 8 105    -55.2   5.72     -0.478        1.06       -0.131       0.755\n 9 106    -55.1   5.72     -0.494        1.24       -0.135       0.853\n10 107    -55.1   5.72     -0.494        1.24       -0.135       0.853\n# ℹ 678 more rows\n# ℹ 523 more variables: X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;,\n#   X1981_07_01 &lt;dbl&gt;, X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;,\n#   X1981_11_01 &lt;dbl&gt;, X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;,\n#   X1982_03_01 &lt;dbl&gt;, X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;,\n#   X1982_07_01 &lt;dbl&gt;, X1982_08_01 &lt;dbl&gt;, X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;,\n#   X1982_11_01 &lt;dbl&gt;, X1982_12_01 &lt;dbl&gt;, X1983_01_01 &lt;dbl&gt;, …\n\n\n\n\n\n2.7 Merge with survey\nNow, we combine the extracted weather data with the survey data using ID as the key matching variable.\n\nspi3_survey &lt;- merge_with_survey(srvy_coord, spi3)\n\nspi3_survey\n\n# A tibble: 4,535 × 535\n   ID           hhid lat_cen long_cen  wave end_date_n x_cell y_cell X1981_01_01\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 1     23903250101    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 2 1     23903250201    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 3 1     23903250301    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 4 1     23903250401    3.85    -54.2     2 2022-12-07  -54.2   3.82          NA\n 5 1     23903252201    3.85    -54.2     2 2022-12-07  -54.2   3.82          NA\n 6 1     23903252301    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 7 1     23903252501    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 8 1     23903252601    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n 9 1     23903253301    3.85    -54.2     2 2022-12-07  -54.2   3.82          NA\n10 1     23903253501    3.85    -54.2     2 2022-12-06  -54.2   3.82          NA\n# ℹ 4,525 more rows\n# ℹ 526 more variables: X1981_02_01 &lt;dbl&gt;, X1981_03_01 &lt;dbl&gt;,\n#   X1981_04_01 &lt;dbl&gt;, X1981_05_01 &lt;dbl&gt;, X1981_06_01 &lt;dbl&gt;, X1981_07_01 &lt;dbl&gt;,\n#   X1981_08_01 &lt;dbl&gt;, X1981_09_01 &lt;dbl&gt;, X1981_10_01 &lt;dbl&gt;, X1981_11_01 &lt;dbl&gt;,\n#   X1981_12_01 &lt;dbl&gt;, X1982_01_01 &lt;dbl&gt;, X1982_02_01 &lt;dbl&gt;, X1982_03_01 &lt;dbl&gt;,\n#   X1982_04_01 &lt;dbl&gt;, X1982_05_01 &lt;dbl&gt;, X1982_06_01 &lt;dbl&gt;, X1982_07_01 &lt;dbl&gt;,\n#   X1982_08_01 &lt;dbl&gt;, X1982_09_01 &lt;dbl&gt;, X1982_10_01 &lt;dbl&gt;, …\n\n\nWe are back at 4535, which matches the original survey.\n\n\n\n2.8 Select based on the interview\nNow that we have merged the SPI values with the survey, we can select just the relevant observations.\nIf we want to select just a subsets of observations we can use the select_by_dates() function. If we want to select based on the date of interview of the survey, we can use select_by_interview(). This last function requires the variable that contains the dates of interview and the interval to select based on the dates. The interval must be express in number of months or in number years. The wide argument specifies how the output should be reported, in wide with each time observation as separate columns, or long, with all observation in one column.\n\nNote that current version of the select_by_interview() functions drops the observations with missing date of interview.\n\nWhat is relevant depends on the particular application, but we can agree that we don’t want to assign weather observations that happened after the interviews, as these cannot influence the answers.\nIn this tutorial we select the 12 months before the interviews using the function select_by_interview(). The argument interview select the variable containing the date of interviews, and the argument interval defines how back in time the function needs to select the observations.\nIf there are missing observations for the date of interviews, the function warns us that these observations are dropped.\n\nspi3_hh &lt;- select_by_interview(spi3_survey,\n                               interview = end_date_n,\n                               interval = \"1 year\",\n                               wide = TRUE)\n\nMissing interview are dropped!\n\n\n\n\n\n2.9 Save\nThe final step of the code is to save the result. In this case, we save it as a dta file using the haven::write_dta() function.\n\nhaven::write_dta(spi1_survey, file.path(path_to_result, \"spi_1.dta\"))\n\nhaven::write_dta(spi3_hh, file.path(path_to_result, \"spi_3.dta\"))"
  },
  {
    "objectID": "spi_by_points.html#take-home-messages",
    "href": "spi_by_points.html#take-home-messages",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "3 Take home messages",
    "text": "3 Take home messages\n\nWhen working with multiple spatial data:\n\nremember to control the Coordinate Reference System of all dataset\nplot the data to check everything is going well\n\nbased on the typology of data we use different function to read them\n\nfor dta use haven::read_dta() or and haven::_write_dta()\nfor spatial vectors read_GAUL() for administrative divisions in specific country and level, otherwise terra::vect()\nfor spatial raster use terra::rast()\n\nExtraction\n\nsince many household share the same locations, we take advantage of this by extracting the weather data only at the unique locations, we achieve this with the function prepare_coord().\nsame of these unique locations may fall within the same value cells, so the actual information might be even lower."
  },
  {
    "objectID": "spi_by_points.html#appendix",
    "href": "spi_by_points.html#appendix",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "4 Appendix",
    "text": "4 Appendix\n\n4.1 Want to know about the data?\n\n4.1.1 Precipitation\nMonthly and daily precipitation from Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS)1 is a 35+ year quasi-global rainfall data set. Spanning 50°S-50°N (and all longitudes) and ranging from 1981 to near-present, CHIRPS incorporates in-house climatology, CHPclim, 0.05° resolution satellite imagery, and in-situ station data to create gridded rainfall time series for trend analysis and seasonal drought monitoring.\nData can be downloaded from here while extra information are available here.\n\n\n\nfeature\nvalue\n\n\n\n\nspatial resolution\n0.05 x 0.05 (~ 5 km)\n\n\ntemporal resolution\nmonthly or daily\n\n\ntemporal frame\n1981 - near present\n\n\nunit of measure\nmm/month or mm/day\n\n\n\n\n\n4.1.2 Surveys\nSuriname Survey of Living Conditions. The 2022 Suriname Survey of Living Conditions is a joint survey made by The Inter-American Development Bank (IDB) and the World Bank. The 2022 Suriname Survey of Living Conditions - administered to a nationally representative sample, which included 7,713 individuals from 2,540 households - was developed to support poverty analysis as well as policy planning and is a helpful tool for policy makers to facilitate fact-based decision making. The survey’s design and execution were financed by the IDB, while the World Bank and IDB are joining forces to analyze data and produce initial findings.\nThe Suriname Survey of Living Conditions (SSLC) 2016/17 is an effort of the Inter-American Development Bank (IDB) with the support of the EnergieBedrijvan Suriname’s (state-owned electrical company of Suriname) and the Central Bank of Suriname. It visited about 2,000 households from October 2016 through September 2017 and collected data on the most important dimensions of welfare, which will support evidence-based policy making in areas such as education, health, housing, employment and poverty alleviation. The survey also gathered information on the consumption patterns, income and expenditures of the Surinamese households, intended to update the Consumption Price Index basket and inform the System of National Accounts.\nFor extra info look here and here.\n\n\n4.1.3 Adminitrative boundaries\nThe Global Administrative Unit Layers (GAUL) 2024 is a vector dataset compiling the most recent available information on administrative boundaries from multiple sources, produced by FAO from 2022 to 2024 in the framework of the Hand-in-Hand Initiative and the Geospatial Data Platform activities.\nThe GAUL 2024 aims at maintaining global layers with a unified coding system at country, first (e.g. departments), and second (e.g. districts) administrative levels. The data, sourced and processed from the United Nations Second Administrative Level Boundaries (UN-SALB) programme and from other relevant data sources, was complemented by the UN-FAO-CSI AgroInformatics Geospatial Analysis team with data from official geospatial data producers. Country boundaries were processed against UN official recognized borders (UN-map 2018), and the administrative subdivisions were checked for geometry and topology, then corrected and validated. The administrative boundaries dataset at level 1 and 2 (Sub-national level) is part of the Global Administrative Unit Layers (GAUL) dataset series which includes information on administrative units for all the countries in the world, providing a contribution to the standardization of the spatial dataset representing administrative units. The administrative boundaries at the level 1 dataset distinguishes States, Provinces, Departments and equivalent. The administrative boundaries at the level 2 dataset distinguishes Districts and equivalent.\nSuggested citation:\n\nFAO. 2024. Global Administrative Unit Layers (GAUL). [Accessed on [DD Month YYYY]]. https://data.apps.fao.org/?lang=en. Licence: CC-BY-4.0\n\nIt is possible to find additional information:\n\nhere\n\nThe data can be freely download from\n\nhere."
  },
  {
    "objectID": "spi_by_points.html#footnotes",
    "href": "spi_by_points.html#footnotes",
    "title": "3: Compute the Standardize Precipitation Index based on spatial point.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFunk, C.C., Peterson, P.J., Landsfeld, M.F., Pedreros, D.H., Verdin, J.P., Rowland, J.D., Romero, B.E., Husak, G.J., Michaelsen, J.C., and Verdin, A.P., 2014, A quasi-global precipitation time series for drought monitoring: U.S. Geological Survey Data Series 832, 4 p. http://pubs.usgs.gov/ds/832/↩︎"
  }
]
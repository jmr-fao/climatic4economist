{"title":"4: Compute the Standardize Precipitation Index based on spatial polygons","markdown":{"yaml":{"title":"4: Compute the Standardize Precipitation Index based on spatial polygons"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<br>\n\n\nIn this tutorial, we extract the CHIRPS precipitation observations in Suriname, based on the administrative divisions of the Suriname Survey of Living Conditions for the years 2016/17 and 2022. Based on the extracted precipitation, the tutorial shows how to compute the Standardized Precipitation Index (SPI) and merge it with the survey based on the date of interviews.\n\n## Code\n### Set Up\nWe start by setting up the stage for our analysis.\n\nFirst, we load the necessary packages. We load only `climatic4economist` package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\n```{r packages}\n#| label: packages\n#| output: false\n\nlibrary(climatic4economist)\n```\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note `..` means one step back to the folder directory, i.e. one folder back.\n\nNote that how to set up the paths depends on your folder organization but there are overall two approaches: \n\n1. you can use the `R project`, by opening the project directly you don't need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder. \n2. you can manually set the working folder with the function `setwd()`.\n\n```{r paths}\n#| label: paths\n\n# path to data folder\npath_to_data <- file.path(\"..\", # <1>\n                          \"..\", \"data\") # <2>\n\n# survey \npath_to_wave_1 <- file.path(path_to_data, \"survey\", \"surname\", \"wave 1\",\n                            \"RT001_Public.dta\")\n\npath_to_wave_2 <- file.path(path_to_data, \"survey\", \"surname\", \"wave 2\", \n                            \"2022 RT001_Housing_plus.dta\")\n\n# administrative division\npath_to_adm_div <- file.path(path_to_data, \"adm_div\", \"GAUL\")\n\n# weather variables\npath_to_pre_monthly <- file.path(path_to_data, \"weather\", \"CHIRPS\", \"monthly\",\n                                 \"chirps-v2.0.monthly.nc\")\n# to result folder\npath_to_result <- file.path(path_to_data, \"result\")\n```\n1. concatenate the string to make a path\n2. `..` means one folder back\n\n<br>\n\n### Read the data\n#### Survey\nWe begin by reading the surveys, which in this case consist of two waves with potentially different locations. As a result, we need to load both waves. The waves are stored as `dta` files, so we use the `haven::read_dta()` function to read them.\n\nWe only need the hhid, the survey coordinates, and the interview dates. We use `dplyr::select()` to choose these variables. This passage is optional and we bring with us all the variables, but we won't use them. Note that the first wave does not include the interview date.\n\nWe combine the two waves using `dplyr::bind_rows()`.\n\nWe can use the `head()` function to preview the data and see how it looks.\n\n```{r read_surveys}\n#| label: read_surveys\n\nwave_1 <- haven::read_dta(path_to_wave_1) |>\n    dplyr::select(hhid, lat_cen, long_cen) |> \n    dplyr::mutate(wave = 1)\n\nwave_2 <- haven::read_dta(path_to_wave_2) |>\n    dplyr::select(hhid, end_date_n, lat_cen, long_cen) |>\n    dplyr::mutate(wave = 2)\n\nsurvey <- dplyr::bind_rows(wave_1, wave_2)\n\nhead(survey)\n```\n\n#### Aministrative Divisions\nWe read the spatial file containing the national borders of Suriname, we use `read_GAUL()` to load it. By printing the spatial data, we can obtain key information, such as the dimensions (number of rows and variables), the geometry (which indicates the type of spatial object), and the coordinate reference system (CRS), which links the coordinates to precise locations on the Earth's surface. The CRS is particularly important when working with different spatial datasets, as mismatched CRSs can prevent the datasets from aligning correctly.\n\n```{r read_adm_div}\n#| label: read_adm_div\n\nadm_div <- read_GAUL(path_to_adm_div, iso = \"SUR\", lvl = 2)\nadm_div\n```\n\n#### Weather\nFinally, we load the precipitation data. Climatic data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or \"cell\" in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region. In this case, the values are monthly precipitation that fell in that region. We use the function `terra::rast()` to load the raster data.\n\nThis particular raster has global coverage, so we crop it to focus on the country area to reduce its size. Although this step is not strictly necessary, it helps decrease the memory load and makes visualizations more manageable. We use the function `crop_with_buffer()` for this purpose.\n\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS). Given the importance of the CRS, we extract it using `terra::crs()` and save it for later use.\n\nWe also rename the raster layers to reflect the corresponding dates for each layer, as this is useful if we want to track the dates. We use `terra::time()` to extract the dates.\n\n::: {.callout-note}\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n:::\n\n```{r read_pre_monthly}\n#| label: read_pre_monthly\n\nweather_monthly <- terra::rast(path_to_pre_monthly) |>\n    crop_with_buffer(adm_div)\nweather_monthly\n\nnames(weather_monthly) <- terra::time(weather_monthly)\nweather_monthly\n```\n\n<br>\n\n### Georeference the survey\n\nAs we've mentioned, the weather data is georeferenced, so we need to ensure the same for the survey data. Since many households share the same coordinates, they are linked to the same weather events. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called `ID`.\n\nThis is handled by the `prepare_coord()` function, which requires the coordinates' variable names as input.\n\nWe can print the result to check the transformation. The new column, `ID`, is created by `prepare_coord()` and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\n```{r prepare_coord}\n#| label: prepare_coord\n\nsrvy_coord <- prepare_coord(survey, lat_var = lat_cen, lon_var = long_cen)\nsrvy_coord\n```\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the `georef_coord()` function requires the coordinates' variable names as input.\n\n```{r georef_coord}\n#| label: georef_coord\n\nsrvy_geo <- georef_coord(srvy_coord,\n                          geom = c(\"long_cen\", \"lat_cen\"),\n                          crs = \"EPSG:4326\")\nsrvy_geo\n```\n\n<br>\n\n### PLot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\n\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n```{r plot_survey_geo}\n#| label: plot_survey_geo\n\nterra::plot(adm_div, col = \"grey\", main = \"Suriname and PSU centroids\")\nterra::points(srvy_geo, col = \"gold\", alpha = 0.5, cex = 0.5)\nsrvy_geo |> \n    tidyterra::filter(ID %in% c(\"21\", \"22\", \"652\")) |>\n    terra::text(\"ID\", halo = TRUE, hc = \"blue\", col = \"white\", hw = 0.2)\n```\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the capital and along the coast.\n\nHowever, some coordinates falls outside the administrative division, we highlights them with the blue halo. It may difficult to see them without the zoom , but it is worth notice that as it has consequences for which administrative division to associate them.\n\n::: {.callout-notes}\nIn many microeconomics applications some coordinates fall outside the national order of the country, this might happen for various reasons. Sometimes, the coordinates are modified to guarantee anonimacy of the household and the modification bring the coordinates outside the borders, other time it is just an error of the GPS, or the borders maps are not enought precise.\n:::\n\nNext, we plot a layer of the precipitation data to see how it overlaps with the spatial coordinates.\n\n```{r plot_precipitation}\n#| label: plot_precipitation\n\nterra::plot(weather_monthly, \"2024-10-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-10 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n```\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the high spatial resolution of the CHIRPS dataset. However, despite this high resolution, some survey coordinates still fall within the same cell.\n\n<br>\n\n### Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\n```{r merge_adm_srvy}\n#| label: merge_adm_srvy\n\nsrvy_adm_div <- get_poly_attr_for_point(point = srvy_geo, poly = adm_div)\nsrvy_adm_div\n```\n\n<br>\n\n### Extraction\nFor weather data, we use a different function for extracting the data, namely `extract_cell_by_poly()`. This function doesn't aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the SPI for each cell and only later aggregate them.\n\n::: {.callout-note}\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n:::\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\n```{r extraction_adm_weather}\n#| label: extraction_adm_weather\n#| output: false\n\npre_cell <- extract_cell_by_poly(weather_monthly, adm_div)\n```\n\n```{r print_pre_cell}\n#| label: print_pre_cell\n\npre_cell\n```\n\n### Compute the SPI\nWe now compute the SPI with the function `compute_spi()`. This function requires the precipitation time series for each location and the time scale at which the SPI is computed.\n\nTo compute the SPI, it is recommended to use at least 30 years of observation to ensure a good estimation of the parameters. More years can strength the estimation but the results can be affected by climate change: if there have been a change in the climate parameters, old observations might be not indicative of the current situation affecting the estimation. There are no clear rule on this, so we leave add the possibility to select the time range of observation with the function `select_by_dates()`. The function requires both or just one between the starting date, `from`, and the end date `to`. If both are provide the the function select between the two dates, if only `from` is provided the function selects all date after, and if only `to` is provided the function selects all date before.\n\nLooking at the result, we see first is the `ID_adm_div` column, that we will use to merge back with the survey. Then we have the coordinates of each cell and the coverage fraction of the cell within the administrate border. The other columns contain the SPI observations over time, specific to each coordinate.\n\n```{r compute_spi3}\n#| label: compute_spi3\n#| warning: false\n\n# coord  <- select_by_dates(coord, from = \"1991-01-01\", to = \"2023-01-01\") \n\nspi3 <- compute_spi(pre_cell, time_scale = 3)\nspi3\n```\n\nIf we want to calculate the SPI with time scale equal to one, we just need to change the `time_scale` argument.\n\n<br>\n\n### Agregate at the adiministrative divisions\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function `agg_to_adm_div()` can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the `coverage_fraction` variable. \n\nThe argument `match_col` identifies which columns are aggregated. In this case we want to select all dates observation, so we define the pattern to look for as any number `[0-9]` repeated four time `{4}`.\n\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, `ID_adm_div`, and a single value of the climatic parameters for each locations.\n\n```{r cell_to_div}\n#| label: cell_to_div\n\nspi3_adm <- agg_to_adm_div(spi3, match_col = \"[0-9]{4}\")\n\nspi3_adm\n```\n\n<br>\n\n### Merge with survey\n\nNow, we combine the extracted weather data with the survey data.\n\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as `srvy_adm_div`.\nWe first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command `|>` assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with `y = _`, where `y` is the name of the argument and `_` refer to the previous merge.\n\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n```{r merge_adm_survey}\n#| label: merge_adm_survey\n \nsrvy_spi3_adm <- dplyr::inner_join(srvy_adm_div, spi3_adm, by = \"ID_adm_div\") |> # <1>\n    merge_by_common(srvy_coord, y = _) # <2>\nsrvy_spi3_adm\n```\n1. merge adm info with spatial var\n2. `_` refers to the output of the previous merge \n\nWe are back at `r nrow(srvy_spi3_adm)`, which matches the original survey.\n\n<br>\n\n### Select based on the interview\nNow that we have merged the SPI values with the survey, we can select just the relevant observations.\n\nIf we want to select just a subsets of observations we can use the `select_by_dates()` function. If we want to select based on the date of interview of the survey, we can use `select_by_interview()`. This last function requires the variable that contains the dates of interview and the interval to select based on the dates. The interval must be express in number of months or in number years. The `wide` argument specifies how the output should be reported, in wide with each time observation as separate columns, or long, with all observation in one column.\n\n>Note that current version of the `select_by_interview()` functions drops the observations with missing date of interview.\n\nWhat is relevant depends on the particular application, but we can agree that we don't want to assign weather observations that happened after the interviews, as these cannot influence the answers.\n\nIn this tutorial we select the 12 months before the interviews using the function `select_by_interview()`. The argument `interview` select the variable containing the date of interviews, and the argument `interval` defines how back in time the function needs to select the observations.\n\nIf there are missing observations for the date of interviews, the function warns us that these observations are dropped.\n\n```{r select_by_interview}\n#| label: select_by_interview\n\n\n\nspi3_hh <- select_by_dates(srvy_spi3_adm, \"2020-01-01\", \"2023-01-01\") |>\n    select_by_interview(interview = end_date_n,\n                        interval = \"1 year\",\n                        wide = FALSE)\nspi3_hh\n```\n\n<br>\n\n### Save\n\nThe final step of the code is to save the result. In this case, we save it as a `dta` file using the `haven::write_dta()` function.\n\n\n```{r write_data}\n#| label: write_data\n#| eval: false\n\nhaven::write_dta(spi3_hh, file.path(path_to_result, \"spi_3_adm.dta\"))\n\n```\n\n<br><br>\n\n","srcMarkdownNoYaml":"\n\n<br>\n\n## Introduction\n\nIn this tutorial, we extract the CHIRPS precipitation observations in Suriname, based on the administrative divisions of the Suriname Survey of Living Conditions for the years 2016/17 and 2022. Based on the extracted precipitation, the tutorial shows how to compute the Standardized Precipitation Index (SPI) and merge it with the survey based on the date of interviews.\n\n## Code\n### Set Up\nWe start by setting up the stage for our analysis.\n\nFirst, we load the necessary packages. We load only `climatic4economist` package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\n```{r packages}\n#| label: packages\n#| output: false\n\nlibrary(climatic4economist)\n```\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note `..` means one step back to the folder directory, i.e. one folder back.\n\nNote that how to set up the paths depends on your folder organization but there are overall two approaches: \n\n1. you can use the `R project`, by opening the project directly you don't need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder. \n2. you can manually set the working folder with the function `setwd()`.\n\n```{r paths}\n#| label: paths\n\n# path to data folder\npath_to_data <- file.path(\"..\", # <1>\n                          \"..\", \"data\") # <2>\n\n# survey \npath_to_wave_1 <- file.path(path_to_data, \"survey\", \"surname\", \"wave 1\",\n                            \"RT001_Public.dta\")\n\npath_to_wave_2 <- file.path(path_to_data, \"survey\", \"surname\", \"wave 2\", \n                            \"2022 RT001_Housing_plus.dta\")\n\n# administrative division\npath_to_adm_div <- file.path(path_to_data, \"adm_div\", \"GAUL\")\n\n# weather variables\npath_to_pre_monthly <- file.path(path_to_data, \"weather\", \"CHIRPS\", \"monthly\",\n                                 \"chirps-v2.0.monthly.nc\")\n# to result folder\npath_to_result <- file.path(path_to_data, \"result\")\n```\n1. concatenate the string to make a path\n2. `..` means one folder back\n\n<br>\n\n### Read the data\n#### Survey\nWe begin by reading the surveys, which in this case consist of two waves with potentially different locations. As a result, we need to load both waves. The waves are stored as `dta` files, so we use the `haven::read_dta()` function to read them.\n\nWe only need the hhid, the survey coordinates, and the interview dates. We use `dplyr::select()` to choose these variables. This passage is optional and we bring with us all the variables, but we won't use them. Note that the first wave does not include the interview date.\n\nWe combine the two waves using `dplyr::bind_rows()`.\n\nWe can use the `head()` function to preview the data and see how it looks.\n\n```{r read_surveys}\n#| label: read_surveys\n\nwave_1 <- haven::read_dta(path_to_wave_1) |>\n    dplyr::select(hhid, lat_cen, long_cen) |> \n    dplyr::mutate(wave = 1)\n\nwave_2 <- haven::read_dta(path_to_wave_2) |>\n    dplyr::select(hhid, end_date_n, lat_cen, long_cen) |>\n    dplyr::mutate(wave = 2)\n\nsurvey <- dplyr::bind_rows(wave_1, wave_2)\n\nhead(survey)\n```\n\n#### Aministrative Divisions\nWe read the spatial file containing the national borders of Suriname, we use `read_GAUL()` to load it. By printing the spatial data, we can obtain key information, such as the dimensions (number of rows and variables), the geometry (which indicates the type of spatial object), and the coordinate reference system (CRS), which links the coordinates to precise locations on the Earth's surface. The CRS is particularly important when working with different spatial datasets, as mismatched CRSs can prevent the datasets from aligning correctly.\n\n```{r read_adm_div}\n#| label: read_adm_div\n\nadm_div <- read_GAUL(path_to_adm_div, iso = \"SUR\", lvl = 2)\nadm_div\n```\n\n#### Weather\nFinally, we load the precipitation data. Climatic data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or \"cell\" in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region. In this case, the values are monthly precipitation that fell in that region. We use the function `terra::rast()` to load the raster data.\n\nThis particular raster has global coverage, so we crop it to focus on the country area to reduce its size. Although this step is not strictly necessary, it helps decrease the memory load and makes visualizations more manageable. We use the function `crop_with_buffer()` for this purpose.\n\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS). Given the importance of the CRS, we extract it using `terra::crs()` and save it for later use.\n\nWe also rename the raster layers to reflect the corresponding dates for each layer, as this is useful if we want to track the dates. We use `terra::time()` to extract the dates.\n\n::: {.callout-note}\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n:::\n\n```{r read_pre_monthly}\n#| label: read_pre_monthly\n\nweather_monthly <- terra::rast(path_to_pre_monthly) |>\n    crop_with_buffer(adm_div)\nweather_monthly\n\nnames(weather_monthly) <- terra::time(weather_monthly)\nweather_monthly\n```\n\n<br>\n\n### Georeference the survey\n\nAs we've mentioned, the weather data is georeferenced, so we need to ensure the same for the survey data. Since many households share the same coordinates, they are linked to the same weather events. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called `ID`.\n\nThis is handled by the `prepare_coord()` function, which requires the coordinates' variable names as input.\n\nWe can print the result to check the transformation. The new column, `ID`, is created by `prepare_coord()` and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\n```{r prepare_coord}\n#| label: prepare_coord\n\nsrvy_coord <- prepare_coord(survey, lat_var = lat_cen, lon_var = long_cen)\nsrvy_coord\n```\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the `georef_coord()` function requires the coordinates' variable names as input.\n\n```{r georef_coord}\n#| label: georef_coord\n\nsrvy_geo <- georef_coord(srvy_coord,\n                          geom = c(\"long_cen\", \"lat_cen\"),\n                          crs = \"EPSG:4326\")\nsrvy_geo\n```\n\n<br>\n\n### PLot\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\n\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n```{r plot_survey_geo}\n#| label: plot_survey_geo\n\nterra::plot(adm_div, col = \"grey\", main = \"Suriname and PSU centroids\")\nterra::points(srvy_geo, col = \"gold\", alpha = 0.5, cex = 0.5)\nsrvy_geo |> \n    tidyterra::filter(ID %in% c(\"21\", \"22\", \"652\")) |>\n    terra::text(\"ID\", halo = TRUE, hc = \"blue\", col = \"white\", hw = 0.2)\n```\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the capital and along the coast.\n\nHowever, some coordinates falls outside the administrative division, we highlights them with the blue halo. It may difficult to see them without the zoom , but it is worth notice that as it has consequences for which administrative division to associate them.\n\n::: {.callout-notes}\nIn many microeconomics applications some coordinates fall outside the national order of the country, this might happen for various reasons. Sometimes, the coordinates are modified to guarantee anonimacy of the household and the modification bring the coordinates outside the borders, other time it is just an error of the GPS, or the borders maps are not enought precise.\n:::\n\nNext, we plot a layer of the precipitation data to see how it overlaps with the spatial coordinates.\n\n```{r plot_precipitation}\n#| label: plot_precipitation\n\nterra::plot(weather_monthly, \"2024-10-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-10 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n```\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the high spatial resolution of the CHIRPS dataset. However, despite this high resolution, some survey coordinates still fall within the same cell.\n\n<br>\n\n### Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\n```{r merge_adm_srvy}\n#| label: merge_adm_srvy\n\nsrvy_adm_div <- get_poly_attr_for_point(point = srvy_geo, poly = adm_div)\nsrvy_adm_div\n```\n\n<br>\n\n### Extraction\nFor weather data, we use a different function for extracting the data, namely `extract_cell_by_poly()`. This function doesn't aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the SPI for each cell and only later aggregate them.\n\n::: {.callout-note}\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n:::\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\n```{r extraction_adm_weather}\n#| label: extraction_adm_weather\n#| output: false\n\npre_cell <- extract_cell_by_poly(weather_monthly, adm_div)\n```\n\n```{r print_pre_cell}\n#| label: print_pre_cell\n\npre_cell\n```\n\n### Compute the SPI\nWe now compute the SPI with the function `compute_spi()`. This function requires the precipitation time series for each location and the time scale at which the SPI is computed.\n\nTo compute the SPI, it is recommended to use at least 30 years of observation to ensure a good estimation of the parameters. More years can strength the estimation but the results can be affected by climate change: if there have been a change in the climate parameters, old observations might be not indicative of the current situation affecting the estimation. There are no clear rule on this, so we leave add the possibility to select the time range of observation with the function `select_by_dates()`. The function requires both or just one between the starting date, `from`, and the end date `to`. If both are provide the the function select between the two dates, if only `from` is provided the function selects all date after, and if only `to` is provided the function selects all date before.\n\nLooking at the result, we see first is the `ID_adm_div` column, that we will use to merge back with the survey. Then we have the coordinates of each cell and the coverage fraction of the cell within the administrate border. The other columns contain the SPI observations over time, specific to each coordinate.\n\n```{r compute_spi3}\n#| label: compute_spi3\n#| warning: false\n\n# coord  <- select_by_dates(coord, from = \"1991-01-01\", to = \"2023-01-01\") \n\nspi3 <- compute_spi(pre_cell, time_scale = 3)\nspi3\n```\n\nIf we want to calculate the SPI with time scale equal to one, we just need to change the `time_scale` argument.\n\n<br>\n\n### Agregate at the adiministrative divisions\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function `agg_to_adm_div()` can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the `coverage_fraction` variable. \n\nThe argument `match_col` identifies which columns are aggregated. In this case we want to select all dates observation, so we define the pattern to look for as any number `[0-9]` repeated four time `{4}`.\n\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, `ID_adm_div`, and a single value of the climatic parameters for each locations.\n\n```{r cell_to_div}\n#| label: cell_to_div\n\nspi3_adm <- agg_to_adm_div(spi3, match_col = \"[0-9]{4}\")\n\nspi3_adm\n```\n\n<br>\n\n### Merge with survey\n\nNow, we combine the extracted weather data with the survey data.\n\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as `srvy_adm_div`.\nWe first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command `|>` assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with `y = _`, where `y` is the name of the argument and `_` refer to the previous merge.\n\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n```{r merge_adm_survey}\n#| label: merge_adm_survey\n \nsrvy_spi3_adm <- dplyr::inner_join(srvy_adm_div, spi3_adm, by = \"ID_adm_div\") |> # <1>\n    merge_by_common(srvy_coord, y = _) # <2>\nsrvy_spi3_adm\n```\n1. merge adm info with spatial var\n2. `_` refers to the output of the previous merge \n\nWe are back at `r nrow(srvy_spi3_adm)`, which matches the original survey.\n\n<br>\n\n### Select based on the interview\nNow that we have merged the SPI values with the survey, we can select just the relevant observations.\n\nIf we want to select just a subsets of observations we can use the `select_by_dates()` function. If we want to select based on the date of interview of the survey, we can use `select_by_interview()`. This last function requires the variable that contains the dates of interview and the interval to select based on the dates. The interval must be express in number of months or in number years. The `wide` argument specifies how the output should be reported, in wide with each time observation as separate columns, or long, with all observation in one column.\n\n>Note that current version of the `select_by_interview()` functions drops the observations with missing date of interview.\n\nWhat is relevant depends on the particular application, but we can agree that we don't want to assign weather observations that happened after the interviews, as these cannot influence the answers.\n\nIn this tutorial we select the 12 months before the interviews using the function `select_by_interview()`. The argument `interview` select the variable containing the date of interviews, and the argument `interval` defines how back in time the function needs to select the observations.\n\nIf there are missing observations for the date of interviews, the function warns us that these observations are dropped.\n\n```{r select_by_interview}\n#| label: select_by_interview\n\n\n\nspi3_hh <- select_by_dates(srvy_spi3_adm, \"2020-01-01\", \"2023-01-01\") |>\n    select_by_interview(interview = end_date_n,\n                        interval = \"1 year\",\n                        wide = FALSE)\nspi3_hh\n```\n\n<br>\n\n### Save\n\nThe final step of the code is to save the result. In this case, we save it as a `dta` file using the `haven::write_dta()` function.\n\n\n```{r write_data}\n#| label: write_data\n#| eval: false\n\nhaven::write_dta(spi3_hh, file.path(path_to_result, \"spi_3_adm.dta\"))\n\n```\n\n<br><br>\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":3,"embed-resources":true,"number-sections":true,"output-file":"spi_by_poly.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","code-annotations":"hover","editor":"source","editor_options":{"chunk_output_type":"console"},"theme":"Cerulean","logo":"images/Climat4Economist_Symbol_6.png","toc-expand":2,"toc-location":"right-body","footnotes-hover":true,"title":"4: Compute the Standardize Precipitation Index based on spatial polygons"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
{"title":"![](images/Climat4Economist_Symbol_6.png){width=1in}\n\nExtract Spatial Variable with Climat4Economist","markdown":{"yaml":{"title":"![](images/Climat4Economist_Symbol_6.png){width=1in}\n\nExtract Spatial Variable with Climat4Economist\n","subtitle":"Tutorial on how to extract spatial raster values based on survey coordinated loaction and survey administrative divisions using the Climat4Economist package","execute":{"eval":true}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nThis tutorial show how too extract spatial control variables based on surveys locations. The survey refers to Ethipia 2019 and comes from the [The World Bank Living Standards Measurement Study (LSMS)](https://www.worldbank.org/en/programs/lsms).\n\n### Is this guide for me?\n\nThis guide provides a step-by-step approach to extract raster data based on survey locations. The target audience includes economists who may have experience with statistical software (e.g. STATA) but are less familiar with spatial data processing in R.\n\nThe document is not meant to be a course on R or on how the functions work. It is just a practice example on how to extract raster data based on coordinate location. This is done by using specific functions that wrap up as many steps as possible to ensure it is easier for the user to follow.\n\n### What do I need before starting?\n\nThe following R packages are necessary: `terra`, `tidyverse`, `haven`, `purrr` and `climatic4economist`. To install the above package you can use `install.packages(\"name_of_package\")`, don't forget the `\"`.\n\nThe latter package, `climatic4economist` is not available on the web and it need to be installed from a local file. You can do it with `devtools::install_local(\"climatic4economist\")`\n\nIf you are not familiar with R check the [appendix] for understanding some coding style used in this tutorial.\n\n<br><br>\n\n## Code\n### Set Up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only `climatic4economist` package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\n```{r packages}\n#| label: packages\n#| output: false\n\nlibrary(climatic4economist)\n```\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note `..` means one step back to the folder directory, i.e. one folder back.\n\nNote that how to set up the paths depends on your folder organization but there are overall two approaches: \n\n1. you can use the `R project`, by opening the project directly you don't need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder. \n2. you can manually set the working folder with the function `setwd()`.\n\n```{r paths}\n#| label: paths\n\n# path to data folder\npath_to_data <- file.path(\"..\", # <1>\n                          \"..\", \"data\") # <2>\n\n# survey and administrative division\npath_to_survey  <- file.path(path_to_data, \"survey\", \"LSMS\", \"LSMS_ETH19.dta\")\npath_to_adm_div <- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tpr.nc\")\npath_to_tmp <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tmp.nc\")\n\n# control variables\npath_to_elevation  <- file.path(path_to_data, \"spatial\", \"elevation\", \"GloFAS\",\n                               \"elevation_glofas_v4_0.nc\")\npath_to_urca       <- file.path(path_to_data, \"spatial\", \"URCA\", \n                               \"URCA.tif\")\npath_to_pop        <- file.path(path_to_data, \"spatial\", \"population\", \"WorldPop\",\n                               \"uncontraint_1km_global\", \"ppp_2019_1km_Aggregated.tif\")\npath_to_nightlight <- file.path(path_to_data, \"spatial\", \"nighttime_light\",\n                                \"VIIRS\", \"VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif\")\npath_to_aez        <- file.path(path_to_data, \"spatial\", \"AgroEcological\", \"AEZ\", \n                                \"GAEZv5\",  \"GAEZ-V5.AEZ33-10km.tif\")\n\n# to result folder\npath_to_result <- file.path(path_to_data, \"result\")\n```\n1. concatenate the string to make a path\n2. `..` means one folder back\n\n<br>\n\n### Read the Data\n#### Survey Data\nWe start by reading the surveys data. The survey is stored as `dta` file, so we use the `haven::read_dta()` function to read it. \n\nWe only need the `hhid`, the survey coordinates, and the interview dates. We use `dplyr::select()` to choose these variables. This passage is optional and we bring with us all the variables, but we won't use them.\n\nThen we create/modify some variables with the function `dplyr::mutate()`. We transform the the variable `interview_date` from string into data, and we get the year of the median value of the date of interviews. This passage is important as it allows us to define the most appropriate year to select for the spatial variables.\n\n```{r read_srvy}\n#| label: read_srvy\n\nsrvy <- haven::read_dta(path_to_survey) |> # <1>\n    dplyr::select(survey_year, hhid, country, lat, lon, interview_date) |> # <2>\n    dplyr::mutate(\n        interview_date = clock::date_parse(interview_date, # <3>\n                                           format = \"%Y-%m-%d\"), # <4> \n        survey_year    = clock::get_year(median(interview_date)), # <5>\n        .before = hhid)\n\n```\n1. read dta type data\n2. select relevant variables\n3. transform string into date type\n4. specify format type\n5. find the median year of the interviews\n\n#### Spatial Data\nFinally, we load the spatial data. This data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or \"cell\" in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region.\n\nTo spatial data is usually stored as `tif` file or `nc`. We can read both of them them with the function `terra::rast()`.\n\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS), i.e. `EPSG:4326`.\n\n::: {.callout-important}\nWhen working with multiple spatial data, you must ensure that they have the same coordinate reference system (CRS). This is important because in this way all the data can \"spatially\" talk to each other.\n:::\n\n```{r read_spatial}\n#| label: read_spatial\n\npop <- terra::rast(path_to_pop) |> # <1>\n  setNames(\"pop\") # <2>\npop\n\nnightlight <- terra::rast(path_to_nightlight) |>\n  setNames(\"nightlight\")\nnightlight\n\nelevation <- terra::rast(path_to_elevation)\nelevation\n\nurca <- terra::rast(path_to_urca)\nurca\n\naez <- terra::rast(path_to_aez) |>\n    setNames(\"aez\") \naez\n\n```\n1. read raster type data\n2. change the name of the layer\n\nNow we also read the weather observation. The same consideration about the coordinate reference system (CRS) is still valid. When we work with raster that have also observations over time, it is important to check how and where the time and date information is stored. \nSometimes it is stored in the metadata and you can access it using `terra::time()`, other time it is already saved as the name of the layer and you can access it using `names()`.\nSometimes, like in this case the date information is stored in the names but the format is based on second passed from `1970-01-01 00:00`. To transform this observation into readable date we can use the function `second_to_date()`.\n\n::: {.callout-warning}\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n:::\n\n```{r read_weather}\n#| label: read_weather\npre <- terra::rast(path_to_pre)\npre\nnames(pre) <- terra::names(pre) |> second_to_date() # <1>\npre\n\ntmp <- terra::rast(path_to_tmp)\nnames(tmp) <- terra::names(tmp) |> second_to_date() # <1>\n```\n1. transform the layers name with second into dates\n\n#### Administrative Boundaries\nWe now move to read the administrative divisions. We use the function `read_adm_div()` to do so. This function looks for spatial polygons for the `iso` and `lvl` provided provided.\n\nEven if we have the coordinates from the survey, we will extract some spatial variables at the administrative division.\n\nThe same consideration about the coordinate reference system (CRS) is still valid.\n\n```{r read_adm_div}\n#| label: read_adm_div\n\nadm_div <- read_adm_div(path_to_adm_div, iso = \"ETH\", lvl = 2)\nadm_div\n\n```\n\n<br>\n\n### Georeference the Surveys\nAs we've mentioned, the spatial data is georeferenced, so we need to ensure the same for the survey data. \nSince many households share the same coordinates, they are linked to the same location. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called `ID`.\n\nThis is handled by the `prepare_coord()` function, which requires the coordinates' variable names as input.\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the `georef_coord()` function requires the coordinates' variable names as input.\nUsually, the WGS 84 CRS is the default coordinate references system for coordinates. In this case it  matches the weather coordinate references system.\n\nWe can print the result to check the transformation. The new column, `ID`, is created by `prepare_coord()` and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\n```{r prepare_coord}\n#| label: prepare_coord\n\nsrvy_coord <- prepare_coord(srvy,\n                            lon_var = lon,\n                            lat_var = lat)\nsrvy_coord\n```\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The function also the coordinates' variable names as input.\n\n```{r georef_coord}\n#| label: georef_coord\n\nsrvy_geo <- georef_coord(srvy_coord,\n                         geom = c(\"lon\", \"lat\"),\n                         crs = \"EPSG:4326\")\nsrvy_geo\n```\n\n::: {.callout-note}\nPay attention on the reduced number of observation between `srvy_coord` and `srvy_geo`. From `r nrow(srvy_coord)` rows to `r nrow(srvy_geo)`, these are the actual unique locations from the survey.\n:::\n<br>\n\n### Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\n```{r merge_adm_srvy}\n#| label: merge_adm_srvy\n\nsrvy_adm_div <- terra::intersect(srvy_geo, adm_div) |> \n    terra::values()\nhead(srvy_adm_div)\n```\n\n<br>\n\n### Crop the spatial variables\nThe spatial variables variables we have just load have a global coverage. It might be convenient to reduce the coverage to just the countries we are interested in.. We can do this by using the `terra::crop()` function and the administrative divisions.\n\nThis is not a compulsory step but it reduce the memory burden and allows for more meaningful plotting.\n\n```{r crop}\n#| label: crop\n\npop_cntry <- terra::crop(pop, adm_div, snap = \"out\")\n\nnghtlght_cntry <- terra::crop(nightlight, adm_div, snap = \"out\")\n\nelevatn_cntry <- terra::crop(elevation, adm_div, snap = \"out\")\n\nurca_cntry <- terra::crop(urca, adm_div, snap = \"out\")\n\naez_cntry <- terra::crop(aez, adm_div, snap = \"out\")\n\npre_cntry <- terra::crop(pre, adm_div, snap = \"out\")\n\ntmp_cntry <- terra::crop(tmp, adm_div, snap = \"out\")\n\n```\n\n<br>\n\n### Plot\n\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\n\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n```{r plot_survey_geo}\n#| label: plot_survey_geo\n\nterra::plot(adm_div, col = \"grey\", main = \"District of Ethiopia and Survey Coordinates\") # <1>\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5) # <2>\n\n```\n1. plot raster\n2. add survey locations\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the major cities and in the North.\n\nNext, we plot the spatial variables to see how it overlaps with the spatial coordinates.\n\n```{r plot_sp_var}\n#| label: plot_sp_var\n\nterra::plot(elevatn_cntry, main = \"Elevation\") # <1>\nterra::lines(adm_div, col = \"white\", lwd = 1) # <2>\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5) # <3>\n\nterra::plot(log(1+pop_cntry), main = \"Log Population\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\nterra::plot(urca_cntry, main = \"URCA\")\nterra::lines(adm_div, col = \"black\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\nterra::plot(log(1+nghtlght_cntry), main = \"Log Nighttime Light\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"ryb\"),\n            main = \"Monthly temperature at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"black\", alpha = 0.5, cex = 0.5)\n```\n1. plot raster\n2. add administrative borders\n3. add survey locations\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the different spatial resolution, with precipitation having a lower one. The consequence is that some survey coordinates still fall within the same cell.\n\n<br>\n\n### Modify the Spatial Variables\n#### Compute Terrain Indicators\nNow we compute some terrain indicators based on elevation. The terrain indicators are:\n\n* TRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and its 8 surrounding cells.\n\n* Slope is the average difference between the value of a cell and its 8 surrounding cells.\n\n* Roughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells.\n\n```{r terrain_indicator}\n#| label: terrain_indicator\n\nterrain_cntry <-  terra::terrain(elevatn_cntry,\n                                 v = c(\"slope\", \"TRI\", \"roughness\"), # <1>\n                                 neighbors = 8, #<2>\n                                 unit = \"degrees\")\n```\n1. the terrain indicators\n2. how many neighboring cells, 8 (queen case) or 4 (rook case)\n\n#### Weather Variable Transformation\nThe original unit of measure of the weather data is in meter for precipitation and Kelvin for temperature. These unit of measure are not very intuitive, therefore we change them into millimeter and Celsius respectively.\n\n```{r transformation}\n#| label: transformation\n\n# From meter to millimeters\npre_cntry_mm <- pre_cntry*1000\n\n# From Kelvin to Celsius\ntmp_cntry_c <- tmp_cntry - 273.15\n```\n\n#### Compute the Surface Area of the Administrative Divisons\nWe now compute the surface are of each administrative division. We will use it for computing the population density. We use the function `dplyr::mutate()` to add the variable `area_km` and the function `terra::expanse()` to compute the surface area.\n\n```{r compute_area}\n#| label: compute_area\n\nadm_div_area <- adm_div |>\n    dplyr::mutate(area_km = terra::expanse(adm_div, unit = \"km\"))\n```\n\n<br>\n\n### Extraction\n#### Based on Survey Coordinates\nNext, we extract the spatial data based on the survey coordinates using the `extract_by_coord()` function. This function requires the raster with the spatial data and the georeferenced coordinates as inputs.\n\nLooking at the result, we see first the `ID` column, that identifies the unique survey coordinates. The second and third column are the coordinates of the cells. The other columns contain the spatial observations, specific to each coordinate. For the weather data we have the time series of observations over time, specific to each coordinate.\n\n```{r extraction_coord}\n#| label: extraction_coord\n\nnghtlght_coord <- extract_by_coord(nghtlght_cntry, srvy_geo)\nnghtlght_coord\n\nelevation_coord <- extract_by_coord(elevatn_cntry, srvy_geo)\n\nterrain_coord <- extract_by_coord(terrain_cntry, srvy_geo)\n\nurca_coord <- extract_by_coord(urca_cntry, srvy_geo)\n\naez_coord <- extract_by_coord(aez_cntry, srvy_geo)\n\npre_coord <- extract_by_coord(pre_cntry, srvy_geo)\n\ntmp_coord <- extract_by_coord(tmp_cntry, srvy_geo)\ntmp_coord\n```\n\nAgain we have a row for each unique location from the survey. However, if we want to know how many different cells there are we can look unique cell coordinates.\n\n```{r cell_coordinate}\n#| label: cell_coordinate\n\nunique_cell <- tmp_coord |>\n  dplyr::distinct(x_cell, y_cell) # <1>\nnrow(unique_cell)\n```\n1. identifies the unique combination of the variables\n\nWe see that now the number of rows is `r nrow(unique_cell)`, this is the actual different weather observations that we can merge with the survey. We start with `r nrow(srvy)` different household, then we have `r nrow(srvy_geo)` different survey coordinates, and we end up with `r nrow(unique_cell)` different weather observations.\n\n#### Based on Administrative Divisions\nWe extract the spatial data based on the administrative division  using the `extract_by_poly()` function. This function requires the raster with the spatial data, the administrative division, and the aggregation function as input. The aggregation function, `fn_agg`, defines how the cell values that fall within an administrative division are combined into a single value. Note that by default all the cell values are weighted by the coverage area of the cell that fall within the division.\n\nContrary to the other spatial variable, for population we use `adm_div_area` to extract the values as we need the surface area to calculate the population density.\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second to fourth column are the additional information coming from the administrative division data. The last column contain the spatial observations aggregated at the administrative division.\n\n```{r extraction_adm}\n#| label: extraction_adm\n\npop_adm <- extract_by_poly(pop_cntry, adm_div_area, fn_agg = \"sum\")\npop_adm\n\nnghtlght_adm <- extract_by_poly(nghtlght_cntry, adm_div, fn_agg = \"mean\")\n\nelevation_adm <- extract_by_poly(elevatn_cntry, adm_div, fn_agg = \"mean\")\n\nterrain_adm <- extract_by_poly(terrain_cntry, adm_div, fn_agg = \"mean\")\n\nurca_adm <- extract_by_poly(urca_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm <- extract_by_poly(aez_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm\n```\n\n#### Weather Data Based on Administrative Divisions\nFro weather data, we use a different function for extracting the data, namely `extract_cell_by_poly()`. Contrary to the function `extract_by_poly()`, this doesn't aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the long run climatic parameter for cell and only later aggregate them.\n\n::: {.callout-note}\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n:::\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\n```{r extraction_adm_weather}\n#| label: extraction_adm_weather\n#| output: false\n\npre_cell <- extract_cell_by_poly(pre_cntry_mm, adm_div)\n\ntmp_cell <- extract_cell_by_poly(tmp_cntry_c, adm_div)\n```\n\n```{r print_tmp_cell}\n#| label: print_tmp_cell\n\ntmp_cell\n```\n\n<br>\n\n### Cmpute Long Run Climatic Parameter\nWe want to describe the long run climatic condition in each locations. Rule of thumb is to use 30 years of weather observations to capture climatic features. Therefore, we select the 30 years before each survey.\n\nCheck the names with the date of observations and how it has changed since before.\n\n```{r select_by_dates}\n#| label: select_by_dates\n\npre_coord_30yrs <- select_by_dates(pre_coord, from = \"1989\", to = \"2019\")\ntmp_coord_30yrs <- select_by_dates(tmp_coord, from = \"1989\", to = \"2019\")\n\npre_cell_30yrs <- select_by_dates(pre_cell, from = \"1989\", to = \"2019\" )\ntmp_cell_30yrs <- select_by_dates(tmp_cell, from = \"1989\", to = \"2019\")\ntmp_cell_30yrs\n```\n\nNow we can compute the long run climatic parameter. We calculate the mean, the standard deviation, and the coefficient of variation. We collect all the parameter in a separate object `parameter`. This object is a names list of functions and we construct it with this structure `name = function`, then the `list()` function puts them together. This passage is not compulsory but allows to perform the computation of multiple parameters in a tidy and efficient way. Otherwise we could have directly add them inside the `calc_par()`.\n\nThe function `calc_par()` calculates the required parameters.\n\nThe results have a similar structure, with the first columns that identify the specific locations and the other the computed parameters. Note how we are still carrying on the `coverage_fraction` variable as we will need it for aggregating the climatic parameter at the administrative division.\n\n```{r cal_parameter}\n#| label: cal_parameter\n\nparameter <- list(std = sd, avg = mean, coef_var = cv)\nparameter\n\npre_par_coord <- calc_par(pre_coord_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_coord <- calc_par(tmp_coord_30yrs, pars = parameter, prefix = \"tmp\")\ntmp_par_coord\n\npre_par_cell <- calc_par(pre_cell_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_cell <- calc_par(tmp_cell_30yrs, pars = parameter, prefix = \"tmp\")\n\ntmp_par_cell\n```\n\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function `agg_to_adm_div()` can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the `coverage_fraction` variable.\n\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, `ID_adm_div`, and a single value of the climatic parameters for each locations.\n\n```{r cell_to_div}\n#| label: cell_to_div\n\npre_par_adm <- agg_to_adm_div(pre_par_cell , match_col = \"pre\")\ntmp_par_adm <- agg_to_adm_div(tmp_par_cell, match_col = \"tmp\")\n\ntmp_par_adm\n```\n\n<br>\n\n### Merge with Survey\nNow that we have everything, we can combine all the extracted data and then merge them with the survey.\nWe start by combining the data into a unique data set. To do so we start by create a list with the function `list()`, each element of the list is a different spatial variable and then we combine the elements of the list with the function `purrr::reduce()`. This last function require another function as input to drive the combination and we choose to use `merge_by_common()`, which merges two data by their common variable names.\n\nWhy not using directly `merge_by_common()`? Because the function works with just two datasets and we have eight different spatial datasets. We can cumulatively merge the datasets one by one or we can use the `purrr::reduce()`. \n\nThen we compute also the population density and the logarithmic transformation of the nighttime light. We use the `dplyr::mutate()` function to add these two new variables. We use the argument `.after` to specify where the position of the variable among the columns.\n\n```{r combine_adm_spt}\n#| label: combine_adm_spt\n\nsptl_adm <- list(pop_adm, # <1>\n                 nghtlght_adm,\n                 terrain_adm, \n                 elevation_adm, \n                 urca_adm, \n                 aez_adm, \n                 pre_par_adm, \n                 tmp_par_adm) |>\n    purrr::reduce(merge_by_common) |> # <2>\n    dplyr::mutate(pop_density = pop/area_km, .after = pop) |> # <3>\n    dplyr::mutate(ln_nightlight = log(1+nightlight), .after = nightlight)\nsptl_adm\n```\n1. combine the data into a list\n2. merge all the elements of the list\n3. create new variables\n\nWe do the same for the coordinate. In this case we don't need to calculate the population density as all the values refer to specific point and not areas. However, differently form the division level data, we need to drop the cell coordinates. This because, the cell resolution is different among the spatial datasets, despite the same CRS, and thus the coordinates of the cells are slightly different among datasets. This impinges the merge and at this stage of the analysis we don't need the information they are carrying anymore.\n\nTo drop them requires a convoluted approach, but it takes advantage that all the datasets are grouped in the same list and the procedure is the same for each dataset. To apply a function to each element of a list we can use the function `purrr::map()`. As arguments, this function requires the function we want to apply, namely `dplyr::select`, and additional arguments for the function, `-c(x_cell, y_cell)` which are the columns we want to to drop. Note the minus symbol as it tells the function we want to drop the columns and not keep them.\n\n```{r combine_coord_spt}\n#| label: combine_coord_spt\n\nsptl_coord <- list(nghtlght_coord, # <1>\n                   terrain_coord, \n                   elevation_coord, \n                   urca_coord, \n                   aez_coord, \n                   pre_par_coord, \n                   tmp_par_coord) |>\n    purrr::map(dplyr::select, -c(x_cell, y_cell)) |> # <2>\n    purrr::reduce(merge_by_common) |> # <3>\n    dplyr::mutate(ln_nightlight = log(1+nightlight), .after = nightlight) # <4>\nsptl_coord\n```\n1. combine the data into a list\n2. remove variables from each elemnt of the list\n3. merge all the elements of the list\n4. create new variable\n\nNow that we have all the control variables together, we can merge them with the surveys information. The function `merge_by_common()` will do it for us.\n\nWe can see that the result has all the information we retained from the surveys and the new extracted spatial variables.\n\n```{r merge_coord_survey}\n#| label: merge_coord_survey\n\nsrvy_sptl_coord <- merge_by_common(srvy_coord, sptl_coord)\nsrvy_sptl_coord\n```\n\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as `srvy_adm_div`.\nWe first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command `|>` assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with `y = _`, where `y` is the name of the argument and `_` refer to the previous merge.\n\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n```{r merge_adm_survey}\n#| label: merge_adm_survey\n \nsrvy_sptl_adm <- merge_by_common(srvy_adm_div, sptl_adm) |> # <1>\n    merge_by_common(srvy_coord, y = _) # <2>\nsrvy_sptl_adm\n```\n1. merge adm info with spatial var\n2. `_` refers to the output of the previous merge \n\n<br>\n\n### Write\nHere we are at the end, let's save the results. We want to save the result as `dta` so we will use the `haven::write_dta()` function.\n\n```{r write}\n#| label: write\n#| eval: false\n\nhaven::write_dta(srvy_sptl_adm,\n                 file.path(path_to_result, \"ETH_sp_adm.dta\"))\nhaven::write_dta(srvy_sptl_adm,\n                 file.path(path_to_result, \"ETH_sp_coord.dta\"))\n\n```\n\n<br><br>\n\n## Take home messages\n\n* When working with multiple spatial data:\n  \n  1. remember to control the Coordinate Reference System of all dataset\n  2. plot the data to check everything is going well\n  \n* based on the typology of data we use different function\n  \n  - reading\n    * for `dta` use `haven::read_dta() or` and `haven::_write_dta()`\n    * for spatial vectors use `terra::vect()` or `read_adm_div()` for administrative divisions in specific country and level.\n    * for spatial raster use `terra::rast()`\n    \n  - extraction\n    \n    * spatial points, use `extract_by_coord()`\n    * spatial polygons, use `extract_by_poly()`\n    * cells within polygons, use `extract_cell_by_poly()`\n    \n* When working with raster data\n\n  1. check the unit of measure\n  2. if it is a time series check also the date format\n    \n* When working with spatial polygons, like administrative divisions valuate if you want to extract the values already aggregated or each cells separately\n  * for example the terrain indicators were computed for each cells and then we moved to the extraction\n  * for the climatic parameters, we extract each cells separately, we compute the parameters for each cells, and only later we aggregate them.\n\n* When georeferencing the survey location we take advantage that many interviews share the same locations. Hence, we extract the variables just for these unique locations. However, same of these unique locations may fall within the same value cells, so the actual information might be even lower.\n  \n<br><br>\n  \n## Appendix\n### New to R? Read this first!\n#### The pipe command\n\nThe pipe command `|>`. It lets you pass the result of one expression as the first argument to the next, creating a fluid chain of functions.\n\nInstead of nesting functions inside each other, you can pipe the output forward, making the code easier to read.\n\n```{r exm_pipe}\n#| label: exm_pipe\n#| eval: false\n\n4 |> log() |> exp()\n\nexp(log(4))\n```\n\nNote:\n\n* The base R pipe `|>` was introduced in R 4.1.0.\n\n* In some tutorials, you might also see `%>%`, which comes from the `magrittr` or `dplyr` packages. Both do a similar thing, but `|>` is now the official base R version.\n<br>\n\n#### The package namespaces\n\nThe package namespaces `package_name::function_name()`. As the name suggests, namespaces provide \"spaces\" for \"names\". They provide a context for looking up the value of an object associated with a name. When we write `terra::vect()` we are asking R to look for the function `vect()` in the `terra` package.\n\nIt's a fairly advanced topic, and by-and-large, not that important! When you first start using namespaces, it'll seem like a lot of work for little gain. However, having a high quality namespace helps encapsulate your package and makes it self-contained. This ensures that other packages won't interfere with your code, that your code won't interfere with other packages, and that your package works regardless of the environment in which it's run.\n\nYou can avoid using every time the name space by just loading the necessary packages at the beginning of the code (in the set up section for example). This is the most known and common approach. To do so just add `library(name_of_package)`, for example `library(terra)`. Then we can just call the function without the name space, like this `vect()`.\n\n<br>\n\n#### The assign operator\n\nThe assign operator `<-`. This is a peculiarity of R and it is used to assign values to variables. Note that the operators `<-` and `=` can be used, almost interchangeably.\n<br>\n\n#### Functions\nIn Stata, you're used to running do-files or programs to automate tasks. In R, functions play a similar role: they help you organize code and reuse it easily.\n\nA function in R looks like this:\n```{r fn_body}\n#| labek: fn_body\n#| eval: false\n\nmy_function <- function(input1, input2) {\n  # Do something with the inputs\n  result <- input1 + input2\n  return(result)\n}\n```\n\n* `my_function` is the function's name.\n\n* `function(input1, input2)` defines what inputs (arguments) it takes.\n\n* Inside `{}`, you write the code that runs when you call the function.\n\n* `return(result)` tells R what the output should be.\n\nYou call the function like this:\n```{r fn_out}\n#| label: fn_out\n#| eval: false\n\nmy_function(3, 5)\n# Output: 8\n```\n\nNote that you can change the order of the inputs if you properly label them.\n```{r fn_out2}\n#| label: fn_out2\n#| eval: false\n\nmy_function(input2 = 5, input1 = 3)\n# Output: 8\n```\n\nKey points for Stata users:\n\n* Functions in R must be assigned to a name using <- (the assignment operator).\n\n* You can think of functions a little like Stata's program define, but in R, every function can return a value to be used later.\n\n* You can nest functions inside other code, making your analysis scripts cleaner and easier to read.\n\n<br><br>\n\n### Want to know about the data?\n#### Weather\nWeather observation are obtained from ERA5-Land reanalysis dataset. H-TESSEL is the land surface model that is the basis of ERA5-Land. The data is a post-processed monthly-mean average of the original ERA5-Land dataset.\n\n| Parameter           | Value                   |\n|:--------------------|:-----------------------:|\n| spatial resolution  | 0.1° x 0.1° lon lat     |\n| temporal resolution | month                   |\n| time frame          | Jan. 1950 - Dec. 2022   |\n| unit of measure     | meter or Kelvin         |\n\nIt is possible to find additional information: \n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=overview) \n* and the related manual [here](https://confluence.ecmwf.int/display/CKB/ERA5-Land). \n\nThe data can be freely download from \n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=overview).\n\n\n###### Total precipitation\nAccumulated liquid and frozen water, including rain and snow, that falls to the Earth's surface. It is the sum of large-scale precipitation and convective precipitation. Precipitation variables do not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth.\n\n###### 2 metre above ground temperature\nTemperature of air at 2m above the surface of land, sea or in-land waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions.\n\n<br>\n\n#### Spatial variables \n##### Agro Ecological Zones\nThe Agro-ecological Zones classification (33 classes) provides a characterization of bio-physical resources relevant to agricultural production systems. AEZ definitions and map classes follow a rigorous methodology and an explicit set of principles. The inventory combines spatial layers of thermal and moisture regimes with broad categories of soil/terrain qualities. It also indicates locations of areas with irrigated soils and shows land with severely limiting bio-physical constraints including very cold and very dry (desert) areas as well as areas with very steep terrain or very poor soil/terrain conditions.\nThe AEZ classification dataset is part of the GAEZ v5 Land and Water Resources theme and Agro-ecological Zones sub-theme. All results are derived from the Agro-ecological Zones (AEZ) modeling framework, developed collaboratively by the Food and Agriculture Organization (FAO) and the International Institute for Applied Systems Analysis (IIASA). \n\n| Parameter           | Value                                                  |\n|:--------------------|:------------------------------------------------------:|\n| spatial resolution  | 10 km.                                                 |\n| temporal resolution | 20 years                                               |\n| time frame          | 2001–2020                                              |\n| unit of measure     | classification by climate/soil/terrain/LC (33 classes) |\n\nSuggested citation:\n\n* FAO & IIASA. 2025. Global Agro-ecological Zoning version 5 (GAEZ v5) Model Documentation. https://github.com/un-fao/gaezv5/wiki\n\n\nIt is possible to find additional information:\n\n* [here](https://github.com/un-fao/gaezv5/wiki). \n\nThe data can be freely download from:\n\n* [here](https://data.apps.fao.org/catalog/iso/2c09f61b-801c-47d2-a989-0abd3500a365).\n\n<br>\n\n##### Urban-Rural Catchment Area (URCA)\n\nUrban–rural catchment areas showing the catchment areas around cities and towns of different sizes (the no data value is 128). Each rural pixel is assigned to one defined travel time category to one of seven urban agglomeration sizes.\n\n| Parameter           | Value                                             |\n|:--------------------|:-------------------------------------------------:|\n| spatial resolution  | 0.03° x 0.03° lon lat                             |\n| temporal resolution | year                                              |\n| time frame          | 2015                                              |\n| unit of measure     | travel time category to different urban hierarchy |\n\n\nSuggested citation:\n\n* Cattaneo, Andrea; Nelson, Andy; McMenomy, Theresa (2020). Urban-rural continuum. figshare. Dataset. https://doi.org/10.6084/m9.figshare.12579572.v4\n\nIt is possible to find additional information:\n\n* [here](https://www.pnas.org/doi/full/10.1073/pnas.2011990118). \n\n\nThe data can be freely download from:\n\n* [here](https://figshare.com/articles/dataset/Urban-rural_continuum/12579572).\n\n<br>\n\n##### Population\nThe units are number of people per pixel. The mapping approach is Random Forest-based dasymetric redistribution.\n\n| Parameter           | Value                                   |\n|:--------------------|:---------------------------------------:|\n| spatial resolution  | 30 arc second (~1km)                    |\n| temporal resolution | year                                    |\n| time frame          | 2010 - 2020                             |\n| unit of measure     | estimated count of people per grid-cell |\n\nSuggested citation:\n\n*\tWorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00647 \n\nIt is possible to find additional information from:\n\n* [here](https://www.worldpop.org/methods/top_down_constrained_vs_unconstrained/)\n* [here](https://www.worldpop.org/methods/populations/). \n\nThe data can be freely download from:\n\n* [here](https://hub.worldpop.org/geodata/summary?id=34984).\n\n<br>\n\n##### Nighttime light\n\nVIIRS nighttime lights (VNL) version V2.1: annual values obtained by from the monthly averages with filtering to remove extraneous features such as biomass burning, aurora, and background.\n\n| Parameter           | Value                       |\n|:--------------------|:---------------------------:|\n| spatial resolution  | 15 arc second               |\n| temporal resolution | year                        |\n| time frame          | 2012 - 2021                 |\n| unit of measure     | nW/cm2/sr, average-masked   |\n\nSuggested citation:\n\n* Elvidge, C.D, Zhizhin, M., Ghosh T., Hsu FC, Taneja J. Annual time series of global VIIRS nighttime lights derived from monthly averages:2012 to 2019. Remote Sensing 2021, 13(5), p.922, doi:10.3390/rs13050922\n\nIt is possible to find additional information:\n\n* [here](https://eogdata.mines.edu/products/vnl/). \n\nThe data can be freely download from:\n\n* [here](https://eogdata.mines.edu/nighttime_light/annual/v21/).\n\n<br>\n\n##### Elevation\nElevation is obtained from the auxiliary variables of GloFAS. Each pixel is the mean height elevation above sea level.\n\n| Parameter           | Value                       |\n|:--------------------|:---------------------------:|\n| spatial resolution  |\t0.03° x 0.03° lon lat       |\n| temporal resolution | 30 years                    |\n| time frame          | 1981 - 2010                 |\n| unit of measure     | Meter (m)                   |\n\n\nWeb resources:\n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-glofas-forecast?tab=overview).\n* [here](https://www.globalfloods.eu/technical-information/products/)\n\nData access:\n\n* [here](https://confluence.ecmwf.int/display/CEMS/Auxiliary+Data)\n\n<br>\n\n#### Survey\n\nThe Living Standards Measurement Study - Integrated Surveys on Agriculture (LSMS-ISA) is a unique system of longitudinal surveys designed to improve the understanding of household and individual welfare, livelihoods and smallholder agriculture in Africa. The LSMS team works with national statistics offices to design and implement household surveys with a strong focus on agriculture.\n\nIt is possible to find additional information:\n\n* [here](https://www.worldbank.org/en/programs/lsms). \n\nThe data can be freely download from:\n\n* [here](https://microdata.worldbank.org/index.php/home).\n\n<br>\n\n#### Administrative boundaries\n\nThe administrative divisions are obtained from GeoBoundaries[^2]. GeoBoundaries Built by the community and William & Mary geoLab, the geoBoundaries Global Database of Political Administrative Boundaries Database is an online, open license (CC BY 4.0) resource of information on administrative boundaries (i.e., state, county) for every country in the world. Since 2016, we have tracked approximately 1 million boundaries within over 200 entities, including all UN member states.\n\n[^2]: Runfola D, Anderson A, Baier H, Crittenden M, Dowker E, Fuhrig S, et al. (2020) geoBoundaries: A global database of political administrative boundaries. PLoS ONE 15(4): e0231866. https://doi.org/10.1371/journal.pone.0231866.\n\nIt is possible to find additional information:\n\n* [here](https://www.geoboundaries.org/countryDownloads.html). \n\nThe data can be freely download from:\n\n* [here](https://www.geoboundaries.org).\n\n\n\n\n","srcMarkdownNoYaml":"\n\n## Introduction\nThis tutorial show how too extract spatial control variables based on surveys locations. The survey refers to Ethipia 2019 and comes from the [The World Bank Living Standards Measurement Study (LSMS)](https://www.worldbank.org/en/programs/lsms).\n\n### Is this guide for me?\n\nThis guide provides a step-by-step approach to extract raster data based on survey locations. The target audience includes economists who may have experience with statistical software (e.g. STATA) but are less familiar with spatial data processing in R.\n\nThe document is not meant to be a course on R or on how the functions work. It is just a practice example on how to extract raster data based on coordinate location. This is done by using specific functions that wrap up as many steps as possible to ensure it is easier for the user to follow.\n\n### What do I need before starting?\n\nThe following R packages are necessary: `terra`, `tidyverse`, `haven`, `purrr` and `climatic4economist`. To install the above package you can use `install.packages(\"name_of_package\")`, don't forget the `\"`.\n\nThe latter package, `climatic4economist` is not available on the web and it need to be installed from a local file. You can do it with `devtools::install_local(\"climatic4economist\")`\n\nIf you are not familiar with R check the [appendix] for understanding some coding style used in this tutorial.\n\n<br><br>\n\n## Code\n### Set Up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only `climatic4economist` package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\n```{r packages}\n#| label: packages\n#| output: false\n\nlibrary(climatic4economist)\n```\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note `..` means one step back to the folder directory, i.e. one folder back.\n\nNote that how to set up the paths depends on your folder organization but there are overall two approaches: \n\n1. you can use the `R project`, by opening the project directly you don't need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder. \n2. you can manually set the working folder with the function `setwd()`.\n\n```{r paths}\n#| label: paths\n\n# path to data folder\npath_to_data <- file.path(\"..\", # <1>\n                          \"..\", \"data\") # <2>\n\n# survey and administrative division\npath_to_survey  <- file.path(path_to_data, \"survey\", \"LSMS\", \"LSMS_ETH19.dta\")\npath_to_adm_div <- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tpr.nc\")\npath_to_tmp <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tmp.nc\")\n\n# control variables\npath_to_elevation  <- file.path(path_to_data, \"spatial\", \"elevation\", \"GloFAS\",\n                               \"elevation_glofas_v4_0.nc\")\npath_to_urca       <- file.path(path_to_data, \"spatial\", \"URCA\", \n                               \"URCA.tif\")\npath_to_pop        <- file.path(path_to_data, \"spatial\", \"population\", \"WorldPop\",\n                               \"uncontraint_1km_global\", \"ppp_2019_1km_Aggregated.tif\")\npath_to_nightlight <- file.path(path_to_data, \"spatial\", \"nighttime_light\",\n                                \"VIIRS\", \"VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif\")\npath_to_aez        <- file.path(path_to_data, \"spatial\", \"AgroEcological\", \"AEZ\", \n                                \"GAEZv5\",  \"GAEZ-V5.AEZ33-10km.tif\")\n\n# to result folder\npath_to_result <- file.path(path_to_data, \"result\")\n```\n1. concatenate the string to make a path\n2. `..` means one folder back\n\n<br>\n\n### Read the Data\n#### Survey Data\nWe start by reading the surveys data. The survey is stored as `dta` file, so we use the `haven::read_dta()` function to read it. \n\nWe only need the `hhid`, the survey coordinates, and the interview dates. We use `dplyr::select()` to choose these variables. This passage is optional and we bring with us all the variables, but we won't use them.\n\nThen we create/modify some variables with the function `dplyr::mutate()`. We transform the the variable `interview_date` from string into data, and we get the year of the median value of the date of interviews. This passage is important as it allows us to define the most appropriate year to select for the spatial variables.\n\n```{r read_srvy}\n#| label: read_srvy\n\nsrvy <- haven::read_dta(path_to_survey) |> # <1>\n    dplyr::select(survey_year, hhid, country, lat, lon, interview_date) |> # <2>\n    dplyr::mutate(\n        interview_date = clock::date_parse(interview_date, # <3>\n                                           format = \"%Y-%m-%d\"), # <4> \n        survey_year    = clock::get_year(median(interview_date)), # <5>\n        .before = hhid)\n\n```\n1. read dta type data\n2. select relevant variables\n3. transform string into date type\n4. specify format type\n5. find the median year of the interviews\n\n#### Spatial Data\nFinally, we load the spatial data. This data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or \"cell\" in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region.\n\nTo spatial data is usually stored as `tif` file or `nc`. We can read both of them them with the function `terra::rast()`.\n\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS), i.e. `EPSG:4326`.\n\n::: {.callout-important}\nWhen working with multiple spatial data, you must ensure that they have the same coordinate reference system (CRS). This is important because in this way all the data can \"spatially\" talk to each other.\n:::\n\n```{r read_spatial}\n#| label: read_spatial\n\npop <- terra::rast(path_to_pop) |> # <1>\n  setNames(\"pop\") # <2>\npop\n\nnightlight <- terra::rast(path_to_nightlight) |>\n  setNames(\"nightlight\")\nnightlight\n\nelevation <- terra::rast(path_to_elevation)\nelevation\n\nurca <- terra::rast(path_to_urca)\nurca\n\naez <- terra::rast(path_to_aez) |>\n    setNames(\"aez\") \naez\n\n```\n1. read raster type data\n2. change the name of the layer\n\nNow we also read the weather observation. The same consideration about the coordinate reference system (CRS) is still valid. When we work with raster that have also observations over time, it is important to check how and where the time and date information is stored. \nSometimes it is stored in the metadata and you can access it using `terra::time()`, other time it is already saved as the name of the layer and you can access it using `names()`.\nSometimes, like in this case the date information is stored in the names but the format is based on second passed from `1970-01-01 00:00`. To transform this observation into readable date we can use the function `second_to_date()`.\n\n::: {.callout-warning}\nNote that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldn’t need to rename the layers.\n:::\n\n```{r read_weather}\n#| label: read_weather\npre <- terra::rast(path_to_pre)\npre\nnames(pre) <- terra::names(pre) |> second_to_date() # <1>\npre\n\ntmp <- terra::rast(path_to_tmp)\nnames(tmp) <- terra::names(tmp) |> second_to_date() # <1>\n```\n1. transform the layers name with second into dates\n\n#### Administrative Boundaries\nWe now move to read the administrative divisions. We use the function `read_adm_div()` to do so. This function looks for spatial polygons for the `iso` and `lvl` provided provided.\n\nEven if we have the coordinates from the survey, we will extract some spatial variables at the administrative division.\n\nThe same consideration about the coordinate reference system (CRS) is still valid.\n\n```{r read_adm_div}\n#| label: read_adm_div\n\nadm_div <- read_adm_div(path_to_adm_div, iso = \"ETH\", lvl = 2)\nadm_div\n\n```\n\n<br>\n\n### Georeference the Surveys\nAs we've mentioned, the spatial data is georeferenced, so we need to ensure the same for the survey data. \nSince many households share the same coordinates, they are linked to the same location. To reduce computation time, we extract data only for the unique coordinates, rather than for each household. Moreover, we must ensure that we can later associate the correct weather data with the right household, we do this by creating an merging variable called `ID`.\n\nThis is handled by the `prepare_coord()` function, which requires the coordinates' variable names as input.\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The CRS is provided as an argument of the function, using the previously saved CRS from the weather data. Also the `georef_coord()` function requires the coordinates' variable names as input.\nUsually, the WGS 84 CRS is the default coordinate references system for coordinates. In this case it  matches the weather coordinate references system.\n\nWe can print the result to check the transformation. The new column, `ID`, is created by `prepare_coord()` and identifies each unique coordinate. This is used to merge the weather data with the household data.\n\n```{r prepare_coord}\n#| label: prepare_coord\n\nsrvy_coord <- prepare_coord(srvy,\n                            lon_var = lon,\n                            lat_var = lat)\nsrvy_coord\n```\n\nOnce we have the unique coordinates, we are ready to transform them into spatial points using the `georef_coord()` function. When performing this transformation, it's crucial to set the correct CRS, which must match that of the weather data. The function also the coordinates' variable names as input.\n\n```{r georef_coord}\n#| label: georef_coord\n\nsrvy_geo <- georef_coord(srvy_coord,\n                         geom = c(\"lon\", \"lat\"),\n                         crs = \"EPSG:4326\")\nsrvy_geo\n```\n\n::: {.callout-note}\nPay attention on the reduced number of observation between `srvy_coord` and `srvy_geo`. From `r nrow(srvy_coord)` rows to `r nrow(srvy_geo)`, these are the actual unique locations from the survey.\n:::\n<br>\n\n### Merge administrative division and survey\nWe want to associated the survey location to the administrative divisions. We do it by looking in which administrative division each survey location fall in. We save this information for later use.\n\n```{r merge_adm_srvy}\n#| label: merge_adm_srvy\n\nsrvy_adm_div <- terra::intersect(srvy_geo, adm_div) |> \n    terra::values()\nhead(srvy_adm_div)\n```\n\n<br>\n\n### Crop the spatial variables\nThe spatial variables variables we have just load have a global coverage. It might be convenient to reduce the coverage to just the countries we are interested in.. We can do this by using the `terra::crop()` function and the administrative divisions.\n\nThis is not a compulsory step but it reduce the memory burden and allows for more meaningful plotting.\n\n```{r crop}\n#| label: crop\n\npop_cntry <- terra::crop(pop, adm_div, snap = \"out\")\n\nnghtlght_cntry <- terra::crop(nightlight, adm_div, snap = \"out\")\n\nelevatn_cntry <- terra::crop(elevation, adm_div, snap = \"out\")\n\nurca_cntry <- terra::crop(urca, adm_div, snap = \"out\")\n\naez_cntry <- terra::crop(aez, adm_div, snap = \"out\")\n\npre_cntry <- terra::crop(pre, adm_div, snap = \"out\")\n\ntmp_cntry <- terra::crop(tmp, adm_div, snap = \"out\")\n\n```\n\n<br>\n\n### Plot\n\nA good practice when working with spatial data is to plot it. This is the best way to verify that everything is working as expected.\n\nFirst, we plot the survey coordinates to ensure they are correctly located within the country and to examine their spatial distribution.\n\n```{r plot_survey_geo}\n#| label: plot_survey_geo\n\nterra::plot(adm_div, col = \"grey\", main = \"District of Ethiopia and Survey Coordinates\") # <1>\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5) # <2>\n\n```\n1. plot raster\n2. add survey locations\n\nWe confirm that the survey locations are within the country borders, which is great! We also observe that the spatial distribution of survey coordinates is neither random nor uniform; most are concentrated near the major cities and in the North.\n\nNext, we plot the spatial variables to see how it overlaps with the spatial coordinates.\n\n```{r plot_sp_var}\n#| label: plot_sp_var\n\nterra::plot(elevatn_cntry, main = \"Elevation\") # <1>\nterra::lines(adm_div, col = \"white\", lwd = 1) # <2>\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5) # <3>\n\nterra::plot(log(1+pop_cntry), main = \"Log Population\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\nterra::plot(urca_cntry, main = \"URCA\")\nterra::lines(adm_div, col = \"black\", lwd = 2)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\nterra::plot(log(1+nghtlght_cntry), main = \"Log Nighttime Light\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 1, cex = 0.6)\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"water\"),\n            main = \"Monthly precipitation at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"red\", alpha = 0.5, cex = 0.5)\n\nterra::plot(tmp_cntry, \"2024-03-01\", col = terra::map.pal(\"ryb\"),\n            main = \"Monthly temperature at 2024-03 and survey location\")\nterra::lines(adm_div, col = \"white\", lwd = 1)\nterra::points(srvy_geo, col = \"black\", alpha = 0.5, cex = 0.5)\n```\n1. plot raster\n2. add administrative borders\n3. add survey locations\n\nOnce again, the survey coordinates align with the precipitation data, which is great! We can also observe the different spatial resolution, with precipitation having a lower one. The consequence is that some survey coordinates still fall within the same cell.\n\n<br>\n\n### Modify the Spatial Variables\n#### Compute Terrain Indicators\nNow we compute some terrain indicators based on elevation. The terrain indicators are:\n\n* TRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and its 8 surrounding cells.\n\n* Slope is the average difference between the value of a cell and its 8 surrounding cells.\n\n* Roughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells.\n\n```{r terrain_indicator}\n#| label: terrain_indicator\n\nterrain_cntry <-  terra::terrain(elevatn_cntry,\n                                 v = c(\"slope\", \"TRI\", \"roughness\"), # <1>\n                                 neighbors = 8, #<2>\n                                 unit = \"degrees\")\n```\n1. the terrain indicators\n2. how many neighboring cells, 8 (queen case) or 4 (rook case)\n\n#### Weather Variable Transformation\nThe original unit of measure of the weather data is in meter for precipitation and Kelvin for temperature. These unit of measure are not very intuitive, therefore we change them into millimeter and Celsius respectively.\n\n```{r transformation}\n#| label: transformation\n\n# From meter to millimeters\npre_cntry_mm <- pre_cntry*1000\n\n# From Kelvin to Celsius\ntmp_cntry_c <- tmp_cntry - 273.15\n```\n\n#### Compute the Surface Area of the Administrative Divisons\nWe now compute the surface are of each administrative division. We will use it for computing the population density. We use the function `dplyr::mutate()` to add the variable `area_km` and the function `terra::expanse()` to compute the surface area.\n\n```{r compute_area}\n#| label: compute_area\n\nadm_div_area <- adm_div |>\n    dplyr::mutate(area_km = terra::expanse(adm_div, unit = \"km\"))\n```\n\n<br>\n\n### Extraction\n#### Based on Survey Coordinates\nNext, we extract the spatial data based on the survey coordinates using the `extract_by_coord()` function. This function requires the raster with the spatial data and the georeferenced coordinates as inputs.\n\nLooking at the result, we see first the `ID` column, that identifies the unique survey coordinates. The second and third column are the coordinates of the cells. The other columns contain the spatial observations, specific to each coordinate. For the weather data we have the time series of observations over time, specific to each coordinate.\n\n```{r extraction_coord}\n#| label: extraction_coord\n\nnghtlght_coord <- extract_by_coord(nghtlght_cntry, srvy_geo)\nnghtlght_coord\n\nelevation_coord <- extract_by_coord(elevatn_cntry, srvy_geo)\n\nterrain_coord <- extract_by_coord(terrain_cntry, srvy_geo)\n\nurca_coord <- extract_by_coord(urca_cntry, srvy_geo)\n\naez_coord <- extract_by_coord(aez_cntry, srvy_geo)\n\npre_coord <- extract_by_coord(pre_cntry, srvy_geo)\n\ntmp_coord <- extract_by_coord(tmp_cntry, srvy_geo)\ntmp_coord\n```\n\nAgain we have a row for each unique location from the survey. However, if we want to know how many different cells there are we can look unique cell coordinates.\n\n```{r cell_coordinate}\n#| label: cell_coordinate\n\nunique_cell <- tmp_coord |>\n  dplyr::distinct(x_cell, y_cell) # <1>\nnrow(unique_cell)\n```\n1. identifies the unique combination of the variables\n\nWe see that now the number of rows is `r nrow(unique_cell)`, this is the actual different weather observations that we can merge with the survey. We start with `r nrow(srvy)` different household, then we have `r nrow(srvy_geo)` different survey coordinates, and we end up with `r nrow(unique_cell)` different weather observations.\n\n#### Based on Administrative Divisions\nWe extract the spatial data based on the administrative division  using the `extract_by_poly()` function. This function requires the raster with the spatial data, the administrative division, and the aggregation function as input. The aggregation function, `fn_agg`, defines how the cell values that fall within an administrative division are combined into a single value. Note that by default all the cell values are weighted by the coverage area of the cell that fall within the division.\n\nContrary to the other spatial variable, for population we use `adm_div_area` to extract the values as we need the surface area to calculate the population density.\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second to fourth column are the additional information coming from the administrative division data. The last column contain the spatial observations aggregated at the administrative division.\n\n```{r extraction_adm}\n#| label: extraction_adm\n\npop_adm <- extract_by_poly(pop_cntry, adm_div_area, fn_agg = \"sum\")\npop_adm\n\nnghtlght_adm <- extract_by_poly(nghtlght_cntry, adm_div, fn_agg = \"mean\")\n\nelevation_adm <- extract_by_poly(elevatn_cntry, adm_div, fn_agg = \"mean\")\n\nterrain_adm <- extract_by_poly(terrain_cntry, adm_div, fn_agg = \"mean\")\n\nurca_adm <- extract_by_poly(urca_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm <- extract_by_poly(aez_cntry, adm_div, fn_agg = \"modal\")\n\naez_adm\n```\n\n#### Weather Data Based on Administrative Divisions\nFro weather data, we use a different function for extracting the data, namely `extract_cell_by_poly()`. Contrary to the function `extract_by_poly()`, this doesn't aggregate the values within the polygons but extract each all the cell values within the division separately. This is important as we want to compute the long run climatic parameter for cell and only later aggregate them.\n\n::: {.callout-note}\nTo extract each cells is more computationally and memory demanding, especially with large countries and long time series, but it increases precision as the aggregation, thus lost of information, is done at very last stage of the process.\n:::\n\nLooking at the result, we see first the `ID_adm_div` column, that identifies the unique administrative divisions. The second and third column are the coordinates of the cells. The fourth is the amount of the cell that actually falls within the administrative division. The other columns contain the weather observations over time specific to each cell.\n\n```{r extraction_adm_weather}\n#| label: extraction_adm_weather\n#| output: false\n\npre_cell <- extract_cell_by_poly(pre_cntry_mm, adm_div)\n\ntmp_cell <- extract_cell_by_poly(tmp_cntry_c, adm_div)\n```\n\n```{r print_tmp_cell}\n#| label: print_tmp_cell\n\ntmp_cell\n```\n\n<br>\n\n### Cmpute Long Run Climatic Parameter\nWe want to describe the long run climatic condition in each locations. Rule of thumb is to use 30 years of weather observations to capture climatic features. Therefore, we select the 30 years before each survey.\n\nCheck the names with the date of observations and how it has changed since before.\n\n```{r select_by_dates}\n#| label: select_by_dates\n\npre_coord_30yrs <- select_by_dates(pre_coord, from = \"1989\", to = \"2019\")\ntmp_coord_30yrs <- select_by_dates(tmp_coord, from = \"1989\", to = \"2019\")\n\npre_cell_30yrs <- select_by_dates(pre_cell, from = \"1989\", to = \"2019\" )\ntmp_cell_30yrs <- select_by_dates(tmp_cell, from = \"1989\", to = \"2019\")\ntmp_cell_30yrs\n```\n\nNow we can compute the long run climatic parameter. We calculate the mean, the standard deviation, and the coefficient of variation. We collect all the parameter in a separate object `parameter`. This object is a names list of functions and we construct it with this structure `name = function`, then the `list()` function puts them together. This passage is not compulsory but allows to perform the computation of multiple parameters in a tidy and efficient way. Otherwise we could have directly add them inside the `calc_par()`.\n\nThe function `calc_par()` calculates the required parameters.\n\nThe results have a similar structure, with the first columns that identify the specific locations and the other the computed parameters. Note how we are still carrying on the `coverage_fraction` variable as we will need it for aggregating the climatic parameter at the administrative division.\n\n```{r cal_parameter}\n#| label: cal_parameter\n\nparameter <- list(std = sd, avg = mean, coef_var = cv)\nparameter\n\npre_par_coord <- calc_par(pre_coord_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_coord <- calc_par(tmp_coord_30yrs, pars = parameter, prefix = \"tmp\")\ntmp_par_coord\n\npre_par_cell <- calc_par(pre_cell_30yrs, pars = parameter, prefix = \"pre\")\ntmp_par_cell <- calc_par(tmp_cell_30yrs, pars = parameter, prefix = \"tmp\")\n\ntmp_par_cell\n```\n\nWe have computed the climatic parameters for each cells but we still need to aggregate them at the administrative divisions. The function `agg_to_adm_div()` can do it for us, be aware the the function aggregate by using the weighted mean, where the weights are provided by the `coverage_fraction` variable.\n\nIn the results we lose the the information on the specific cells and we are left only with the administrative division id, `ID_adm_div`, and a single value of the climatic parameters for each locations.\n\n```{r cell_to_div}\n#| label: cell_to_div\n\npre_par_adm <- agg_to_adm_div(pre_par_cell , match_col = \"pre\")\ntmp_par_adm <- agg_to_adm_div(tmp_par_cell, match_col = \"tmp\")\n\ntmp_par_adm\n```\n\n<br>\n\n### Merge with Survey\nNow that we have everything, we can combine all the extracted data and then merge them with the survey.\nWe start by combining the data into a unique data set. To do so we start by create a list with the function `list()`, each element of the list is a different spatial variable and then we combine the elements of the list with the function `purrr::reduce()`. This last function require another function as input to drive the combination and we choose to use `merge_by_common()`, which merges two data by their common variable names.\n\nWhy not using directly `merge_by_common()`? Because the function works with just two datasets and we have eight different spatial datasets. We can cumulatively merge the datasets one by one or we can use the `purrr::reduce()`. \n\nThen we compute also the population density and the logarithmic transformation of the nighttime light. We use the `dplyr::mutate()` function to add these two new variables. We use the argument `.after` to specify where the position of the variable among the columns.\n\n```{r combine_adm_spt}\n#| label: combine_adm_spt\n\nsptl_adm <- list(pop_adm, # <1>\n                 nghtlght_adm,\n                 terrain_adm, \n                 elevation_adm, \n                 urca_adm, \n                 aez_adm, \n                 pre_par_adm, \n                 tmp_par_adm) |>\n    purrr::reduce(merge_by_common) |> # <2>\n    dplyr::mutate(pop_density = pop/area_km, .after = pop) |> # <3>\n    dplyr::mutate(ln_nightlight = log(1+nightlight), .after = nightlight)\nsptl_adm\n```\n1. combine the data into a list\n2. merge all the elements of the list\n3. create new variables\n\nWe do the same for the coordinate. In this case we don't need to calculate the population density as all the values refer to specific point and not areas. However, differently form the division level data, we need to drop the cell coordinates. This because, the cell resolution is different among the spatial datasets, despite the same CRS, and thus the coordinates of the cells are slightly different among datasets. This impinges the merge and at this stage of the analysis we don't need the information they are carrying anymore.\n\nTo drop them requires a convoluted approach, but it takes advantage that all the datasets are grouped in the same list and the procedure is the same for each dataset. To apply a function to each element of a list we can use the function `purrr::map()`. As arguments, this function requires the function we want to apply, namely `dplyr::select`, and additional arguments for the function, `-c(x_cell, y_cell)` which are the columns we want to to drop. Note the minus symbol as it tells the function we want to drop the columns and not keep them.\n\n```{r combine_coord_spt}\n#| label: combine_coord_spt\n\nsptl_coord <- list(nghtlght_coord, # <1>\n                   terrain_coord, \n                   elevation_coord, \n                   urca_coord, \n                   aez_coord, \n                   pre_par_coord, \n                   tmp_par_coord) |>\n    purrr::map(dplyr::select, -c(x_cell, y_cell)) |> # <2>\n    purrr::reduce(merge_by_common) |> # <3>\n    dplyr::mutate(ln_nightlight = log(1+nightlight), .after = nightlight) # <4>\nsptl_coord\n```\n1. combine the data into a list\n2. remove variables from each elemnt of the list\n3. merge all the elements of the list\n4. create new variable\n\nNow that we have all the control variables together, we can merge them with the surveys information. The function `merge_by_common()` will do it for us.\n\nWe can see that the result has all the information we retained from the surveys and the new extracted spatial variables.\n\n```{r merge_coord_survey}\n#| label: merge_coord_survey\n\nsrvy_sptl_coord <- merge_by_common(srvy_coord, sptl_coord)\nsrvy_sptl_coord\n```\n\nHowever, the surveys do not carry information on the administrative division we have used, therefore we need an additional step to provide this information. We calculated this link information before and save it as `srvy_adm_div`.\nWe first merge the link information with the spatial extracted variables, the output is then merge with the survey. Note that the pipe command `|>` assumes that the left side is the first argument in the function, as it is not the case for us we need to specify it with `y = _`, where `y` is the name of the argument and `_` refer to the previous merge.\n\nWe can see that the result has all the information we retained from the surveys, the information about the administrative divisions, and the new extracted spatial variables.\n\n```{r merge_adm_survey}\n#| label: merge_adm_survey\n \nsrvy_sptl_adm <- merge_by_common(srvy_adm_div, sptl_adm) |> # <1>\n    merge_by_common(srvy_coord, y = _) # <2>\nsrvy_sptl_adm\n```\n1. merge adm info with spatial var\n2. `_` refers to the output of the previous merge \n\n<br>\n\n### Write\nHere we are at the end, let's save the results. We want to save the result as `dta` so we will use the `haven::write_dta()` function.\n\n```{r write}\n#| label: write\n#| eval: false\n\nhaven::write_dta(srvy_sptl_adm,\n                 file.path(path_to_result, \"ETH_sp_adm.dta\"))\nhaven::write_dta(srvy_sptl_adm,\n                 file.path(path_to_result, \"ETH_sp_coord.dta\"))\n\n```\n\n<br><br>\n\n## Take home messages\n\n* When working with multiple spatial data:\n  \n  1. remember to control the Coordinate Reference System of all dataset\n  2. plot the data to check everything is going well\n  \n* based on the typology of data we use different function\n  \n  - reading\n    * for `dta` use `haven::read_dta() or` and `haven::_write_dta()`\n    * for spatial vectors use `terra::vect()` or `read_adm_div()` for administrative divisions in specific country and level.\n    * for spatial raster use `terra::rast()`\n    \n  - extraction\n    \n    * spatial points, use `extract_by_coord()`\n    * spatial polygons, use `extract_by_poly()`\n    * cells within polygons, use `extract_cell_by_poly()`\n    \n* When working with raster data\n\n  1. check the unit of measure\n  2. if it is a time series check also the date format\n    \n* When working with spatial polygons, like administrative divisions valuate if you want to extract the values already aggregated or each cells separately\n  * for example the terrain indicators were computed for each cells and then we moved to the extraction\n  * for the climatic parameters, we extract each cells separately, we compute the parameters for each cells, and only later we aggregate them.\n\n* When georeferencing the survey location we take advantage that many interviews share the same locations. Hence, we extract the variables just for these unique locations. However, same of these unique locations may fall within the same value cells, so the actual information might be even lower.\n  \n<br><br>\n  \n## Appendix\n### New to R? Read this first!\n#### The pipe command\n\nThe pipe command `|>`. It lets you pass the result of one expression as the first argument to the next, creating a fluid chain of functions.\n\nInstead of nesting functions inside each other, you can pipe the output forward, making the code easier to read.\n\n```{r exm_pipe}\n#| label: exm_pipe\n#| eval: false\n\n4 |> log() |> exp()\n\nexp(log(4))\n```\n\nNote:\n\n* The base R pipe `|>` was introduced in R 4.1.0.\n\n* In some tutorials, you might also see `%>%`, which comes from the `magrittr` or `dplyr` packages. Both do a similar thing, but `|>` is now the official base R version.\n<br>\n\n#### The package namespaces\n\nThe package namespaces `package_name::function_name()`. As the name suggests, namespaces provide \"spaces\" for \"names\". They provide a context for looking up the value of an object associated with a name. When we write `terra::vect()` we are asking R to look for the function `vect()` in the `terra` package.\n\nIt's a fairly advanced topic, and by-and-large, not that important! When you first start using namespaces, it'll seem like a lot of work for little gain. However, having a high quality namespace helps encapsulate your package and makes it self-contained. This ensures that other packages won't interfere with your code, that your code won't interfere with other packages, and that your package works regardless of the environment in which it's run.\n\nYou can avoid using every time the name space by just loading the necessary packages at the beginning of the code (in the set up section for example). This is the most known and common approach. To do so just add `library(name_of_package)`, for example `library(terra)`. Then we can just call the function without the name space, like this `vect()`.\n\n<br>\n\n#### The assign operator\n\nThe assign operator `<-`. This is a peculiarity of R and it is used to assign values to variables. Note that the operators `<-` and `=` can be used, almost interchangeably.\n<br>\n\n#### Functions\nIn Stata, you're used to running do-files or programs to automate tasks. In R, functions play a similar role: they help you organize code and reuse it easily.\n\nA function in R looks like this:\n```{r fn_body}\n#| labek: fn_body\n#| eval: false\n\nmy_function <- function(input1, input2) {\n  # Do something with the inputs\n  result <- input1 + input2\n  return(result)\n}\n```\n\n* `my_function` is the function's name.\n\n* `function(input1, input2)` defines what inputs (arguments) it takes.\n\n* Inside `{}`, you write the code that runs when you call the function.\n\n* `return(result)` tells R what the output should be.\n\nYou call the function like this:\n```{r fn_out}\n#| label: fn_out\n#| eval: false\n\nmy_function(3, 5)\n# Output: 8\n```\n\nNote that you can change the order of the inputs if you properly label them.\n```{r fn_out2}\n#| label: fn_out2\n#| eval: false\n\nmy_function(input2 = 5, input1 = 3)\n# Output: 8\n```\n\nKey points for Stata users:\n\n* Functions in R must be assigned to a name using <- (the assignment operator).\n\n* You can think of functions a little like Stata's program define, but in R, every function can return a value to be used later.\n\n* You can nest functions inside other code, making your analysis scripts cleaner and easier to read.\n\n<br><br>\n\n### Want to know about the data?\n#### Weather\nWeather observation are obtained from ERA5-Land reanalysis dataset. H-TESSEL is the land surface model that is the basis of ERA5-Land. The data is a post-processed monthly-mean average of the original ERA5-Land dataset.\n\n| Parameter           | Value                   |\n|:--------------------|:-----------------------:|\n| spatial resolution  | 0.1° x 0.1° lon lat     |\n| temporal resolution | month                   |\n| time frame          | Jan. 1950 - Dec. 2022   |\n| unit of measure     | meter or Kelvin         |\n\nIt is possible to find additional information: \n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=overview) \n* and the related manual [here](https://confluence.ecmwf.int/display/CKB/ERA5-Land). \n\nThe data can be freely download from \n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=overview).\n\n\n###### Total precipitation\nAccumulated liquid and frozen water, including rain and snow, that falls to the Earth's surface. It is the sum of large-scale precipitation and convective precipitation. Precipitation variables do not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth.\n\n###### 2 metre above ground temperature\nTemperature of air at 2m above the surface of land, sea or in-land waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions.\n\n<br>\n\n#### Spatial variables \n##### Agro Ecological Zones\nThe Agro-ecological Zones classification (33 classes) provides a characterization of bio-physical resources relevant to agricultural production systems. AEZ definitions and map classes follow a rigorous methodology and an explicit set of principles. The inventory combines spatial layers of thermal and moisture regimes with broad categories of soil/terrain qualities. It also indicates locations of areas with irrigated soils and shows land with severely limiting bio-physical constraints including very cold and very dry (desert) areas as well as areas with very steep terrain or very poor soil/terrain conditions.\nThe AEZ classification dataset is part of the GAEZ v5 Land and Water Resources theme and Agro-ecological Zones sub-theme. All results are derived from the Agro-ecological Zones (AEZ) modeling framework, developed collaboratively by the Food and Agriculture Organization (FAO) and the International Institute for Applied Systems Analysis (IIASA). \n\n| Parameter           | Value                                                  |\n|:--------------------|:------------------------------------------------------:|\n| spatial resolution  | 10 km.                                                 |\n| temporal resolution | 20 years                                               |\n| time frame          | 2001–2020                                              |\n| unit of measure     | classification by climate/soil/terrain/LC (33 classes) |\n\nSuggested citation:\n\n* FAO & IIASA. 2025. Global Agro-ecological Zoning version 5 (GAEZ v5) Model Documentation. https://github.com/un-fao/gaezv5/wiki\n\n\nIt is possible to find additional information:\n\n* [here](https://github.com/un-fao/gaezv5/wiki). \n\nThe data can be freely download from:\n\n* [here](https://data.apps.fao.org/catalog/iso/2c09f61b-801c-47d2-a989-0abd3500a365).\n\n<br>\n\n##### Urban-Rural Catchment Area (URCA)\n\nUrban–rural catchment areas showing the catchment areas around cities and towns of different sizes (the no data value is 128). Each rural pixel is assigned to one defined travel time category to one of seven urban agglomeration sizes.\n\n| Parameter           | Value                                             |\n|:--------------------|:-------------------------------------------------:|\n| spatial resolution  | 0.03° x 0.03° lon lat                             |\n| temporal resolution | year                                              |\n| time frame          | 2015                                              |\n| unit of measure     | travel time category to different urban hierarchy |\n\n\nSuggested citation:\n\n* Cattaneo, Andrea; Nelson, Andy; McMenomy, Theresa (2020). Urban-rural continuum. figshare. Dataset. https://doi.org/10.6084/m9.figshare.12579572.v4\n\nIt is possible to find additional information:\n\n* [here](https://www.pnas.org/doi/full/10.1073/pnas.2011990118). \n\n\nThe data can be freely download from:\n\n* [here](https://figshare.com/articles/dataset/Urban-rural_continuum/12579572).\n\n<br>\n\n##### Population\nThe units are number of people per pixel. The mapping approach is Random Forest-based dasymetric redistribution.\n\n| Parameter           | Value                                   |\n|:--------------------|:---------------------------------------:|\n| spatial resolution  | 30 arc second (~1km)                    |\n| temporal resolution | year                                    |\n| time frame          | 2010 - 2020                             |\n| unit of measure     | estimated count of people per grid-cell |\n\nSuggested citation:\n\n*\tWorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00647 \n\nIt is possible to find additional information from:\n\n* [here](https://www.worldpop.org/methods/top_down_constrained_vs_unconstrained/)\n* [here](https://www.worldpop.org/methods/populations/). \n\nThe data can be freely download from:\n\n* [here](https://hub.worldpop.org/geodata/summary?id=34984).\n\n<br>\n\n##### Nighttime light\n\nVIIRS nighttime lights (VNL) version V2.1: annual values obtained by from the monthly averages with filtering to remove extraneous features such as biomass burning, aurora, and background.\n\n| Parameter           | Value                       |\n|:--------------------|:---------------------------:|\n| spatial resolution  | 15 arc second               |\n| temporal resolution | year                        |\n| time frame          | 2012 - 2021                 |\n| unit of measure     | nW/cm2/sr, average-masked   |\n\nSuggested citation:\n\n* Elvidge, C.D, Zhizhin, M., Ghosh T., Hsu FC, Taneja J. Annual time series of global VIIRS nighttime lights derived from monthly averages:2012 to 2019. Remote Sensing 2021, 13(5), p.922, doi:10.3390/rs13050922\n\nIt is possible to find additional information:\n\n* [here](https://eogdata.mines.edu/products/vnl/). \n\nThe data can be freely download from:\n\n* [here](https://eogdata.mines.edu/nighttime_light/annual/v21/).\n\n<br>\n\n##### Elevation\nElevation is obtained from the auxiliary variables of GloFAS. Each pixel is the mean height elevation above sea level.\n\n| Parameter           | Value                       |\n|:--------------------|:---------------------------:|\n| spatial resolution  |\t0.03° x 0.03° lon lat       |\n| temporal resolution | 30 years                    |\n| time frame          | 1981 - 2010                 |\n| unit of measure     | Meter (m)                   |\n\n\nWeb resources:\n\n* [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-glofas-forecast?tab=overview).\n* [here](https://www.globalfloods.eu/technical-information/products/)\n\nData access:\n\n* [here](https://confluence.ecmwf.int/display/CEMS/Auxiliary+Data)\n\n<br>\n\n#### Survey\n\nThe Living Standards Measurement Study - Integrated Surveys on Agriculture (LSMS-ISA) is a unique system of longitudinal surveys designed to improve the understanding of household and individual welfare, livelihoods and smallholder agriculture in Africa. The LSMS team works with national statistics offices to design and implement household surveys with a strong focus on agriculture.\n\nIt is possible to find additional information:\n\n* [here](https://www.worldbank.org/en/programs/lsms). \n\nThe data can be freely download from:\n\n* [here](https://microdata.worldbank.org/index.php/home).\n\n<br>\n\n#### Administrative boundaries\n\nThe administrative divisions are obtained from GeoBoundaries[^2]. GeoBoundaries Built by the community and William & Mary geoLab, the geoBoundaries Global Database of Political Administrative Boundaries Database is an online, open license (CC BY 4.0) resource of information on administrative boundaries (i.e., state, county) for every country in the world. Since 2016, we have tracked approximately 1 million boundaries within over 200 entities, including all UN member states.\n\n[^2]: Runfola D, Anderson A, Baier H, Crittenden M, Dowker E, Fuhrig S, et al. (2020) geoBoundaries: A global database of political administrative boundaries. PLoS ONE 15(4): e0231866. https://doi.org/10.1371/journal.pone.0231866.\n\nIt is possible to find additional information:\n\n* [here](https://www.geoboundaries.org/countryDownloads.html). \n\nThe data can be freely download from:\n\n* [here](https://www.geoboundaries.org).\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"embed-resources":true,"output-file":"sp_cntrl.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","author":{"name":"JMR","email":"jan.rossi@fao.org","affiliation":{"name":"SERA team, ESP divition, FAO","url":"https://www.fao.org/socioeconomic-research-analysis/en"}},"toc-expand":2,"code-annotations":"hover","editor":"source","editor_options":{"chunk_output_type":"console"},"vignette":"%\\VignetteIndexEntry{Vignette's Title} %\\VignetteEncoding{UTF-8} %\\VignetteEngine{quarto::html}\n","toc-location":"right-body","footnotes-hover":true,"title":"![](images/Climat4Economist_Symbol_6.png){width=1in}\n\nExtract Spatial Variable with Climat4Economist\n","subtitle":"Tutorial on how to extract spatial raster values based on survey coordinated loaction and survey administrative divisions using the Climat4Economist package"},"extensions":{"book":{"multiFile":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","toc":true,"toc-depth":4,"output-file":"sp_cntrl.docx"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"author":{"name":"JMR","email":"jan.rossi@fao.org","affiliation":{"name":"SERA team, ESP divition, FAO","url":"https://www.fao.org/socioeconomic-research-analysis/en"}},"toc-expand":2,"code-annotations":"hover","editor":"source","editor_options":{"chunk_output_type":"console"},"vignette":"%\\VignetteIndexEntry{Vignette's Title} %\\VignetteEncoding{UTF-8} %\\VignetteEngine{quarto::html}\n","toc-location":"body","title":"![](images/Climat4Economist_Symbol_6.png){width=1in}\n\nExtract Spatial Variable with Climat4Economist\n","subtitle":"Tutorial on how to extract spatial raster values based on survey coordinated loaction and survey administrative divisions using the Climat4Economist package"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","docx"]}
{
  "hash": "38b2bff456c7f06a6f52c4257b3ae915",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: | \n     ![](images/Climat4Economist_Symbol.png){width=1in}        ![](images/fao-logo-blue-3lines-en.svg){width=5in}\n     \n     Extract Spatial Variable with Climat4Economist\nsubtitle: Tutorial on how to extract spatial raster values based on survey coordinated loaction and survey administrative divisions using Climat4Economist\n---\n\n\n\n\n<br>\n\n## Introduction\nThis tutorial show how too extract spatial control variables based on surveys locations. The survey refers to Ethipia 2019 and comes from the [The World Bank Living Standards Measurement Study (LSMS)](https://www.worldbank.org/en/programs/lsms).\n\n### Is this guide for me?\n\nThis guide provides a step-by-step approach to extract raster data based on survey locations. The target audience includes economists who may have experience with statistical software (e.g. STATA) but are less familiar with spatial data processing in R.\n\nThe document is not meant to be a course on R or on how the functions work. It is just a practice example on how to extract raster data based on coordinate location. This is done by using specific functions that wrap up as many steps as possible to ensure it is easier for the user to follow.\n\n### Overview of Steps\n\nIn this guide, we will go through the following steps:\n\n1.  Load the data\n2.  Georeference the survey\n3.  Plot the data\n4.  Extract the raster values\n5.  Merge the extracted values with the survey\n6.  Save the results.\n\n### What do I need before starting?\n\nThe following R packages are necessary: `terra`, `tidyverse`, `haven`, `purrr` and `climatic4economist`. To install the above package you can use `install.packages(\"name_of_package\")`, don't forget the `\"`.\n\nThe latter package, `climatic4economist` is not available on the web and it need to be installed from a local file. You can do it with `devtools::install_local(\"climatic4economist\")`\n\nIf you are not familiar with R check the [appendix] for understanding some coding style used in this tutorial.\n\n<br>\n\n## Code\n### Set Up\nWe start by setting up the stage for our analysis.\nFirst, we load the necessary packages. We load only `climatic4economist` package that contains several functions meant to extract and merge spatial variables with surveys. During the tutorial we will use other packages but instead of loading all the package at the begging we will call specific function each time.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(climatic4economist)\n```\n:::\n\n\n\n\nIn the setup, we also want to create the paths to the various data sources and load the necessary functions for extraction. Note `..` means one step back to the folder directory, i.e. one folder back.\n\nNote that how to set up the paths depends on your folder organization but there are overall two approaches: \n\n1. you can use the `R project`, by opening the project directly you don't need to set up the path to the project. Automatically the project figures out on its own where it is located in the computer and set that path as working folder. \n2. you can manually set the working folder with the function `setwd()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# path to data folder\npath_to_data <- file.path(\"..\", # <1>\n                          \"..\", \"data\") # <2>\n\n# survey and administrative division\npath_to_survey  <- file.path(path_to_data, \"survey\", \"LSMS\", \"LSMS_ETH19.dta\")\npath_to_adm_div <- file.path(path_to_data, \"adm_div\", \"geoBoundaries\")\n\n# weather variables\npath_to_pre <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tpr.nc\")\npath_to_tmp <- file.path(path_to_data, \"weather\", \"ERA5_Land\", \"AFR\", \"monthly\",\n                         \"afr_month_50_25_tmp.nc\")\n\n# control variables\npath_to_elevation  <- file.path(path_to_data, \"spatial\", \"elevation\", \"GloFAS\",\n                               \"elevation_glofas_v4_0.nc\")\npath_to_urca       <- file.path(path_to_data, \"spatial\", \"URCA\", \n                               \"URCA.tif\")\npath_to_pop        <- file.path(path_to_data, \"spatial\", \"population\", \"WorldPop\",\n                               \"uncontraint_1km_global\", \"ppp_2019_1km_Aggregated.tif\")\npath_to_nightlight <- file.path(path_to_data, \"spatial\", \"nighttime_light\",\n                                \"VIIRS\", \"VNL_v21_npp_2019_global_vcmslcfg_c202205302300.average_masked.dat.tif\")\npath_to_aez        <- file.path(path_to_data, \"spatial\", \"AgroEcological\", \"AEZ\", \n                                \"GAEZv5\",  \"GAEZ-V5.AEZ33-10km.tif\")\n\n# to result folder\npath_to_result <- file.path(path_to_data, \"result\")\n```\n:::\n\n\n\n1. concatenate the string to make a path\n2. `..` means one folder back\n\n### Read the Data\n#### Survey Data\nWe start by reading the surveys data. The survey is stored as `dta` file, so we use the `haven::read_dta()` function to read it. \n\nWe only need the `hhid`, the survey coordinates, and the interview dates. We use `dplyr::select()` to choose these variables. This passage is optional and we bring with us all the variables, but we won't use them.\n\nThen we create/modify some variables with the function `dplyr::mutate()`. We transform the the variable `interview_date` from string into data, and we get the year of the median value of the date of interviews. This passage is important as it allows us to define the most appropriate year to select for the spatial variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsrvy <- haven::read_dta(path_to_survey) |> # <1>\n    dplyr::select(survey_year, hhid, country, lat, lon, interview_date) |> # <2>\n    dplyr::mutate(\n        interview_date = clock::date_parse(interview_date, # <3>\n                                           format = \"%Y-%m-%d\"), # <4> \n        survey_year    = clock::get_year(median(interview_date)), # <5>\n        .before = hhid)\n```\n:::\n\n\n\n1. read dta type data\n2. select relevant variables\n3. transform string into date type\n4. specify format type\n5. find the median year of the interviews\n\n#### Spatial Data\nFinally, we load the spatial data. This data typically comes in the form of raster data. A raster represents a two-dimensional image as a rectangular matrix or grid of pixels. These are spatial rasters because they are georeferenced, meaning each pixel (or \"cell\" in GIS terms) represents a square region of geographic space. The value of each cell reflects a measurable property (either qualitative or quantitative) of that region.\n\nTo spatial data is usually stored as `tif` file or `nc`. We can read both of them them with the function `terra::rast()`.\n\nWhen we print the raster, we obtain several key details. The dimension tells us how many cells the raster consists of and the number of layers, each layer corresponds to a particular months for which the observations were made. We also get the spatial resolution, which defines the size of each square region in geographic space, and the coordinate reference system (CRS). Note how all the data sets have the same coordinate reference system (CRS), i.e. `EPSG:4326`. This is important because in this way all the data can \"spatially\" talk to each other.\n\nWith the function `setNames()` we change the names of the layer.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npop <- terra::rast(path_to_pop) |> # <1>\n  setNames(\"pop\") # <2>\npop\n\nnightlight <- terra::rast(path_to_nightlight) |>\n  setNames(\"nightlight\")\nnightlight\n\nelevation <- terra::rast(path_to_elevation)\nelevation\n\nurca <- terra::rast(path_to_urca)\nurca\n\naez <- terra::rast(path_to_aez) |>\n    setNames(\"aez\") \naez\n```\n:::\n\n\n\n1. read raster type data\n2. change the name of the layer\n\nNow we also read the weather observation. The same consideration about the coordinate reference system (CRS) is still valid. When we work with raster that have also observations over time, it is important to check how and where the time and date information is stored. \n\n> Note that rasters can store time information in different ways, so it may not always be possible to retrieve dates in this manner. A common alternative is for dates to be embedded in the layer names, in which case we wouldnâ€™t need to rename the layers.\n\nSometimes it is stored in the metadata and you can access it using `terra::time()`, other time it is already saved as the name of the layer and you can access it using `names()`.\nSometimes, like in this case the date information is stored in the names but the format is based on second passed from `1970-01-01 00:00`. To transform this observation into readable date we can use the function `second_to_date()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npre <- terra::rast(path_to_pre)\npre\nnames(pre) <- terra::names(pre) |> second_to_date() # <1>\npre\n\ntmp <- terra::rast(path_to_tmp)\nnames(tmp) <- terra::names(tmp) |> second_to_date() # <1>\n```\n:::\n\n\n\n\n1. transform the layers name with second into dates\n\n#### Administrative Boundaries\nWe now move to read the administrative divisions. We use the function `read_adm_div()` to do so. This function looks for spatial polygons for the `iso` and `lvl` provided provided.\n\nEven if we have the coordinates from the survey, we will extract some spatial variables at the administrative division.\n\nThe same consideration about the coordinate reference system (CRS) is still valid.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadm_div <- read_adm_div(path_to_adm_div, iso = \"ETH\", lvl = 2)\nadm_div\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}